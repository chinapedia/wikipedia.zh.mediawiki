{{NoteTA
|G1 = IT
}}
[[Image:Entropy-mutual-information-relative-entropy-relation-diagram.svg|thumb]]

在[[概率论|概率论]]和[[信息论|信息论]]中，两个[[随机变量|随机变量]]的'''互信息'''（mutual Information，MI）度量了两个变量之间相互依赖的程度。具体来说，对于两个随机变量，MI是在获得一个随机变量的信息之后，观察另一个随机变量所获得的“信息量”（单位通常为比特）。互信息的概念与随机变量的[[熵_(信息论)|熵]]紧密相关，[[熵_(信息论)|熵]]是[[信息论|信息论]]中的基本概念，它量化的是随机变量中所包含的“{{Link-en|信息量|Information content}}”。 

MI不仅仅是度量实值随机变量和线性相关性(如[[相关系数|相关系数]])，它更为通用。MI决定了随机变量<math>{\displaystyle (X,Y)}</math>的[[联合分布|联合分布]]与<math>X</math>和<math>Y</math>的边缘分布的乘积之间的差异。MI是点互信息（{{Link-en|Pointwise Mutual Information|pointwise mutual information}}，PMI）的[[期望|期望]]。[[克劳德·香农|克劳德·香农]]在他的论文{{Link-en|A Mathematical Theory of Communication|A Mathematical Theory of Communication}}中定义并分析了这个度量，但是当时他并没有将其称为“互信息”。这个词后来由[[罗伯特·法诺|罗伯特·法诺]]<ref>{{cite journal | last1 = Kreer | first1 = J. G. | year = 1957 | title = A question of terminology | journal = IRE Transactions on Information Theory| volume = 3 | issue = 3 | page = 208 | doi = 10.1109/TIT.1957.1057418}}</ref>创造。互信息也称为[[信息增益|信息增益]]。

==互信息的定义==
设随机变量<math>{\displaystyle (X,Y)}</math>是空间<math>{\displaystyle {\mathcal {X}}\times {\mathcal {Y}}}</math>中的一对随机变量。若他们的联合分布是<math>{\displaystyle p(x,y)}</math>，边缘分布分别是<math>{\displaystyle p(x)}</math>和<math>{\displaystyle p(y)}</math>，那么，它们之间的互信息可以定义为：

:<math>{\displaystyle I(X;Y)=D_{\mathrm {KL} }(p(x,y)\|p(x)\otimes p(y))}
</math>

其中，<math>{\displaystyle D_{\mathrm {KL} }}</math>为KL散度(Kullback–Leibler divergence)。注意，根据KL散度的性质，若联合分布<math>p(x,y)</math>等于边缘分布<math>p(x)</math>和<math>p(y)</math>的乘积，则<math>I(X;Y)=0</math>，即当<math>X</math>和<math>Y</math>相互独立的时候，观测到Y对于我们预测X没有任何帮助，此时他们的互信息为0。

=== 离散变量的互信息 ===
离散随机变量 X 和 Y 的互信息可以计算为：

:<math> I(X;Y) = \sum_{y \in Y} \sum_{x \in X} 
                 p(x,y) \log{ \left(\frac{p(x,y)}{p(x)\,p(y)}
                              \right) }, \,\!
</math>

其中 ''p''(''x'', ''y'') 是 ''X'' 和 ''Y'' 的联合概率质量函数，而 <math>p(x)</math> 和 <math>p(y)</math>  分别是 ''X'' 和 ''Y'' 的边缘概率质量函数。

=== 连续变量的互信息 ===
在[[连续函数|连续随机变量]]的情形下，求和被替换成了[[二重积分|二重定积分]]：

:<math> I(X;Y) = \int_Y \int_X 
                 p(x,y) \log{ \left(\frac{p(x,y)}{p(x)\,p(y)}
                              \right) } \; dx \,dy,

 </math>

其中 ''p''(''x'', ''y'') 当前是 ''X'' 和 ''Y'' 的联合概率''密度''函数，而 <math>p(x)</math> 和 <math>p(y)</math> 分别是 ''X'' 和 ''Y'' 的边缘概率密度函数。

如果对数以 2 为基底，互信息的单位是[[位元|bit]]。

直观上，互信息度量 ''X'' 和 ''Y'' 共享的信息：它度量知道这两个变量其中一个，对另一个不确定度减少的程度。例如，如果 ''X'' 和 ''Y'' 相互独立，则知道 ''X'' 不对 ''Y'' 提供任何信息，反之亦然，所以它们的互信息为零。在另一个极端，如果 ''X'' 是 ''Y'' 的一个确定性函数，且 ''Y'' 也是 ''X'' 的一个确定性函数，那么传递的所有信息被 ''X'' 和 ''Y'' 共享：知道 ''X'' 决定 ''Y'' 的值，反之亦然。因此，在此情形互信息与 ''Y''（或 ''X''）单独包含的不确定度相同，称作 ''Y''（或 ''X''）的[[信息熵|熵]]。而且，这个互信息与 ''X'' 的熵和 ''Y'' 的熵相同。（这种情形的一个非常特殊的情况是当 ''X'' 和 ''Y'' 为相同随机变量时。）

互信息是 ''X'' 和 ''Y'' 的[[联合分布|联合分布]]相对于假定 ''X'' 和 ''Y'' 独立情况下的联合分布之间的内在依赖性。
于是互信息以下面方式度量依赖性：''I''(''X''; ''Y'') = 0 [[当且仅当|当且仅当]] ''X'' 和 ''Y'' 为独立随机变量。从一个方向很容易看出：当 ''X'' 和 ''Y'' 独立时，''p''(''x'',''y'') = ''p''(''x'') ''p''(''y'')，因此：

:<math> \log{ \left( \frac{p(x,y)}{p(x)\,p(y)} \right) } = \log 1 = 0. \,\!
 </math>

此外，互信息是非负的（即 <math>I(X;Y)\ge0</math>; 见下文），而且是{{Link-en|对称函数|Symmetric function|对称的}}（即 <math>I(X;Y) = I(Y;X)</math>）。

==与其他量的关系==
互信息又可以等价地表示成

:<math>
\begin{align}
I(X;Y) & {} = H(X) - H(X|Y) \\ 
& {} = H(Y) - H(Y|X) \\ 
& {} = H(X) + H(Y) - H(X,Y) \\
& {} = H(X,Y) - H(X|Y) - H(Y|X)
\end{align}
</math>

其中 <math>\ H(X)</math> 和 <math>\ H(Y)</math> 是边缘[[信息熵|熵]]，''H''(''X''|''Y'') 和 ''H''(''Y''|''X'') 是[[条件熵|条件熵]]，而 ''H''(''X'',''Y'') 是 ''X'' 和 ''Y'' 的[[联合熵|联合熵]]。注意到这组关系和并集、差集和交集的关系类似，于是用Venn图表示。

在互信息定义的基础上使用[[琴生不等式|琴生不等式]]，我们可以证明 ''I''(''X'';''Y'') 是非负的，因此 <math>\ H(X) \ge H(X|Y)</math>。这里我们给出 I(X;Y) = H(Y) - H(Y|X) 的详细推导:

:<math>
\begin{align}
I(X;Y) & {} = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}\\ 
& {} = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)} - \sum_{x,y} p(x,y) \log p(y)  \\ 

& {} = \sum_{x,y} p(x)p(y|x) \log p(y|x) - \sum_{x,y} p(x,y) \log p(y) \\
& {} = \sum_x p(x) \left(\sum_y p(y|x) \log p(y|x)\right) - \sum_y \log p(y) \left(\sum_x p(x,y)\right) \\

& {} = -\sum_x p(x) H(Y|X=x) - \sum_y \log p(y) p(y) \\
& {} = -H(Y|X) + H(Y)  \\
& {} = H(Y) - H(Y|X).  \\
\end{align}
</math>
上面其他性质的证明类似。

直观地说，如果把熵 ''H''(''Y'') 看作一个随机变量於不确定度的量度，那么 ''H''(''Y''|''X'') 就是"在已知 ''X'' 事件後''Y''事件會發生"的不確定度。於是第一个等式的右边就可以读作“將"Y事件的不确定度"，减去 --- "在基於''X''事件後''Y''事件因此發生的不確定度"”。

这证实了互信息的直观意义为: "因X而有Y事件"的熵( 基於已知隨機變量的不確定性) 在"Y事件"的熵之中具有多少影響地位( "Y事件所具有的不確定性" 其中包含了多少 "Y|X事件所具有的不確性" )，意即"Y具有的不確定性"有多少程度是起因於X事件; 

     ''舉例來說，當 I(X;Y) = 0時，也就是 H(Y) = H(Y|X)時，即代表此時 "Y的不確定性" 即為 "Y|X的不確定性"，這說明了互信息的具體意義是在度量'''兩個事件彼此之間的關聯性'''。''

所以具體的解釋就是: 互信息越小，兩個來自不同事件空間的隨機變量彼此之間的關聯性越低; 互信息越高，關聯性則越高 。



注意到离散情形 ''H''(''X''|''X'') = 0，于是 ''H''(''X'') = ''I''(''X'';''X'')。因此 ''I''(''X'';''X'') ≥ ''I''(''X'';''Y'')，我们可以制定”一个变量至少包含其他任何变量可以提供的与它有关的信息“的基本原理。

互信息也可以表示为两个随机变量的[[边缘分布|边缘分布]] ''X'' 和 ''Y'' 的乘积 ''p''(''x'') × ''p''(''y'') 相对于随机变量的[[联合熵|联合熵]] ''p''(''x'',''y'') 的[[相对熵|相对熵]]：

:<math> I(X;Y) = D_{\mathrm{KL}}(p(x,y)\|p(x)p(y)). </math>

此外，令 ''p''(''x''|''y'') = ''p''(''x'', ''y'') / ''p''(''y'')。则

:<math>
\begin{align}
I(X;Y) & {} = \sum_y p(y) \sum_x p(x|y) \log_2 \frac{p(x|y)}{p(x)} \\
& {} =  \sum_y p(y) \; D_{\mathrm{KL}}(p(x|y)\|p(x)) \\
& {} = \mathbb{E}_Y\{D_{\mathrm{KL}}(p(x|y)\|p(x))\}.
\end{align}
</math>

注意到，这里相对熵涉及到仅对随机变量 ''X'' 积分，表达式 <math>D_{\mathrm{KL}}(p(x|y)\|p(x))</math> 现在以 ''Y'' 为变量。于是互信息也可以理解为相对熵 ''X'' 的单变量分布 ''p''(''x'') 相对于给定 ''Y'' 时 ''X'' 的[[条件分布|条件分布]] ''p''(''x''|''y'') ：分布 ''p''(''x''|''y'') 和 ''p''(''x'') 之间的平均差异越大，[[相对熵|信息增益]]越大。

=== 連續互信息的量化 ===
對連續型隨機變數量化的定義如下：

<math>f(x_i)\Delta=\int_{i\Delta}^{(i+1)\Delta}f(x)dx=p_i
</math>

量化後的隨機變數<math>X^{\Delta}</math>:

<math>X^{\Delta}=x_i, i \Delta \leq X <(i+1)\Delta </math>。

則,

<math>I(X^{\Delta};Y^{\Delta})=H(X^{\Delta})-H(X^\Delta|Y^\Delta)  </math>

<math>\approx h(X)-log{\Delta}-(h(X|Y)-log{\Delta})</math>

<math>=I(X;Y)</math>

廣義而言，我們可以將互信息定義在有限多個連續隨機變數[[值域|值域]]的[[集合划分|劃分]]。

令<math>\chi</math>為連續型隨機變數的值域，<math>P_i\in P </math>, 其中<math>P</math>為<math>\chi</math>劃分所構成的集合，意即<math>\cup_iP_i=\chi</math>。

以<math>P</math>量化連續型隨機變數<math>X</math>後，所得結果為離散型隨機變數,

<math>Pr([X]_P=i)=\int_{P_i} dF(x)</math>。

對於兩連續型隨機變數X、Y，其劃分分別為P、Q，則其互信息可表示為：

<math>I(X;Y)=\underset{P,Q}{sup}I([X]_P;[Y]_Q)</math>。

== 参见 ==
* {{le|点间互信息|Pointwise mutual information}}
* {{le|量子互信息|Quantum mutual information}}

== 注释 ==
<references/>

== 参考文献 ==
* {{Cite_journal
  | last = Cilibrasi 
  | first = Rudi 
  | author2 = Paul M.B. Vitan´ yi
  | title = Clustering by compression
  | journal = IEEE Transactions on Information Theory
  | volume = 51
  | issue = 4
  | pages = 1523–1545
  | year = 2005
  | url = http://www.cwi.nl/~paulv/papers/cluster.pdf
  | format = [[PDF|PDF]]
  | doi = 10.1109/TIT.2005.844059
  | ref = harv
}}
* Cronbach L. J. (1954). On the non-rational application of information measures in psychology, in [[Henry_Quastler|Henry Quastler]], ed., ''Information Theory in Psychology: Problems and Methods'', Free Press, Glencoe, Illinois, pp. 14–30.
* {{Cite_journal|first1=Kenneth Ward|last1=Church|first2=Patrick|last2=Hanks|title=Word association norms, mutual information, and lexicography|journal=Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics|year=1989|url=http://delivery.acm.org/10.1145/90000/89095/p22-church.pdf?ip=87.193.196.98&id=89095&acc=OPEN&key=BF13D071DEA4D3F3B0AA4BA89B4BCA5B&CFID=238314452&CFTOKEN=56636212&__acm__=1375963734_fac6312f7247e2f4d7915da1a876acbe}}{{Dead link|date=2020年7月 |bot=InternetArchiveBot |fix-attempted=yes }}<!--Syntax highlighter補丁-->
* {{Cite_book|author=Guiasu, Silviu|year=1977|title=Information Theory with Applications|publisher=McGraw-Hill, New York|isbn=978-0070251090}}
* {{Cite_book
  | last = Li
  | first = Ming
  | author2 = Paul M.B. Vitan´ yi
  | title = An introduction to Kolmogorov complexity and its applications
  | location = New York
  | publisher = [[Springer-Verlag|Springer-Verlag]]
  | date = February 1997
  | isbn = 0-387-94868-6 }}
* Lockhead G. R. (1970). Identification and the form of multidimensional discrimination space, ''Journal of Experimental Psychology'' '''85'''(1), 1–10.
* David J. C. MacKay. ''[https://web.archive.org/web/20160217105359/http://www.inference.phy.cam.ac.uk/mackay/itila/book.html Information Theory, Inference, and Learning Algorithms]'' Cambridge: Cambridge University Press, 2003. ISBN 0-521-64298-1 (available free online)
* Haghighat, M. B. A., Aghagolzadeh, A., & Seyedarabi, H. (2011). A non-reference image fusion metric based on mutual information of image features. Computers & Electrical Engineering, 37(5), 744-756.
* [[Athanasios_Papoulis|Athanasios Papoulis]]. ''Probability, Random Variables, and Stochastic Processes'', second edition. New York: McGraw-Hill, 1984. ''(See Chapter 15.)''
* {{Cite_book|author=Witten, Ian H. & Frank, Eibe|year=2005|title=Data Mining: Practical Machine Learning Tools and Techniques|publisher=Morgan Kaufmann, Amsterdam|isbn=978-0-12-374856-0|url=http://www.cs.waikato.ac.nz/~ml/weka/book.html|access-date=2015-04-02|archive-date=2020-11-27|archive-url=https://web.archive.org/web/20201127014857/https://www.cs.waikato.ac.nz/~ml/weka/book.html|dead-url=no}}
* {{Cite_journal|author=Peng, H.C., Long, F., and Ding, C.|title=Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence|volume=27|issue=8|pages=1226–1238|year=2005|url=http://research.janelia.org/peng/proj/mRMR/index.htm|doi=10.1109/tpami.2005.159|access-date=2015-04-02|archive-date=2009-05-22|archive-url=https://web.archive.org/web/20090522065535/http://research.janelia.org/peng/proj/mRMR/index.htm|dead-url=no}}
* {{Cite_journal|author=Andre S. Ribeiro, Stuart A. Kauffman, Jason Lloyd-Price, Bjorn Samuelsson, and Joshua Socolar|year=2008|title=Mutual Information in Random Boolean models of regulatory networks|journal=Physical Review E|volume=77|issue=1|arxiv=0707.3642}}
* {{Cite_journal
 |last        = Wells
 |first       = W.M. III
 |author2     = Viola
 |author3     = P.
 |author4     = Atsumi
 |author5     = H.
 |author6     = Nakajima
 |author7     = S.
 |author8     = Kikinis
 |author9     = R.
 |title       = Multi-modal volume registration by maximization of mutual information
 |journal     = Medical Image Analysis
 |volume      = 1
 |issue       = 1
 |pages       = 35–51
 |year        = 1996
 |pmid        = 9873920
 |doi         = 10.1016/S1361-8415(01)80004-9
 |url         = http://www.ai.mit.edu/people/sw/papers/mia.pdf
 |format      = [[PDF|PDF]]
 |ref         = harv
 |deadurl     = yes
 |archiveurl  = https://web.archive.org/web/20080906201633/http://www.ai.mit.edu/people/sw/papers/mia.pdf
 |archivedate = 2008-09-06
 |access-date = 2015-04-02
}}

{{DEFAULTSORT:Mutual Information}}
[[Category:信息论|Category:信息论]]
[[Category:信息學熵|Category:信息學熵]]