{{noteTA
|G1 = IT
}}
{{about|模拟生物神经网络的'''数学模型'''|生物的神经网络|生物神经网络}}
{{机器学习导航栏}}

'''人工神经网络'''（{{lang-en|Artificial Neural Network}}，ANN），简称'''神经网络'''（Neural Network，NN）或'''類-{}-神經網絡'''，在[[机器学习|机器学习]]和[[认知科学|认知科学]]领域，是一种[[仿生學|模仿]][[生物神经网络|生物神经网络]]（动物的[[中樞神經系統|中樞神經系統]]，特别是[[大脑|大脑]]）的结构和功能的[[数学模型|数学模型]]或[[计算模型|计算模型]]，用于对[[函数|函数]]进行估计或近似。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种[[自适应系统|自适应系统]]，通俗地讲就是具备学习功能。现代神经网络是一种[[非线性|非线性]][[统计性数据建模|统计性数据建模]]工具，神经网络通常是通过一个基于数学统计学类型的学习方法（Learning Method）得以优化，所以也是数学统计学方法的一种实际应用，通过统计学的标准数学方法我们能够得到大量的可以用函数来表达的局部结构空间，另一方面在人工智能学的人工感知领域，我们通过数学统计学的应用可以来做人工感知方面的决定问题（也就是说通过统计学的方法，人工神经网络能够类似人一样具有简单的决定能力和简单的判断能力），这种方法比起正式的逻辑学推理演算更具有优势。

和其他[[机器学习|机器学习]]方法一样，神经网络已经被用于解决各种各样的问题，例如[[机器视觉|机器视觉]]和[[语音识别|语音识别]]。这些问题都是很难被传统基于规则的编程所解决的。

== 背景 ==
对人类[[中枢神经系统|中枢神经系统]]的观察启发了人工神经网络这个概念。在人工神经网络中，简单的人工节点，称作[[神經元|神經元]]（neurons），连接在一起形成一个类似[[生物神经网络|生物神经网络]]的网状结构。

人工神经网络目前没有一个统一的正式定义。不过，具有下列特点的[[统计模型|统计模型]]可以被称作是“神经化”的：
* 具有一组可以被调节的权重（被学习算法调节的数值参数）
* 可以估计输入数据的[[非線性系統|非线性函数]]关系
这些可调节的权重可以被看做神经元之间的连接强度。

人工神经网络与生物神经网络的相似之处在于，它可以集体地、并行地计算函数的各个部分，而不需要描述每一个单元的特定任务。神经网络这个词一般指[[统计学|统计学]]、[[认知心理学|认知心理学]]和[[人工智能|人工智能]]领域使用的模型，而控制中央神经系统的神经网络属于[[理论神经学|理论神经科学]]和[[计算神经科学|计算神经科学]]。<ref>{{Cite journal|title=Methodology and Coronary Artery Disease Cure|url=https://www.researchgate.net/publication/281017979_Methodology_and_Coronary_Artery_Disease_Cure|last=Hentrich|first=Michael William|date=2015-08-16|doi=10.1709/TIT.2015.1083925|journal=|access-date=2016-07-31|archive-date=2020-02-03|archive-url=https://web.archive.org/web/20200203235029/https://www.researchgate.net/publication/281017979_Methodology_and_Coronary_Artery_Disease_Cure|dead-url=no}}</ref>

在神经网络的现代软件实现中，被[[生物学|生物学]]启发的那种方法已经很大程度上被抛弃了，取而代之的是基于[[统计学|统计学]]和[[信号处理|信号处理]]的更加实用的方法。在一些软件系统中，神经网络或者神经网络的一部分（例如人工神经元）是大型系统中的一个部分。这些系统结合了适应性的和非适应性的元素。虽然这种系统使用的这种更加普遍的方法更适宜解决现实中的问题，但是这和传统的连接主义人工智能已经没有什么关联了。不过它们还有一些共同点：非线性、分布式、并行化，局部性计算以及适应性。从历史的角度讲，神经网络模型的应用标志着二十世纪八十年代后期从高度符号化的人工智能（以用条件规则表达知识的[[专家系统|专家系统]]为代表）向低符号化的机器学习（以用动力系统的参数表达知识为代表）的转变。

== 历史 ==
[[沃伦·麦卡洛克|沃伦·麦卡洛克]]和[[沃尔特·皮茨|沃尔特·皮茨]]（1943）<ref>{{Cite journal|title=A logical calculus of the ideas immanent in nervous activity|url=http://link.springer.com/article/10.1007/BF02478259|last=McCulloch|first=Warren S.|last2=Pitts|first2=Walter|date=1943-12-01|journal=The bulletin of mathematical biophysics|issue=4|doi=10.1007/BF02478259|volume=5|pages=115–133|language=en|issn=0007-4985|access-date=2016-07-31|archive-date=2020-12-15|archive-url=https://web.archive.org/web/20201215090742/https://link.springer.com/article/10.1007%2FBF02478259|dead-url=no}}</ref>基于数学和一种称为[[阈值|阈值]][[逻辑|逻辑]]的[[算法|算法]]创造了一种神经网络的计算模型。这种模型使得神经网络的研究分裂为两种不同研究思路。一种主要关注大脑中的生物学过程，另一种主要关注神经网络在人工智能裡的应用。

=== 赫布型学习 ===
二十世纪40年代后期，心理学家[[唐纳德·赫布|唐纳德·赫布]]根据神经可塑性的机制创造了一种对学习的假说，现在称作[[赫布型学习|赫布型学习]]。赫布型学习被认为是一种典型的[[非监督式学习|非监督式学习]]规则，它后来的变种是[[長期增強作用|长期增强作用]]的早期模型。从1948年开始，研究人员将这种计算模型的思想应用到B型图灵机上。

法利和[[韦斯利·A·克拉克|韦斯利·A·克拉克]]（1954）<ref>{{Cite journal|title=Simulation of self-organizing systems by digital computer|url=http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1057468|last=Farley|first=B.|last2=Clark|first2=W.|date=1954-09-01|journal=Transactions of the IRE Professional Group on Information Theory|issue=4|doi=10.1109/TIT.1954.1057468|volume=4|pages=76–84|issn=2168-2690}}</ref>首次使用计算机，当时称作计算器，在MIT模拟了一个赫布网络。[[纳撒尼尔·罗切斯特|纳撒尼尔·罗切斯特]]（1956）等人<ref>{{Cite journal|title=Tests on a cell assembly theory of the action of the brain, using a large digital computer|url=http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1056810|last=Rochester|first=N.|last2=Holland|first2=J.|date=1956-09-01|journal=IRE Transactions on Information Theory|issue=3|doi=10.1109/TIT.1956.1056810|volume=2|pages=80–93|issn=0096-1000|last3=Haibt|first3=L.|last4=Duda|first4=W.}}</ref>模拟了一台 IBM 704计算机上的抽象神经网络的行为。 

{{tsl|en|Frank Rosenblatt|弗兰克·罗森布拉特}}创造了[[感知机|感知机]]<ref>{{cite journal|last=Rosenblatt|first=F.|title=The Perceptron: A Probabilistic Model For Information Storage And Organization In The Brain|url=https://archive.org/details/sim_psychological-review_1958-11_65_6/page/386|journal=Psychological Review|year=1958|volume=65|pages=386–408|doi=10.1037/h0042519|pmid=13602029|issue=6|citeseerx=10.1.1.588.3775}}</ref>。这是一种模式识别算法，用简单的加减法实现了两层的计算机学习网络。罗森布拉特也用数学符号描述了基本感知机里没有的回路，例如异或回路。这种回路一直无法被神经网络处理，直到[[保罗·韦伯斯|保罗·韦伯斯]](1975)创造了[[反向传播算法|反向传播算法]]。

在[[马文·闵斯基|马文·明斯基]]和[[西摩爾·派普特|西摩爾·派普特]]（1969）发表了一项关于机器学习的研究以后，神经网络的研究停滞不前。他们发现了神经网络的两个关键问题。第一是基本感知机无法处理异或回路。第二个重要的问题是电脑没有足够的能力来处理大型神经网络所需要的很长的计算时间。直到计算机具有更强的计算能力之前，神经网络的研究进展缓慢。

=== 反向传播算法与复兴 ===
后来出现的一个关键的进展是[[保罗·韦伯斯|保罗·韦伯斯]]发明的[[反向传播算法|反向传播算法]]（Werbos 1975）。这个算法有效地解决了异或的问题，还有更普遍的训练多层神经网络的问题。

在二十世纪80年代中期，[[分布式并行处理|分布式并行处理]]（当时称作[[联结主义|联结主义]]）流行起来。{{tsl|en|David E. Rumelhart|戴维·鲁姆哈特}}和{{tsl|en|James McClelland|詹姆斯·麦克里兰德}}的教材对于联结主义在计算机模拟神经活动中的应用提供了全面的论述。

神经网络传统上被认为是大脑中的神经活动的简化模型，虽然这个模型和大脑的生理结构之间的关联存在争议。人们不清楚人工神经网络能多大程度地反映大脑的功能。

[[支持向量机|支持向量机]]和其他更简单的方法（例如[[线性分类器|线性分类器]]）在机器学习领域的流行度逐渐超过了神经网络，但是在2000年代后期出现的[[深度学习|深度学习]]重新激发了人们对神经网络的兴趣。

=== 2006年之后的进展 ===
人们用[[CMOS|CMOS]]创造了用于生物物理模拟和神经形态计算的计算装置。最新的研究显示了用于大型[[主成分分析|主成分分析]]和[[卷积神经网络|卷积神经网络]]的纳米装置<ref>{{cite journal | last1 = Yang | first1 = J. J. | last2 = Pickett | first2 = M. D. | last3 = Li | first3 = X. M. | last4 = Ohlberg | first4 = D. A. A. | last5 = Stewart | first5 = D. R. | last6 = Williams | first6 = R. S. | year = 2008 | title =  Memristive switching mechanism for metal/oxide/metal nanodevices| url = | journal = Nat. Nanotechnol | volume = 3 | issue = | pages = 429–433 | doi = 10.1038/nnano.2008.160 }}</ref>具有良好的前景。如果成功的话，这会创造出一种新的神经计算装置<ref>{{cite journal | last1 = Strukov | first1 = D. B. | last2 = Snider | first2 = G. S. | last3 = Stewart | first3 = D. R. | last4 = Williams | first4 = R. S. | year = 2008 | title =  The missing memristor found| url = | journal = Nature | volume = 453 | issue = | pages = 80–83 | doi=10.1038/nature06932 | pmid=18451858}}</ref>，因为它依赖于学习而不是编程，并且它从根本上就是[[模擬信號|模拟]]的而不是[[数字化|数字化]]的，虽然它的第一个实例可能是数字化的CMOS装置。

在2009到2012年之间，Jürgen Schmidhuber在Swiss AI Lab IDSIA的研究小组研发的[[循环神经网络|循环神经网络]]和深前馈神经网络赢得了8项关于模式识别和机器学习的国际比赛。<ref>[http://www.kurzweilai.net/how-bio-inspired-deep-learning-keeps-winning-competitions 2012 Kurzweil AI Interview] {{Wayback|url=http://www.kurzweilai.net/how-bio-inspired-deep-learning-keeps-winning-competitions |date=20180831075249 }} with [[Jürgen_Schmidhuber|Jürgen Schmidhuber]] on the eight competitions won by his Deep Learning team 2009–2012</ref><ref>http://www.kurzweilai.net/how-bio-inspired-deep-learning-keeps-winning-competitions {{Wayback|url=http://www.kurzweilai.net/how-bio-inspired-deep-learning-keeps-winning-competitions |date=20180831075249 }} 2012 Kurzweil AI Interview with [[Jürgen_Schmidhuber|Jürgen Schmidhuber]] on the eight competitions won by his Deep Learning team 2009–2012</ref>例如，Alex Graves et al.的双向、多维的LSTM赢得了2009年ICDAR的3项关于连笔字识别的比赛，而且之前并不知道关于将要学习的3种语言的信息。<ref>Graves, Alex; and Schmidhuber, Jürgen; ''[http://www.idsia.ch/~juergen/nips2009.pdf Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks] {{Wayback|url=http://www.idsia.ch/~juergen/nips2009.pdf |date=20130722083149 }}'', in Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris K. I.; and Culotta, Aron (eds.), ''Advances in Neural Information Processing Systems 22 (NIPS'22), 7–10 December 2009, Vancouver, BC'', Neural Information Processing Systems (NIPS) Foundation, 2009, pp. 545–552.</ref><ref>{{cite journal | last1 = Graves | first1 = A. | authorlink6 = Jürgen Schmidhuber | last2 = Liwicki | first2 = M. | last3 = Fernandez | first3 = S. | last4 = Bertolami | first4 = R. | last5 = Bunke | first5 = H. | last6 = Schmidhuber | first6 = J. | title = A Novel Connectionist System for Improved Unconstrained Handwriting Recognition | url = http://www.idsia.ch/~juergen/tpami_2008.pdf | format = PDF | journal = IEEE Transactions on Pattern Analysis and Machine Intelligence | volume = 31 | issue = 5 | year = 2009 | access-date = 2016-07-31 | archive-date = 2014-01-02 | archive-url = https://web.archive.org/web/20140102212756/http://www.idsia.ch/~juergen/tpami_2008.pdf | dead-url = no }}</ref><ref>Graves, Alex; and Schmidhuber, Jürgen; ''Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks'', in Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris K. I.; and Culotta, Aron (eds.), ''Advances in Neural Information Processing Systems 22 (NIPS'22), December 7th–10th, 2009, Vancouver, BC'', Neural Information Processing Systems (NIPS) Foundation, 2009, pp. 545–552</ref><ref>{{cite journal | last1 = Graves | first1 = A. | last2 = Liwicki | first2 = M. | last3 = Fernandez | first3 = S. | last4 = Bertolami | first4 = R. | last5 = Bunke | first5 = H. | last6 = Schmidhuber | first6 = J. | title = A Novel Connectionist System for Improved Unconstrained Handwriting Recognition | url = | journal = IEEE Transactions on Pattern Analysis and Machine Intelligence | volume = 31 | issue = 5| year = 2009 }}</ref>

IDSIA的Dan Ciresan和同事根据这个方法编写的基于GPU的实现赢得了多项模式识别的比赛，包括IJCNN 2011交通标志识别比赛等等。<ref>D. C. Ciresan, U. Meier, J. Masci, [[Jürgen_Schmidhuber|J. Schmidhuber]]. Multi-Column Deep Neural Network for Traffic Sign Classification. Neural Networks, 2012.
</ref><ref>D. C. Ciresan, U. Meier, J. Masci, J. Schmidhuber. Multi-Column Deep Neural Network for Traffic Sign Classification. Neural Networks, 2012.</ref>他们的神经网络也是第一个在重要的基准测试中（例如IJCNN 2012交通标志识别和NYU的[[扬·勒丘恩|扬·勒丘恩]]（Yann LeCun）的MNIST手写数字问题）能达到或超过人类水平的人工模式识别器。

类似1980年Kunihiko Fukushima发明的neocognitron<ref name="K. Fukushima. Neocognitron 1980">{{cite journal|year=1980|title=Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position|journal=Biological Cybernetics|volume=36|issue=4|pages=93–202|doi=10.1007/BF00344251|pmid=7370364|author=Fukushima, K.}}</ref>和视觉标准结构<ref name="M Riesenhuber, 1999">M Riesenhuber, [[Tomaso_Poggio|T Poggio]]. Hierarchical models of object recognition in cortex. Nature neuroscience, 1999.</ref>（由David H. Hubel和Torsten Wiesel在初级视皮层中发现的那些简单而又复杂的细胞启发）那样有深度的、高度非线性的神经结构可以被多伦多大学[[杰弗里·辛顿|杰弗里·辛顿]]实验室的非监督式学习方法所训练。<ref>[http://www.scholarpedia.org/article/Deep_belief_networks Deep belief networks] {{Wayback|url=http://www.scholarpedia.org/article/Deep_belief_networks |date=20151204000832 }} at Scholarpedia.</ref><ref>{{Cite journal | last1 = Hinton | first1 = G. E. | authorlink1 = Geoff Hinton | last2 = Osindero | first2 = S. | last3 = Teh | first3 = Y. W. | doi = 10.1162/neco.2006.18.7.1527 | title = A Fast Learning Algorithm for Deep Belief Nets | journal = [[Neural_Computation_(journal)|Neural Computation]] | volume = 18 | issue = 7 | pages = 1527–1554 | year = 2006 | pmid = 16764513 | pmc =  | url = http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf | access-date = 2016-07-31 | archive-date = 2015-12-23 | archive-url = https://web.archive.org/web/20151223164129/http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf | dead-url = no }}</ref><ref>{{cite journal
|doi = 10.1162/neco.2006.18.7.1527
|last1 = Hinton
|first1 = G. E.
|authorlink1 = Geoffrey Hinton
|last2 = Osindero
|first2 = S.
|last3 = Teh
|first3 = Y.
|year = 2006
|title = A fast learning algorithm for deep belief nets
|journal = [[Neural_Computation_(journal)|Neural Computation]]
|volume = 18
|issue = 7
|pages = 1527–1554
|url = http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf
|pmid = 16764513
|access-date = 2016-07-31
|archive-date = 2015-12-23
|archive-url = https://web.archive.org/web/20151223164129/http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf
|dead-url = no
}}</ref>
2012年，神經網路出現了快速的發展，主要原因在於計算技術的提高，使得很多複雜的運算變得成本低廉。以AlexNet為標誌，大量的深度網路開始出現。

2014年出现了残差神经网络，该网络极大解放了神经网络的深度限制，出现了深度学习的概念。

==构成==
典型的人工神经网络具有以下三个部分：

*'''结构'''（'''Architecture'''）结构指定了网络中的变量和它们的拓扑关系。例如，神经网络中的变量可以是神经元连接的[[权重|权重]]（weights）和神经元的激励值（activities of the neurons）。
*'''激励函数（Activation Rule）'''大部分神经网络模型具有一个短时间尺度的动力学规则，来定义神经元如何根据其他神经元的活动来改变自己的激励值。一般激励函数依赖于网络中的权重（即该网络的参数）。
*'''学习规则（Learning Rule）'''学习规则指定了网络中的权重如何随着时间推进而调整。这一般被看做是一种长时间尺度的动力学规则。一般情况下，学习规则依赖于神经元的激励值。它也可能依赖于监督者提供的目标值和当前权重的值。例如，用于[[手写识别|手写识别]]的一个神经网络，有一组输入神经元。输入神经元会被输入图像的数据所激發。在激励值被加权并通过一个[[函数|函数]]（由网络的设计者确定）后，这些神经元的激励值被传递到其他神经元。这个过程不断重复，直到输出神经元被激發。最后，输出神经元的激励值决定了识别出来的是哪个字母。

==神经元==
神经元示意图：
[[File:Ncell.png|center]]

*a1~an为输入向量的各个分量
*w1~wn为神经元各个突触的权值
*b为偏置
*f为传递函数，通常为非线性函数。一般有traingd(),tansig(),hardlim()。以下默认为hardlim()
*t为神经元输出
数学表示
<math>t = f(\vec{W'}\vec{A}+b)</math>
*<math>\vec{W}</math>为权向量，<math>\vec{W'}</math>为<math>\vec{W}</math>的转置
*<math>\vec{A}</math>为输入向量
*<math>b</math>为偏置
*<math>f</math>为传递函数

可见，一个神经元的功能是求得输入向量与权向量的[[内积|内积]]后，经一个非线性传递函数得到一个[[标量_(数学)|标量]]结果。

单个神经元的作用：把一个n维向量空间用一个超平面分割成两部分（称之为判断边界），给定一个输入向量，神经元可以判断出这个向量位于超平面的哪一边。

该超平面的方程：<math>\vec{W'}\vec{p}+b=0</math> 
*<math>\vec{W}</math>权向量
*<math>b</math>偏置
*<math>\vec{p}</math>超平面上的向量<ref>{{cite book | last = Hagan | first = Martin | authorlink = | coauthors = | title = Neural Network Design | publisher = PWS Publishing Company | date = 1996 | pages = | url = | doi = | id = | isbn = 7-111-10841-8 }}</ref>

==神经元网络==

===单层神经元网络===
是最基本的神经元网络形式，由有限个神经元构成，所有神经元的输入向量都是同一个向量。由于每一个神经元都会产生一个标量结果，所以单层神经元的输出是一个向量，向量的维数等于神经元的数目。

示意图：
[[File:SingleLayerNeuralNetwork_english.png|center]]

===多层神经元网络===

==人工神经网络的实用性==
人工神经网络是一个能够学习，能够总结归纳的系统，也就是说它能够通过已知数据的实验运用来学习和归纳总结。人工神经网络通过对局部情况的对照比较（而这些比较是基于不同情况下的自动学习和要实际解决问题的复杂性所决定的），它能够推理产生一个可以自动识别的系统。与之不同的基于符号系统下的学习方法，它们也具有推理功能，只是它们是建立在逻辑演算法的基础上，也就是说它们之所以能够推理，基础是需要有一个推理演算法则的集合。

==人工神经元网络模型==
通常来说，一个人工神经元网络是由一个多层神经元结构组成，每一层神经元拥有输入（它的输入是前一层神经元的输出）和输出，每一层（我们用符号记做）Layer(i)是由Ni(Ni代表在第i层上的N)个网络神经元组成，每个Ni上的网络神经元把对应在Ni-1上的神经元输出做为它的输入，我们把神经元和与之对应的神经元之间的连线用生物学的名称，叫做'''突触'''（{{Lang-en|Synapse}}），在数学模型中每个突触有一个加权数值，我们称做权重，那么要计算第i层上的某个神经元所得到的势能等于每一个权重乘以第i-1层上对应的神经元的输出，然后全体求和得到了第i层上的某个神经元所得到的势能，然后势能数值通过该神经元上的激活函数（activation function，常是∑函数（{{Lang-en|Sigmoid function}}）以控制輸出大小，因為其可微分且連續，方便差量规则（{{Lang-en|Delta rule}}）處理。），求出该神经元的输出，注意的是该输出是一个非线性的数值，也就是说通过激励函数求的数值根据极限值来判断是否要激活该神经元，换句话说我们对一个神经元网络的输出是否线性不感兴趣。

==基本結構==
一种常见的多层结构的前馈网络（Multilayer Feedforward Network）由三部分組成，
*輸入層（Input layer），眾多神經元（Neuron）接受大量非線形輸入訊息。輸入的訊息稱為輸入向量。
*輸出層（Output layer），訊息在神經元鏈接中傳輸、分析、權衡，形成輸出結果。輸出的訊息稱為輸出向量。
*隱藏層（Hidden layer），簡稱「隱層」，是輸入層和輸出層之間眾多神經元和鏈接組成的各個層面。隱層可以有一层或多層。隱層的節點（神經元）數目不定，但數目越多神經網絡的非線性越顯著，從而神經網絡的[[強健性_(電腦科學)|強健性]]（控制系統在一定結構、大小等的參數攝動下，維持某些性能的特性）更顯著。習慣上會選輸入節點1.2至1.5倍的節點。

这种网络一般称为[[感知器|感知器]]（对单隐藏层）或[[多层感知器|多层感知器]]（对多隐藏层），神经网络的类型已经演变出很多种，这种分层的结构也并不是对所有的神经网络都适用。

==學習過程==
通過訓練樣本的校正，對各個層的權重進行校正（learning）而建立模型的過程，稱為自動學習過程（training algorithm）。具体的学习方法则因网络结构和模型不同而不同，常用[[反向传播算法|反向传播算法]]（Backpropagation/倒傳遞/逆傳播，以output利用一次微分{{tsl|en|Delta rule|}}來修正weight）來驗證。

==種類==
人工神經網络分類為以下兩種：

1.依學習策略（Algorithm）分類主要有：
*[[監督式學習網絡|-{zh-hans:监督式学习网络;zh-hant:監督式學習網絡}-]]（Supervised Learning Network）為主
*[[無監督式學習網絡|-{zh-hans:无监督式学习网络;zh-hant:無監督式學習網絡}-]]（Unsupervised Learning Network）
*[[混合式學習網絡|-{zh-hans:混合式学习网络;zh-hant:混合式學習網絡}-]]（Hybrid Learning Network）
*[[聯想式學習網絡|-{zh-hans:联想式学习网络;zh-hant:聯想式學習網絡}-]]（Associate Learning Network）
*[[最適化學習網絡|-{zh-hans:最适化学习网络;zh-hant:最適化學習網絡}-]]（Optimization Application Network）

2.依網络架構（Connectionism）分類主要有：
*[[前馈神经网络|前馈神经网络]]（Feed Forward Network）
*[[循环神经网络|循环神经网络]]（Recurrent Network）
*[[强化式架構|强化式架構]]（Reinforcement Network）

==理论性质==
===计算能力===
[[多层感知器|多层感知器]]（Multilayer Perceptron，縮寫MLP）是一个通用的函数逼近器，由Cybenko定理证明。然而，证明不依赖特定的神经元数量或权重。Hava Siegelmann和Eduardo D. Sontag的工作证明了，一个具有有理数权重值的特定递归结构（与全精度实数权重值相对应）由有限个神经元和标准的线性关系构成的神经网络相当于一个通用[[图灵机|图灵机]]。<ref>{{Cite journal | title = Turing computability with neural nets | url = http://www.math.rutgers.edu/~sontag/FTP_DIR/aml-turing.pdf | year = 1991 | journal = Appl. Math. Lett. | pages = 77–80 | volume = 4 | issue = 6 | last1 = Siegelmann | first1 = H.T. | last2 = Sontag | first2 = E.D. | doi = 10.1016/0893-9659(91)90080-F | deadurl = yes | archiveurl = https://web.archive.org/web/20130502094857/http://www.math.rutgers.edu/~sontag/FTP_DIR/aml-turing.pdf | archivedate = 2013-05-02 | access-date = 2013-12-09 }}</ref>他们进一步表明，使用无理数权重值会产生一个超图灵机。

===容量===
人工神经网络模型有一个属性，称为“容量”，这大致相当于他们记住（'''而非'''正确分类）输入数据的能力。它与网络的参数、和结构有关。谷歌在研究<ref>{{cite conference |author=Chiyuan Zhang |title=UNDERSTANDING DEEP LEARNING REQUIRES RETHINKING GENERALIZATION |conference=International Conference on Learning Representations |year=2017}}</ref>中使用打乱标签的方法，来测试模型是否能否记住所有的输出。虽然很明显，这样模型在测试集上的表现几乎是随机猜测，但是模型能够记住所有训练集的输入数据，即记住他们被打乱后的标签。而记住有限的样本的信息（Expressivity），需要的模型的参数（权重）数量存在下限。

===收敛性===
模型并不总是收敛到唯一解，因为它取决于一些因素。首先，函数可能存在许多局部极小值，这取决于成本函数和模型。其次，在远离局部最小值时，优化方法可能无法保证收敛。第三，对大量的数据或参数，一些方法变得不切实际。在一般情况下，我们发现，理论保证的收敛不能成为实际应用的一个可靠的指南。
===综合统计===
在目标是创建一个普遍系统的应用程序中，过度训练的问题出现了。这出现在回旋或过度具体的系统中当网络的容量大大超过所需的自由参数。为了避免这个问题，有两个方向：第一个是使用交叉验证和类似的技术来检查过度训练的存在和选择最佳参数如最小化泛化误差。二是使用某种形式的正规化。这是一个在概率化（贝叶斯）框架里出现的概念，其中的正则化可以通过为简单模型选择一个较大的先验概率模型进行；而且在统计学习理论中，其目的是最大限度地减少了两个数量：“风险”和“结构风险”，相当于误差在训练集和由于过度拟合造成的预测误差。

== 相关 ==
* [[機器學習|機器學習]]
* [[深度學習|深度學習]]
* [[邏輯迴歸|邏輯迴歸]]
* [[線性迴歸|線性迴歸]]

== 參看 ==
* [[感知机|感知机]]
* [[多层感知器|多层感知器]]
* [[ER随机图|ER随机图]]

== 参见 ==
* [[人工智能|人工智能]]

== 参考文献 ==
{{Reflist|30em}}

==外部連結==
<!--
Do not post software links here. If you must, use the Neural Network Software article. Remember however that wikipedia is neither a link repository nor the place to promote products or show off your code.
-->
* [http://www.ibm.com/developerworks/cn/linux/other/l-neural/index.html 神經網絡介紹] {{Wayback|url=http://www.ibm.com/developerworks/cn/linux/other/l-neural/index.html |date=20200203235322 }}
* [http://tunedit.org/results?e=&d=UCI%2F&a=neural+rbf+perceptron&n= Performance comparison of neural network algorithms tested on UCI data sets] {{Wayback|url=http://tunedit.org/results?e=&d=UCI%2F&a=neural+rbf+perceptron&n= |date=20210419184735 }}
* [http://www.learnartificialneuralnetworks.com A close view to Artificial Neural Networks Algorithms] {{Wayback|url=http://www.learnartificialneuralnetworks.com/ |date=20180818083004 }}
* {{dmoz|Computers/Artificial_Intelligence/Neural_Networks|Neural Networks}}
* [http://www.dkriesel.com/en/science/neural_networks A Brief Introduction to Neural Networks (D. Kriesel)] - Illustrated, bilingual manuscript about artificial neural networks; Topics so far: Perceptrons, Backpropagation, Radial Basis Functions, Recurrent Neural Networks, Self Organizing Maps, Hopfield Networks.
* [http://www.msm.cam.ac.uk/phase-trans/abstracts/neural.review.html Neural Networks in Materials Science] {{Wayback|url=http://www.msm.cam.ac.uk/phase-trans/abstracts/neural.review.html |date=20150607101310 }}
* [http://www.ai-junkie.com/ann/evolved/nnt1.html A practical tutorial on Neural Networks] {{Wayback|url=http://www.ai-junkie.com/ann/evolved/nnt1.html |date=20201108091926 }}
* [https://web.archive.org/web/20100817033153/http://www.peltarion.com/doc/index.php?title=Applications_of_adaptive_systems Applications of neural networks]
* [http://4rdp.blogspot.tw/2013/10/artificial-neural-network.html XOR實例] {{Wayback|url=http://4rdp.blogspot.tw/2013/10/artificial-neural-network.html |date=20170729133549 }}
{{机器學習}}
{{Differentiable computing}}
{{Authority control}}
[[Category:資訊科學|Category:資訊科學]]
[[Category:人工智能|Category:人工智能]]
[[Category:机器学习|Category:机器学习]]
[[Category:计算机科学|Category:计算机科学]]
[[Category:神經網絡|]]