{{expert|time=2010-03-13|subject=计算机科学}}
{{NoteTA
| G1 = IT
| G2 = Math
| 1 = zh-cn: K-均值; zh-hant: K-平均;
}}
{{not|K-平均算法}}
{{机器学习导航栏}}

在[[模式识别|模式识别]]领域中，'''最近鄰居法'''（'''KNN'''算法，又譯'''K-近邻算法'''）是一种用于[[分类问题|分类]]和[[迴歸分析|回归]]的[[無母數統計|無母數統計]]方法<ref>{{cite journal |last=Altman |first=N. S. |title=An introduction to kernel and nearest-neighbor nonparametric regression |journal=The American Statistician |volume=46 |issue=3 |year=1992 |pages=175–185 |doi = 10.1080/00031305.1992.10475879}}</ref>。在这两种情况下，输入包含{{link-en|特徵空間(機器學習)|Feature Space|特徵空間}}（Feature Space）中的'''''k'''''个最接近的训练样本。

:* 在''k-NN分类''中，输出是一个分类族群。一个对象的分类是由其邻居的“多数表决”确定的，''k''个最近邻居（''k''为正[[整数|整数]]，通常较小）中最常见的分类决定了赋予该对象的类别。若''k'' = 1，则该对象的类别直接由最近的一个节点赋予。

:* 在''k-NN回归''中，输出是该对象的属性值。该值是其''k''个最近邻居的值的平均值。

最近鄰居法採用向量空間模型來分類，概念為相同類別的案例，彼此的相似度高，而可以藉由計算與已知類別案例之相似度，來評估未知類別案例可能的分類。

K-NN是一种{{le|基于实例的学习|instance-based learning}}，或者是局部近似和将所有计算推迟到分类之后的{{le|惰性学习|lazy learning}}。k-近邻算法是所有的[[机器学习|机器学习]]算法中最简单的之一。

无论是分类还是回归，衡量邻居的权重都非常有用，使较近邻居的权重比较远邻居的权重大。例如，一种常见的加权方案是给每个邻居权重赋值为1/ d，其中d是到邻居的距离。{{NoteTag|这个方案是一个[[线性插值|线性插值]]的推广。}}

邻居都取自一组已经正确分类（在回归的情况下，指属性值正确）的对象。虽然没要求明确的训练步骤，但这也可以当作是此算法的一个训练样本集。

k-近邻算法的缺点是对数据的局部结构非常敏感。

[[K-平均算法|K-平均算法]]也是流行的机器学习技术，其名稱和k-近邻算法相近，但兩者没有关系。[[标准化_(统计学)|数据标准化]]可以大大提高该算法的准确性<ref name=":0">{{Cite journal|last1=Piryonesi S. Madeh|last2=El-Diraby Tamer E.|date=2020-06-01|title=Role of Data Analytics in Infrastructure Asset Management: Overcoming Data Size and Quality Problems|journal=Journal of Transportation Engineering, Part B: Pavements|volume=146|issue=2|pages=04020022|doi=10.1061/JPEODX.0000175}}</ref><ref>{{Cite book|last=Hastie, Trevor.|title=The elements of statistical learning : data mining, inference, and prediction : with 200 full-color illustrations|date=2001|publisher=Springer|others=Tibshirani, Robert., Friedman, J. H. (Jerome H.)|isbn=0-387-95284-5|location=New York|oclc=46809224}}</ref>。 

==算法==
[[File:KnnClassification.svg|thumb]]

训练样本是多维特征空间向量，其中每个训练样本带有一个类别标签。算法的训练阶段只包含存储的[[特征向量|特征向量]]和训练样本的标签。

在分类阶段，''k''是一个用户定义的常数。一个没有类别标签的向量（查询或测试点）将被归类为最接近该点的''k''个样本点中最频繁使用的一类。

一般情况下，将[[欧氏距离|欧氏距离]]作为距离度量，但是这是只适用于[[概率分布#连续分布|连续变量]]。在文本分类这种离散变量情况下，另一个度量——'''重叠度量'''（或[[海明距离|海明距离]]）可以用来作为度量。例如对于基因表达微阵列数据，''k''-NN也与Pearson和Spearman相关系数结合起来使用。<ref>{{cite journal|last1=Jaskowiak|first1=P. A.|last2=Campello|first2=R. J. G. B.|title=Comparing Correlation Coefficients as Dissimilarity Measures for Cancer Classification in Gene Expression Data|publisher=Brazilian Symposium on Bioinformatics (BSB 2011)|pages=1–8|citeseerx=10.1.1.208.993}}</ref>通常情况下，如果运用一些特殊的算法来计算度量的话，''k''近邻分类精度可显著提高，如运用[[大间隔最近邻居|大间隔最近邻居]]或者[[邻里成分分析|邻里成分分析]]法。

“多数表决”分类会在类别分布偏斜时出现缺陷。也就是说，出现频率较多的样本将会主导测试点的预测结果，因为他们比较大可能出现在测试点的K邻域而测试点的属性又是通过''k''邻域内的样本计算出来的。<ref name="Coomans_Massart1982">{{Cite journal
 | doi = 10.1016/S0003-2670 (01)95359-0
 | author1 = D. Coomans
 | author2 = D.L. Massart
 | title = Alternative k-nearest neighbour rules in supervised pattern recognition : Part 1. k-Nearest neighbour classification by using alternative voting rules
 | journal = [[Analytica_Chimica_Acta|Analytica Chimica Acta]]
 | volume =  136
 | issue =
 | pages = 15–27
 | year = 1982
}}
</ref>解决这个缺点的方法之一是在进行分类时将样本到''k''个近邻点的距离考虑进去。''k''近邻点中每一个的分类（对于回归问题来说，是数值）都乘以与测试点之间距离的成反比的权重。另一种克服偏斜的方式是通过数据表示形式的抽象。例如，在[[自组织映射|自组织映射]]（SOM）中，每个节点是相似的点的一个集群的代表（中心），而与它们在原始训练数据的密度无关。''K''-NN可以应用到SOM中。

==参数选择==
如何选择一个最佳的K值取决于数据。一般情况下，在分类时较大的K值能够减小雜訊的影响，<ref>Everitt, B. S., Landau, S., Leese, M. and Stahl, D.（2011）Miscellaneous Clustering Methods, in Cluster Analysis, 5th Edition, John Wiley & Sons, Ltd, Chichester, UK.</ref>  但会使类别之间的界限变得模糊。一个较好的K值能通过各种启发式技术（见{{le|超参数优化|Hyperparameter optimization}}）来获取。

噪声和非相关性特征的存在，或特徵尺度与它们的重要性不一致会使K近邻算法的准确性严重降低。对于选取和缩放特征来改善分类已经作了很多研究。一个普遍的做法是利用[[进化算法|进化算法]]优化功能扩展<ref>Nigsch F, Bender A, van Buuren B, Tissen J, Nigsch E, Mitchell JB (2006). "Melting point prediction employing k-nearest neighbor algorithms and genetic parameter optimization". Journal of Chemical Information and Modeling 46 (6): 2412–2422. doi:10.1021/ci060149f. PMID 17125183.</ref>，还有一种较普遍的方法是利用训练样本的[[互信息|互信息]]进行选择特征。

在二元（两类）分类问题中，选取''k''为奇数有助于避免两个分类平票的情形。在此问题下，选取最佳经验''k''值的方法是[[自助法|自助法]]。<ref name="HPS2008">{{Cite journal
 | author = Hall P, Park BU, Samworth RJ
 | title = Choice of neighbor order in nearest-neighbor classification
 | url = https://archive.org/details/sim_annals-of-statistics_2008-10_36_5/page/2135
 | journal = [[Annals_of_Statistics|Annals of Statistics]]
 | volume = 36
 | issue = 5
 | pages = 2135–2152
 | year = 2008
 | doi = 10.1214/07-AOS537
}}
</ref>

==加权最近邻分类器==
{{mvar | k}}- 最近邻分类器可以被视为为{{mvar | k}}最近邻居分配权重<math> 1 / k </math>以及为所有其他邻居分配{{mvar | 0}}权重。这可以推广到加权最近邻分类器。也就是说，第{{mvar | i}}近的邻居被赋予权重<math> w_{ni}</math>，其中<math> \sum_{i = 1}^n w_{ni}= 1 </math>。关于加权最近邻分类器的强一致性的类似结果也成立。<ref name = "Stone">{{Cite journal | author = Stone C. J.
 | title = Consistent nonparametric regression
 | url = https://archive.org/details/sim_annals-of-statistics_1977-07_5_4/page/595
 | journal = [[Annals_of_Statistics|Annals of Statistics]]
 | volume = 5
 | pages = 595–620
 | year = 1977 | doi = 10.1214/aos/1176343886 | issue = 4
}}
</ref> 

设<math> C ^{wnn}_n </math>表示权重为<math> \{w_{ni}\}_{i = 1}^n </math>的加权最近邻分类器。根据类别分布的规律性条件，超额风险具有以下渐近展开<ref name="Samworth12"/>
:<math>\mathcal{R}_\mathcal{R}(C^{wnn}_{n}) - \mathcal{R}_{\mathcal{R}}(C^{Bayes}) = \left(B_1 s_n^2 + B_2 t_n^2\right) \{1+o(1)\},</math>
对常数 <math>B_1</math> and <math>B_2</math> 当 <math>s_n^2 = \sum_{i=1}^n w_{ni}^2</math> 并且 <math>t_n = n^{-2/d}\sum_{i=1}^n w_{ni}\{i^{1+2/d} - (i-1)^{1+2/d}\}</math>。

最佳加权方案<math> \{w_{ni}^* \}_{i = 1}^n </math>用于平衡上面显示中的两个项，如下所示：令 <math> k ^* = \lfloor B n ^{\frac 4 {d + 4}}\rfloor </math>，
:<math>w_{ni}^*  = \frac 1 {k^*} \left[1 + \frac d 2 - \frac d {2{k^*}^{2/d}} \{ i ^{1+2/d} - (i-1)^{1+2/d}\}\right] </math> 对 <math>i=1,2,\dots,k^*</math> 并且 
:<math> w^*_{ni} = 0 </math> 对 <math> i = k^*+1,\dots,n</math>.

利用最优权重，超额风险的渐近展开中的主项是<math>\mathcal{O}(n^{-\frac 4 {d+4}})</math>。当使用{{link-en|自助聚合|bootstrap aggregating|bagged 最近邻分类器}}时，类似的结果也是如此。

==属性==
原始朴素的算法通过計算测试点到存储样本点的距离是比较容易实现的，但它属于计算密集型的，特别是当训练样本集变大时，计算量也会跟着增大。多年来，许多用来减少不必要距离评价的近邻搜索算法已经被提出来。使用一种合适的近邻搜索算法能使K近邻算法的计算变得简单许多。

近邻算法具有较强的一致性结果。随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍<ref>Cover TM, Hart PE (1967). "Nearest neighbor pattern classification". IEEE Transactions on Information Theory 13 (1): 21–27. doi:10.1109/TIT.1967.1053964.</ref>。对于一些K值，K近邻保证错误率不会超过贝叶斯的。

== 决策边界 ==
近邻算法能用一种有效的方式隐含的计算[[决策边界|决策边界]]。另外，它也可以显式的计算决策边界，以及有效率的这样做计算，使得计算复杂度是边界复杂度的函数。<ref>Bremner D, Demaine E, Erickson J, Iacono J, Langerman S, Morin P, Toussaint G (2005). "Output-sensitive algorithms for computing nearest-neighbor decision boundaries". Discrete and Computational Geometry 33 (4): 593–604. doi:10.1007/s00454-004-1152-0</ref>

==连续变量估计==
K近邻算法也适用于连续变量估计，比如适用反距离加权平均多个K近邻点确定测试点的值。该算法的功能有：
#从目标区域抽样计算欧式或马氏距离；
#在交叉验证后的RMSE基础上选择启发式最优的K邻域；
#计算多元k-最近邻居的距离倒数加权平均。

==發展==
然而k最近鄰居法因為計算量相當的大，所以相當的耗時，Ko與Seo提出一演算法'''TCFP'''（'''t'''ext '''c'''ategorization using '''f'''eature '''p'''rojection），嘗試利用{{le|特徵投影法|feature projection}}來降低與分類無關的特徵對於系統的影響，並藉此提昇系統效能，其實驗結果顯示其分類效果與k最近鄰居法相近，但其運算所需時間僅需k最近鄰居法運算時間的五十分之一。

除了針對文件分類的效率，尚有研究針對如何促進''k''最近鄰居法在文件分類方面的效果，如Han等人於2002年嘗試利用[[貪心法|貪心法]]，針對文件分類實做可調整權重的k最近鄰居法'''WAkNN'''（'''w'''eighted '''a'''djusted '''k''' '''n'''earest '''n'''eighbor），以促進分類效果；而Li等人於2004年提出由於不同分類的文件本身有數量上有差異，因此也應該依照訓練集合中各種分類的文件數量，選取不同數目的最近鄰居，來參與分類。

== 参见 ==
{{colbegin}}
*[[最邻近搜索|最邻近搜索]]
*[[聚类分析|聚类分析]]
*[[数据挖掘|数据挖掘]]
*[[机器学习|机器学习]]
*[[模式识别|模式识别]]
*[[预测分析|预测分析]]
*[[维数灾难|维数灾难]]
*[[主成分分析|主成分分析]]
*[[最小哈希|最小哈希]]
{{colend}}

== 注释 ==
{{NoteFoot}}

== 參考文獻 ==
=== 引用 ===
{{Reflist
|refs =
<ref name = "Samworth12">{{Cite journal | author = Samworth R. J.
 | title = Optimal weighted nearest neighbour classifiers
 | url = https://archive.org/details/sim_annals-of-statistics_2012-10_40_5/page/2733
 | journal = [[Annals_of_Statistics|Annals of Statistics]]
 | volume = 40
 | issue = 5
 | pages = 2733–2763
 | year = 2012
 | doi = 10.1214/12-AOS1049
| arxiv = 1101.5783
 }}
</ref>
}}

=== 来源 ===
{{ReflistH}}
* E. H. Han, G. Karypis and V. Kumar, Text categorization using weight adjusted k-Nearest Neighbor classification, Pacific-Asia Conference on Knowledge Discovery and Data Mining, pp. 53–65, 2001.
* Y. J. Ko and Y. J. Seo, Text categorization using feature projections, Proceedings of the Nineteenth international conference on Computational linguistics, Volume 1, pp. 1–7, 2002.
* B. L. Li, Q. Lu and S. W. Yu, An adaptive k-nearest neighbor text categorization strategy, ACM Transactions on Asian Language Information Processing, Volume 3 , Issue 4, pp. 215–226, 2004.
}}
{{ReflistF}}

== 拓展阅读 ==
* [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.1422 When Is "Nearest Neighbor" Meaningful?]{{Wayback|url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.1422 |date=20090726012702 }}
* {{cite book |editor=[[Belur_V._Dasarathy|Belur V. Dasarathy]] |year=1991 |title=Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques |isbn=0-8186-8930-7}}
* {{cite book |title=Nearest-Neighbor Methods in Learning and Vision |editor=Shakhnarovish, Darrell, and Indyk |publisher=[[MIT_Press|MIT Press]] |year=2005 |isbn=0-262-19547-X}}
* {{cite journal |doi=10.1016/j.foreco.2004.02.049 |author=Mäkelä H Pekkarinen A |title=Estimation of forest stand volumes by Landsat TM imagery and stand-level field-inventory data |journal=[[Forest_Ecology_and_Management|Forest Ecology and Management]] |volume=196 |issue=2–3 |date=2004-07-26 |pages=245–255}}
* Fast k nearest neighbor search using [[GPU|GPU]]. In Proceedings of the CVPR Workshop on Computer Vision on GPU, Anchorage, Alaska, USA, June 2008. V. Garcia and E. Debreuve and M. Barlaud.
* [http://www.scholarpedia.org/article/K-nearest_neighbor Scholarpedia article on ''k''-NN] {{Wayback|url=http://www.scholarpedia.org/article/K-nearest_neighbor |date=20210128122727 }}
* [https://code.google.com/p/google-all-pairs-similarity-search/ google-all-pairs-similarity-search] {{Wayback|url=https://code.google.com/p/google-all-pairs-similarity-search/ |date=20160116173026 }}

[[Category:分類演算法|Category:分類演算法]]
[[Category:搜尋演算法|Category:搜尋演算法]]
[[Category:機器學習演算法|Category:機器學習演算法]]
[[Category:人工智能|Category:人工智能]]