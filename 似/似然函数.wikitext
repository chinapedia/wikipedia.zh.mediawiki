{{NoteTA
|T=zh-tw:概似函數;zh-cn:似然函数
|G1=Math
|1= zh-cn: 参数;zh-tw:母數;zh-hant:參數
}}
{{各地中文名
|名詞 = likelihood function
|cn = 似然函數
|tw = 概似函數
}}
{{贝叶斯统计}}
在[[数理统计学|数理统计学]]中，'''似然函数（'''{{Lang-en|likelihood function}}）是一种关于[[统计模型|统计模型]]中的[[母數|参数]]的[[函数|函数]]，表示模型参数中的'''似然性'''（{{Lang-en|likelihood}}）。似然函数在[[統計推論|統計推論]]中有重大作用，如在[[最大似然估计|最大似然估计]]和[[费雪信息|费雪信息]]之中的应用等等。文字意義上，“似然性”与“或然性”或“[[概率|概率]]”意思相近，都是指某种事件发生的可能性，但是在[[统计学|统计学]]中，“似然性”和“概率”（或然性）有明确的区分：概率，用于在已知一些参数的情況下，预测接下来在观测上所得到的结果；似然性，则是用于在已知某些观测所得到的结果时，对有关事物之性质的参数进行估值，也就是說已觀察到某事件後，對相關母數進行猜測。

在这种意义上，似然函数可以理解为[[条件概率|条件概率]]的逆反。在已知某个参数'''B'''时，事件'''A'''会发生的概率写作：
:<math>P(A \mid B) = \frac{P(A , B)}{P(B)} \!</math>

利用[[貝氏定理|-{zh-hk:貝葉斯定理;zh-hans:贝叶斯定理;zh-tw:貝氏定理;}-]]，
:<math>P(B \mid A) = \frac{P(A \mid B)\;P(B)}{P(A)} \!</math>

因此，我们可以反过来构造表示似然性的方法：已知有事件'''A'''发生，运用似然函数<math>\mathbb{L}(B \mid A)</math>，我们估计或猜測参数'''B'''的不同值的可能性。形式上，似然函数也是一种条件概率函数，但我们关注的[[变量|变量]]改变了：
:<math>b\mapsto P(A \mid B=b)  \!</math>

注意到这里并不要求似然函数满足归一性：<math>\sum_{b \in \mathcal{B}}P(A \mid B=b) = 1</math>。一个似然函数乘以一个正的常数之后仍然是似然函数。对所有<math>\alpha > 0</math>，都可以有似然函数：
:<math>L(b \mid A) = \alpha \; P(A \mid B=b) \!</math>

== 例子 ==
考虑投掷硬币的实验。通常来说，已知掷出一枚“公平的硬币”（即正面朝上和反面朝上的機率相同）時，正面（Head）朝上的概率为<math>p_H = 0.5</math>，我們可以此推論得知投掷若干次后出现各种结果的可能性。比如说，連續投两次都是正面朝上的概率是<math>0.25</math>。用条件概率表示，就是：

:<math>P(\mbox{HH} \mid p_H = 0.5) = 0.5^2 = 0.25</math>

其中<math>\mbox{H}</math>表示正面朝上。

在统计学中，我们更关心的是在''<u>已知一系列投掷的结果时，关于單獨投擲一次硬币时正面朝上的機率（即<math>p_H</math>）爲何</u>''。我們實際上是無法從一系列投擲的結果來逆推真實的<math>p_H</math>，但是我們可以推估<math>p_H</math>是某個值的可能性爲何。舉例來說，假設因爲這可能不是一枚真正“公平的硬幣”，所以我們不知道<math>p_H</math>是多少，也無法計算投擲三次硬幣其中兩次是正面的機率是多少。現在如果我們真的實際去擲了三次硬幣，結果其中兩次爲正面，那我們是否能夠依此次實驗逆推出<math>p_H</math>的資訊？如果無法逆推出真實的<math>p_H</math>，那我們有沒有辦法知道，譬如說<math>p_H=0.5</math>的可能性爲何？<math>p_H=0.6</math>的可能性又爲何？或甚至再更退一步，至少我們能不能知道<math>p_H=0.5</math>跟<math>p_H=0.6</math>哪一個''<u>比較</u>''有可能？

投擲一次硬幣，正面朝上的機率用<math>p_H</math>來代表，它就是我們這個例子的母數，而我們用事件<math>\mbox{A}</math>來代表投擲三次硬幣其中兩次是正面這個事實。使用聯合機率（{{Lang-en|joint probability}}）計算可知

:<math>P(\mbox{A} \mid p_H) = 3 \times p_H^2 \times (1-p_H)</math>

我們首先假設<math>p_H=0.5</math>，則看到三次投擲中兩次是正面的機率爲<math>P(\mbox{A} \mid p_H=0.5) = 0.375</math>。再來如果假設<math>p_H=0.6</math>，則看到三次投擲中兩次是正面的機率爲<math>P(\mbox{A} \mid p_H=0.6) = 0.432</math>。顯然地，如果<math>p_H=0.6</math>的話，我們看到兩個正面的機會比較高。所以當我們投擲了三次硬幣並且看到了兩次正面，即使我們無法知道實際<math>p_H</math>到底是多少，我們至少知道<math>p_H</math>是<math>0.6</math>的可能性比是<math>0.5</math>的可能性還要高。我們可以合理猜測，<math>p_H</math>比較可能是<math>0.6</math>而非<math>0.5</math>。

這裏我們就引進了概似性的概念：概似性代表某個母數爲特定值的可能性。從上面例子得知在已觀察到事件<math>\mbox{A}</math>的情況下，关于事件A的似然估计为

:<math>L(p_H \mid \mbox{A}) = P(\mbox{A} \mid p_H)</math>

其中<math>p_H</math>为我们所要确定的参数。所以當我們投擲硬幣三次，其中兩次是正面，則<math>p_H=0.5</math>的概似性是<math>L(p_H=0.5 \mid \mbox{A})=P(\mbox{A} \mid p_H=0.5) = 0.375</math>，而<math>p_H=0.6</math>的概似性是<math>L(p_H=0.6 \mid \mbox{A})=P(\mbox{A} \mid p_H=0.6) = 0.432</math>。注意，<math>L(p_H=0.5 \mid \mbox{A}) = 0.375</math>並不是說當已知<math>\mbox{A}</math>發生了，則<math>p_H</math>爲<math>0.5</math>的機率是<math>0.375</math>。''<u>概似性跟機率具有不同的意義。</u>''

若單獨看<math>0.375</math>這個數字或<math>0.432</math>這個數字是沒有意義的，因爲概似性並不是機率，並不是一定介於<math>0</math>到<math>1</math>之間，而所有可能的<math>p_H</math>的概似性加起來也不是<math>1</math>，所以單獨得知<math>L(p_H=0.5 \mid \mbox{A}) = 0.375</math>是沒有意義的。概似性是用在把各種可能的<math>p_H</math>值放在一起比較，來得知哪個<math>p_H</math>值的可能性比較高。而概似函數（在這個例子中，即<math>L(p_H \mid \mbox{A}) = 3 \times p_H^2 \times (1-p_H)</math>），除了用來計算概似性外，則是用來瞭解當母數<math>p_H</math>改變時，概似性怎麼變化，用來尋找最大可能性的<math>p_H</math>值會是多少。
[[File:likelihoodFunctionAfterHH.png|thumb]][[File:likelihoodFunctionAfterHHT.png|thumb]]圖1所示爲連續擲兩次硬幣都爲正面的情況下（即此節開頭的事件<math>\mbox{HH}</math>），<math>p_H</math>從<math>0</math>到<math>1</math>的概似性。我們可以看出最大概似性發生在<math>p_H=1</math>，所以當我們投擲硬幣兩次，兩次都正面時，我們可以猜說<math>p_H</math>最有可能是<math>1</math>（即使實際上<math>p_H</math>也許是<math>0.5</math>，但我們無法知道這件事)。圖2則爲投擲硬幣三次，其中兩次爲正面、一次爲反面的情況下，<math>p_H</math>從<math>0</math>到<math>1</math>的概似性。最大概似性發生在<math>p_H=\frac{2}{3}</math>。所以當我們擲了三次硬幣得到兩次正面，最合理的猜測應該是<math>p_H=\frac{2}{3}</math>（同理，也許實際上<math>p_H=0.5</math>，但我們無從得知，所以只能做“最合理”猜測）。<br />

我們可以得到一個結論：
       '''对同一个似然函数，其所代表的模型中，某项参数值具有多种可能，但如果存在一个参数值，使得概似函数值达到最大的话，那么这个值就是该项参数最为“合理”的参数值。'''

== 应用 ==
=== 最大似然估计 ===
{{main|最大似然估计}}
最大似然估计是似然函数最初也是最自然的应用。上文已经提到，似然函数取得最大值表示相应的参数能够使得统计模型最为合理。从这样一个想法出发，最大似然估计的做法是：首先选取似然函数（一般是[[概率密度函数|概率密度函数]]或[[概率质量函数|概率质量函数]]），整理之后求最大值点。实际应用中一般会取'''似然函数的对数作为求最大值的函数'''，这样求出的最大值点和直接求最大值点得到的结果是相同的。'''似然函数的最大值点不一定唯一，也不一定存在'''。与矩法估计比较，最大似然估计的精确度较高，信息损失较少，但计算量较大。

=== 似然比检验 ===
{{main|似然比检验}}
似然比检验是利用似然函数来检测某个假设（或限制）是否有效的一种检验。一般情况下，要检测某个附加的参数限制是否是正确的，可以将加入附加限制条件的较复杂模型的似然函数最大值与之前的较简单模型的似然函数最大值进行比较。如果参数限制是正确的，那么加入这样一个参数应当不会造成似然函数最大值的大幅变动。一般使用两者的比例来进行比较，這個比值是[[卡方分配|卡方分配]]。

[[尼曼-皮尔森引理|尼曼-皮尔森引理]]说明，似然比检验是所有具有同等[[显著性差异|显著性差异]]的检验中最有[[统计效力|统计效力]]的检验。

== 参考来源 ==
* {{cite article |title={{tsl|en|Francis Ysidro Edgeworth|}}, Statistician| author=[[史蒂芬·史蒂格勒|史蒂芬·史蒂格勒]]|journal= Journal of the Royal Statistical Society. Series A (General)| volume=141 |number= 3 |year= 1978 | pages =287–322 }} Stable URL: http://www.jstor.org/stable/2344804{{Wayback|url=http://www.jstor.org/stable/2344804 |date=20161010231201 }}
* {{cite book|author=[[Stephen_Stigler|Stephen Stigler]]|title=The History of Statistics: The Measurement of Uncertainty before 1900| isbn=0-674-40340-1|publisher=Harvard University Press}}
* {{cite book|author=[[Stephen_Stigler|Stephen Stigler]]|title=Statistics on the Table: The History of Statistical Concepts and Methods| isbn=0-674-83601-4|publisher=Harvard University Press}}
* {{cite article | title=On the History of Maximum Likelihood in Relation to Inverse Probability and Least Squares| author= Anders Hald |journal=Statistical Science|volume= 14| number=2 | pages =214–222 |date=1999年5月}} Stable URL: http://www.jstor.org/stable/2676741{{Wayback|url=http://www.jstor.org/stable/2676741 |date=20160305140229 }}
* {{cite book| author=Hald, A. |year=1998| title=A History of Mathematical Statistics from 1750 to 1930| publisher=Wiley| location=New York}}

[[Category:统计学|Category:统计学]]
[[Category:贝叶斯统计|Category:贝叶斯统计]]