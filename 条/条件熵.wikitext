{{NoteTA
|G1 = Communication
|G2 = IT
}}
在[[信息论|信息论]]中，'''条件熵'''描述了在已知第二个[[随机变量|随机变量]] <math>X</math> 的值的前提下，随机变量 <math>Y</math> 的信息熵还有多少。同其它的信息熵一样，条件熵也用Sh、[[纳特|nat]]、Hart等信息单位表示。'''基于 <math>X</math> 條件的 <math>Y</math> 的信息熵'''，用 <math>\Eta(Y|X)</math> 表示。

== 定义 ==
如果 <math>\Eta(Y|X=x)</math> 爲變數 <math>Y</math> 在變數 <math>X</math> 取特定值 <math>x</math> 條件下的熵，那麼 <math>\Eta(Y|X)</math> 就是 <math>\Eta(Y|X=x)</math> 在 <math>X</math> 取遍所有可能的 <math>x</math> 後取平均的結果。

给定随机变量 <math>X</math> 与 <math>Y</math>，定義域分別爲 <math>\mathcal X</math> 與 <math>\mathcal Y</math>，在給定 <math>X</math> 條件下 <math>Y</math> 的條件熵定義爲：<ref>{{cite book|first1=Thomas M.|last1=Cover|first2=Joy A.|last2=Thomas|title=Elements of information theory|year=1991|publisher=Wiley|location=New York|isbn=0-471-06259-6|edition=1st}}</ref>

:<math>
\begin{align}
\Eta(Y|X)\ &\equiv \sum_{x\in\mathcal X}\,p(x)\,\Eta(Y|X=x)\\
& =-\sum_{x\in\mathcal X} p(x)\sum_{y\in\mathcal Y}\,p(y|x)\,\log\, p(y|x)\\
& =-\sum_{x\in\mathcal X}\sum_{y\in\mathcal Y}\,p(x,y)\,\log\,p(y|x)\\
& =-\sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log\,p(y|x)\\
& =-\sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log \frac {p(x,y)} {p(x)}. \\
& = \sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log \frac {p(x)} {p(x,y)}. \\
\end{align}
</math>

''注意：'' 可以理解，對於確定的 ''c''>0，表達式 0 log 0 和 0 log (''c''/0) 應被認作等於零。<!-- because p(x,y) could still equal 0 even if p(x) != 0 and p(y) != 0. What about p(x,y)=p(x)=0? -->

當且僅當 <math>Y</math> 的值完全由 <math>X</math> 確定時，<math>\Eta(Y|X)=0</math>。相反，當且僅當 <math>Y</math> 和 <math>X</math> 爲[[統計獨立|獨立隨機變數]]時<math>\Eta(Y|X) = \Eta(Y)</math>。

== 链式法则 ==

假設兩個隨機變數 ''X'' 和 ''Y'' 確定的組合系統的[[聯合熵|聯合熵]]爲 <math>\Eta(X,Y)</math>，即我們需要 <math>\Eta(X,Y)</math> bit的信息來描述它的確切狀態。
現在，若我們先學習 <math>X</math> 的值，我們得到了 <math>\Eta(X)</math> bits的信息。
一旦知道了 <math>X</math>，我們只需 <math>\Eta(X,Y)-\Eta(X)</math> bits來描述整個系統的狀態。
這個量正是 <math>\Eta(Y|X)</math>，它給出了條件熵的''链式法则''：
:<math>\Eta(Y|X)\,=\,\Eta(X,Y)-\Eta(X) \, .</math>

链式法则接著上面條件熵的定義：

:<math>
\begin{align}
\Eta(Y|X)&= \sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log \frac {p(x)} {p(x,y)}\\
&= -\sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log\,p(x,y) + \sum_{x\in\mathcal X, y\in\mathcal Y}p(x,y)\log\,p(x)\\
&=  \Eta(X,Y) + \sum_{x \in \mathcal X} p(x)\log\,p(x)\\
&=  \Eta(X,Y) - \Eta(X).
\end{align}
</math>

== 貝葉斯規則 ==
條件熵的{{Link-en|貝葉斯規則|Bayes' rule}}表述爲
:<math>H(Y|X) \,=\, H(X|Y) - H(X) + H(Y) \,.</math>

''證明.'' <math>H(Y|X) = H(X,Y) - H(X)</math> and <math>H(X|Y) = H(Y,X) - H(Y)</math>。對稱性意味著 <math>H(X,Y) = H(Y,X)</math>。將兩式相減即爲貝葉斯規則。

== 推广到量子理论 ==
在[[量子信息|量子信息]]论中，条件熵都概括为[[量子条件熵|量子条件熵]]。

== 參考文獻 ==
{{Reflist}}

[[Category:信息學熵|Category:信息學熵]]
[[Category:信息论|Category:信息论]]