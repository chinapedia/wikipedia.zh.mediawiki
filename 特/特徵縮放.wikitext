'''特徵縮放'''是用來統一資料中的自變項或特徵範圍的方法，在[[資料處理|資料處理]]中，通常會被使用在資料前處理這個步驟。

==動機==
因為在原始的資料中，各變數的範圍大不相同。對於某些[[機器學習|機器學習]]的演算法，若沒有做過標準化，目標函數會無法適當的運作。舉例來說，多數的[[分类问题|分類器]]利用兩點間的距離計算兩點的差異，若其中一
個特徵具有非常廣的範圍，那兩點間的差異就會被該特徵左右，因此，所有的特徵都該被標準化，這樣才能大略的使各特徵依比例影響距離。

另外一個做特徵縮放的理由是他能使加速[[梯度下降法|梯度下降法]]的收斂。

==方法==

===重新縮放===
最簡單的方式是重新縮放特徵的範圍到[0, 1]或[-1, 1]， 依據原始的資料選擇目標範圍，通式如下：

                                    <math>x' = \frac{x - \text{min}(x)}{\text{max}(x)-\text{min}(x)}</math>

<math>x</math>是原始的值，<math>x'</math>是被標準化後的值。例如，假設我們有學生的體重資料，範圍落在[160磅, 200磅]，為了重新縮放這個資料，我們會先將每個學生的體重減掉160，接著除與40(最大體重與最小體重的差值)

===標準化===
在[[機器學習|機器學習]]中，我們可能要處理不同種類的資料，例如，音訊和圖片上的像素值，這些資料可能是高維度的，資料標準化後會使每個特徵中的數值平均變為0(將每個特徵的值都減掉原始資料中該特徵的平均)、標準差變為1，這個方法被廣泛的使用在許多機器學習演算法中(例如：[[支持向量機|支持向量機]]、[[邏輯迴歸|邏輯迴歸]]和[[類神經網路|類神經網路]])。

===縮放至單位長度===
该方法也在机器学习中常用。缩放特征向量的分量，将每个分量除以向量的[[欧几里得距离|欧几里得距离]]，使整个向量的长度为1。
:<math>x' = \frac{x}{\left\|{x}\right\|}</math>

== 應用 ==
在隨機[[梯度下降法|梯度下降法]]中， 特徵縮放有時能加速其收斂速度。而在支持向量機中，他可以使其花費更少時間找到支持向量，特徵縮放會改變支持向量機的結果。

==參考==
{{reflist}}

* S. Aksoy and R. Haralick, “Feature normalization and likelihood-based similarity measures for image retrieval,” Pattern Recognit. Lett., Special Issue on Image and Video Retrieval, 2000 http://www.cs.bilkent.edu.tr/~saksoy/papers/prletters01_likelihood.pdf {{Wayback|url=http://www.cs.bilkent.edu.tr/~saksoy/papers/prletters01_likelihood.pdf |date=20180921011432 }}

* S. Tsakalidis, V. Doumpiotis & W. Byrne, “Discriminative Linear Transforms for Feature Normalization and Speaker Adaptation in HMM Estimation”, Proc. ICSLP'02, Denver. http://malach.umiacs.umd.edu/pubs/VD_05_Discrim_linear.pdf {{Wayback|url=http://malach.umiacs.umd.edu/pubs/VD_05_Discrim_linear.pdf |date=20170809043420 }}

* Liefeng Bo, Ling Wang, and Licheng Jiao, “Feature Scaling for Kernel Fisher Discriminant Analysis Using Leave-one-out Cross Validation”, Neural Computation (NECO), vol. 18(4), pp. 961–978, 2006 http://www.cs.washington.edu/homes/lfb/paper/nc06.pdf {{Wayback|url=http://www.cs.washington.edu/homes/lfb/paper/nc06.pdf |date=20110401075633 }}

* A. Stolcke, S. Kajarekar, and L. Ferrer, “Nonparametric feature normalization for SVM-based speaker verification,” in Proc. ICASSP, Las Vegas, Apr. 2008. http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=4517925

* E. Youn, M. K. Jeong, “Class dependent feature scaling method using naive Bayes classifier for text datamining” Pattern Recognition Letters, 2009. http://www.sciencedirect.com/science/article/pii/S0167865508003553 {{Wayback|url=http://www.sciencedirect.com/science/article/pii/S0167865508003553 |date=20150924164044 }}

* S. Theodoridis, K. Koutroumbas. (2008) “Pattern Recognition”, Academic Press, 4 edition, ISBN 978-1-59749-272-0

== 外部連結==
*[http://openclassroom.stanford.edu/MainFolder/VideoPage.php?course=MachineLearning&video=03.1-LinearRegressionII-FeatureScaling&speed=100/ Lecture by Andrew Ng on feature scaling] {{Wayback|url=http://openclassroom.stanford.edu/MainFolder/VideoPage.php?course=MachineLearning&video=03.1-LinearRegressionII-FeatureScaling&speed=100%2F |date=20170314193728 }}

[[Category:機器學習|Category:機器學習]]