{{NoteTA
| G1 = Math
}}

'''线性判别分析''' ('''LDA''')是对'''费舍尔的线性鉴别方法'''的归纳，这种方法使用[[统计学|统计学]]，[[模式识别|模式识别]]和[[机器学习|机器学习]]方法，试图找到两类物体或事件的特征的一个[[线性组合|线性组合]]，以能够特征化或区分它们。所得的组合可用来作为一个[[线性分类器|线性分类器]]，或者，更常见的是，为后续的[[分类问题|分类]]做降维处理。

LDA与[[變異數分析|變異數分析]]（ANOVA）和[[迴归分析|迴归分析]]紧密相关，这两种分析方法也试图透过一些特征或测量值的线性组合来表示一个因变量。<ref name="Fisher:1936"><cite class="citation journal">[[羅納德·費雪|Fisher, R. A.]] (1936). </cite></ref><ref name="McLachlan:2004"><cite class="citation book">McLachlan, G. J. (2004). </cite></ref> 然而，變異數分析使用类别型的自变量和连续型的因变量，而判别分析則使用连续型自变量和类别型因变量（即类标签）。<ref>Analyzing Quantitative Data: An Introduction for Social Researchers, Debra Wetcher-Hendricks, p.288</ref> [[逻辑迴归|逻辑迴归]]和概率迴归比變異數分析更类似于LDA，因为他们也是用连续型自变量来解释类别型因变量。LDA的基本假设是自变量是常态分布的，当这一假设无法满足时，在实际应用中更倾向于用上述的其他方法。

LDA也与[[主成分分析|主成分分析]]（PCA）和[[因素分析|因素分析]]紧密相关，它们都在寻找最佳解释数据的变量线性组合。<ref name="Martinez:2001"><cite class="citation journal">Martinez, A. M.; Kak, A. C. (2001). </cite></ref> LDA明确地尝试在不同数据类之间建立模型，而PCA則不考虑类别上的不同（只是在保留大部分訊息的前提下降低维度數），因素分析則是根据相異處而非相同處来建立特征组合。判别分析跟因素分析的差異还在于，它不是一个相互依存技术：即必须区分出自变量和因变量（也称为准则变量）的不同。

當自变量每一次的观察测量值都是连续量的时候，LDA能發揮作用。如果是处理类别型自变量，与LDA相对应的技术称为判别反应分析。<ref name="Abdi 2007">Abdi, H. (2007) [http://www.utdallas.edu/~herve/Abdi-DCA2007-pretty.pdf "Discriminant correspondence analysis."] {{Wayback|url=http://www.utdallas.edu/~herve/Abdi-DCA2007-pretty.pdf |date=20170921222033 }}</ref><ref name="Perriere 2003">Perriere, G.; &  Thioulouse, J. (2003). </ref>

== 二类LDA ==
考虑在已知类別 ''y'' 中每一个对象或事件的一组观察量 <math> { \vec x } </math> （也称为特征、属性、变量或测量）；这一组样本称为[[训练集、验证集和测试集|训练集]]。分类问题即為在给定观察量 <math> \vec x </math> 为 ''y'' 类的情況下找一个良好的预测器，使得任意具有相同分布的样品（未必来自训练集合）都能被正确地判断。<ref name="Venables:2002">{{cite book |title=Modern Applied Statistics with S |first1=W. N. |last1=Venables |first2=B. D. |last2=Ripley |authorlink2=Brian Ripley |publisher=Springer Verlag |isbn=0-387-95457-0 |year=2002 |edition=4th}}
</ref>{{rp|338}}

LDA 的方法是，透过假设条件[[機率密度函數|概率密度函数]] <math>p(\vec x|y=0)</math> 和 <math>p(\vec x|y=1)</math> 都是[[正态分布|正态分布]]，分别具有均值和[[协方差|协方差]]<math>\left(\vec \mu_0, \Sigma_0\right)</math> 和 <math>\left(\vec \mu_1, \Sigma_1\right)</math>。根据这一假设，贝叶斯最佳解决方案是如果预测点的可能性比率之对数值低于某一阈值 T，其就属于第二类：
: <math> (\vec x- \vec \mu_0)^T \Sigma_0^{-1} ( \vec x- \vec \mu_0) + \ln|\Sigma_0| - (\vec x- \vec \mu_1)^T \Sigma_1^{-1} ( \vec x- \vec \mu_1) - \ln|\Sigma_1| \ > \ T </math>
如果没有任何进一步的假设，所得到的分类器称为QDA（[[二次分類器|二次判别分析]]）。

相反地，LDA做出额外简化的同方差性假设（即不同类之间的协方差相同，<math>\Sigma_0 = \Sigma_1 = \Sigma</math>），并且协方差是满[[秩_(线性代数)|秩]]的。在这种情况下，可以消掉一些项：
: <math> {\vec x}^T \Sigma_0^{-1} \vec x = {\vec x}^T \Sigma_1^{-1} \vec x</math>
: <math>{\vec x}^T {\Sigma_i}^{-1} \vec{\mu_i} = {\vec{\mu_i}}^T{\Sigma_i}^{-1} \vec x</math>  因为<math>\Sigma_i</math>是[[埃尔米特矩阵|埃尔米特矩阵]]
如此一來，上面的判断准则就变成了判断[[点积|点积]]的阈值是否大於某常數 ''c''
: <math> \vec w \cdot \vec x > c </math>
而
: <math>\vec w = \Sigma^{-1} (\vec \mu_1 - \vec \mu_0)</math>
: <math> c = \frac{1}{2}(T-{\vec{\mu_0}}^T \Sigma_0^{-1} {\vec{\mu_0}}+{\vec{\mu_1}}^T \Sigma_1^{-1} {\vec{\mu_1}})</math>
这意味着，一个输入 <math> \vec x </math> 属于类 ''y'' 的标准就纯粹成为一个已知观察值的线性组合的函数。

从几何学的角度来看这个结论通常会有些帮助：判断一个输入 <math> \vec x </math> 是否为类 ''y'' 的标准就是一个将多维空间上的点 <math> \vec x </math> 投影到向量 <math> \vec w </math>（我们仅仅考虑其方向）的函数。换句话说，如果相对应的 <math> \vec x </math> 位于一个垂直于 <math> \vec w </math> 的超平面的某一侧，那么观察值就属于''y''类。平面的位置由阈值c来决定。

== ''k''类正则判别分析 ==
正则判别分析法（CDA）寻找最优区分类别的坐标轴（''k''-1个[[正则坐标|正则坐标]]，''k''为类别的数量）。 这些线性函数是不相关的，实际上，它们通过''n''维数据云定义了一个最优化的''k-1''个空间，能够最优的区分''k''个类（通过其在空间的投影）。详细请参见下面的“多类LDA”。

== 费舍尔的线性判别 ==
''费舍尔的线性判别''和''LDA''的叫法往往是可以互换使用，尽管[[羅納德·費雪|费舍尔]]最早的文章<ref name="Fisher:1936"><cite class="citation journal">[[羅納德·費雪|Fisher, R. A.]] (1936). </cite></ref>实际上描述了一个稍微不同的判别，他没有作出一些类似LDA所作的假设，比如正态分布的各类或者相等的类协方差。

假设观察的两个类分别有均值<math> \vec \mu_0, \vec \mu_1 </math>和协方差<math>\Sigma_0,\Sigma_1 </math>。那么特征的线性组合<math> \vec w \cdot \vec x </math>将具有均值<math> \vec w \cdot \vec \mu_i </math>和协方差<math> \vec w^T \Sigma_i \vec w </math> 其中<math> i=0,1 </math>。费舍尔把区分这两类分布的规则为类间方差与类内方差的比率：

:<math>S=\frac{\sigma_{\text{between}}^2}{\sigma_{\text{within}}^2}= \frac{(\vec w \cdot \vec \mu_1 - \vec w \cdot \vec \mu_0)^2}{\vec w^T \Sigma_1 \vec w + \vec w^T \Sigma_0 \vec w} = \frac{(\vec w \cdot (\vec \mu_1 - \vec \mu_0))^2}{\vec w^T (\Sigma_0+\Sigma_1) \vec w} </math>

从某种意义上说，这一方法是测量类标签的[[信噪比|信噪比]]。它可以显示，当满足如下条件时，会产生最大的区分

:<math> \vec w \propto (\Sigma_0+\Sigma_1)^{-1}(\vec \mu_1 - \vec \mu_0) </math>

当LDA的假设满足时，上述方程式就是LDA。

一定要注意的矢量<math>\vec w</math>是判别[[超平面|超平面]]的[[法线|法线]]。 以二维空间为例，能区分两类的最优线就是与<math>\vec w</math>。

一般来说，要判别数据点投影到<math>\vec w</math>；然后从一维分布中选取区分数据的最佳阈值。选取阈值没有通用的规则。然而，如果两类的投影点显示出近似的分布，那么选取两个均值的投影<math>\vec w \cdot \vec \mu_0 </math>和<math>\vec w \cdot \vec \mu_1 </math>之间的超平面是比较合适的。在这种情况下，<math> \vec w \cdot \vec x > c </math>条件下的阈值参数c可以明确表达为：

:<math> c = \vec w \cdot \frac12 (\vec \mu_0 + \vec \mu_1) = \frac{1}{2} \vec\mu_1^T \Sigma^{-1} \vec\mu_1 - \frac{1}{2} \vec\mu_0^T \Sigma^{-1} \vec\mu_0 </math>.

[[大津算法|大津算法]]与费舍尔的线性判别有些相关，它的建立是通过最优选取黑白间的阈值，来二元化灰度图像中像素的直方图，既能使黑白像素两类间的方差最大化，又能使两类内各自的方差最小化。

== 多类LDA ==
当出现超过两类的情况时，可以使用由费舍尔判别派生出的分析方法，它延伸为寻找一个保留了所有类的变化性的子空间。这是由 C.R.Rao 总结出来的。<ref name="Rao:1948">{{cite journal |last=Rao |first=R. C. |authorlink=Calyampudi Radhakrishna Rao |title=The utilization of multiple measurements in problems of biological classification |journal=Journal of the Royal Statistical Society, Series B |volume=10 |issue=2 |pages=159–203 |year=1948 |jstor=2983775}}</ref> 假设，C个类中每一个类都有均值<math> \mu_i </math>和相同的协方差<math> \Sigma </math>。 那么，类间的变化可以通过类均值的协方差来定义
:<math> \Sigma_b = \frac{1}{C} \sum_{i=1}^C (\mu_i-\mu) (\mu_i-\mu)^T </math>
这里<math> \mu </math>是各类均值的均值。在<math> \vec w </math>的方向区分类有下式给出
:<math> S = \frac{\vec w^T \Sigma_b \vec w}{\vec w^T \Sigma \vec w} </math>

这意味着如果<math> \vec w </math>是<math> \Sigma^{-1} \Sigma_b </math>的特征向量，等同于用对应的特征值进行分类。

如果<math> \Sigma^{-1} \Sigma_b </math>是可对角化矩阵，特征之间的变化性就会被保留在''C-1''个最大特征值对应的特征向量构成子空间内（因为 <math> \Sigma_b </math>最大可能的秩是''C'' − 1）。与PCA相同，这些特征向量的主要用途是缩减特征数。较小的特征值对应的特征向量对训练数据的选择非常敏感，所以常常需要使用下段中描述的调整方法。

如果只是为了分类，而不需要[[降维|降维]]，还有一些替代技术可用。例如，多个类本身就是分离的，可用费舍尔判别或者LDA对每个区进行分类。一个常见的例子是“一个对其余”，指的是从一个类里出来的点，放入一组里，其他的归入另一组，这就可以用LDA。这将导致C类分类器，它的结果是组合在一起的。另一个常见的方法就是配对分类，给每一对类别创建一个分类器（总共有''C''(''C'' − 1)/2个分类器)，再将每个分类器的结果综合得到最终结果。

== 增量LDA ==
要实现典型的LDA技术前提是所有的样本都必须提前准备完毕。但有些情况下，没有现成的完整数据集或者输入观察数据是流的形式。这样，就要求LDA的特征提取有能力随着观察新样本的增加而更新LDA的特征，而不是在整个数据集上运行算法。例如，在移动机器人或实时脸部识别等实时应用中，提取的LDA特征能随着新观察值实时更新是非常重要的。这种能够通过简单观察新样本来更新LDA特征的技术就叫做增量LDA算法，在过去二十年里，它已经被广泛的研究过。<ref name=":0">{{Cite journal|title = Fast incremental LDA feature extraction|url = http://www.sciencedirect.com/science/article/pii/S0031320314005214|journal = Pattern Recognition|date = 2015-06-01|pages = 1999–2012|volume = 48|issue = 6|doi = 10.1016/j.patcog.2014.12.012|first = Youness|last = Aliyari Ghassabeh|first2 = Frank|last2 = Rudzicz|first3 = Hamid Abrishami|last3 = Moghaddam}}</ref> Catterjee和Roychowdhury提出了一种增量自组织LDA算法来更新LDA特征。<ref name=":1">{{Cite journal|title = On self-organizing algorithms and networks for class-separability features|url = http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=572105&url=http%253A%252F%252Fieeexplore.ieee.org%252Fiel4%252F72%252F12383%252F00572105|journal = IEEE Transactions on Neural Networks|date = 1997-05-01|issn = 1045-9227|pages = 663–678|volume = 8|issue = 3|doi = 10.1109/72.572105|first = C.|last = Chatterjee|first2 = V.P.|last2 = Roychowdhury}}</ref> 另外，Demir和Ozmehmet提出利用误差改正和[[赫布理论|赫布]]学习规则的线上本地学习算法来更新LDA特征。<ref>{{Cite journal|title = Online Local Learning Algorithms for Linear Discriminant Analysis|url = http://dx.doi.org/10.1016/j.patrec.2004.08.005|journal = Pattern Recogn. Lett.|date = 2005-03-01|issn = 0167-8655|pages = 421–431|volume = 26|issue = 4|doi = 10.1016/j.patrec.2004.08.005|first = G. K.|last = Demir|first2 = K.|last2 = Ozmehmet}}</ref> 最后，Aliyari等人提供了快速增量LDA算法。<ref name=":0" />

== 实际使用 ==
在实际中，类的均值和协方差都是未知的。然而，它们可以从训练集合中估算出来。[[最大似然估计|最大似然估计]]和[[最大后验概率|最大后验概率]]估计都可以用来替代上述方程里面的相应值。虽然协方差的估值在某种意义上是最优的，即使对类的正态分布假设是正确的，也并不能表明用这些估值替换得到的判别是最优的。

另一个应用LDA和费舍尔判别的复杂情况是，每个样品测量值的数量（即每个数据向量的维度）超过类中的样品数量。<ref name="Martinez:2001" /> 在这种情况下，协方差估值不是满秩，因此不能取逆。有几种方法可以应对这种情况。一种是在上述公式中使用[[广义逆阵|广义逆阵]]替代通常的逆矩阵。不过，先把问题投影到<math> \Sigma_b </math>构成的子空间上可以得到更好的数字稳定性。<ref>Yu, H.; Yang, J. (2001). "A direct LDA algorithm for high-dimensional data — with application to face recognition", ''Pattern Recognition'', 34 (10), 2067–2069</ref> 另一个处理小样本的策略是，用协方差矩阵的收缩估算，可以表达为以下数学式
:<math> \Sigma = (1-\lambda) \Sigma+\lambda I\,</math>
这里<math> I </math>的[[单位矩阵|单位矩阵]]，<math> \lambda </math>是的''收<span>缩度</span>''或''归一参数。''这产生了归一判别分析<ref name="Friedman:2001">{{cite journal |last=Friedman |first=J. H. |title=Regularized Discriminant Analysis |journal=[[Journal_of_the_American_Statistical_Association|Journal of the American Statistical Association]] |volume=84 |issue=405 |pages=165–175 |year=1989 |url=http://www.slac.stanford.edu/cgi-wrap/getdoc/slac-pub-4389.pdf |doi=10.2307/2289860 |publisher=American Statistical Association |jstor=2289860 |mr=0999675 |access-date=2016-11-29 |archive-date=2020-06-06 |archive-url=https://web.archive.org/web/20200606084547/https://www.slac.stanford.edu/cgi-bin/getdoc/slac-pub-4389.pdf |dead-url=no }}</ref> 或收缩判别分析<ref>Ahdesmäki, M.; Strimmer K. (2010) [http://projecteuclid.org/euclid.aoas/1273584465 "Feature selection in omics prediction problems using cat scores and false nondiscovery rate control"] {{Wayback|url=http://projecteuclid.org/euclid.aoas/1273584465 |date=20170716191016 }}, ''Annals of Applied Statistics'', 4 (1), 503–519.</ref>的框架。

此外，在许多实际情形中线性判别是不合适的。 LDA和费舍尔判别可以通过核函数扩展到非线性分类。原始的观察值有效的映射到一个高维的非线性空间里。在这个非线性空间里的线性分类，就相当于在原始空间里面的非线性分类。最常用的例子就是核函数费舍尔判别。

LDA可以推广到多类判别分析，其中''c''变为一个具有''N''个可能状态，而不是两个状态的类别变量。类似地，如果各类的条件概率密度 <math>p(\vec x\mid c=i)</math>都是正态分布有共同的协方差，那么<math>P(c\mid\vec x)</math>的充分统计量就是协方差逆矩阵在''N''个均值构成的子平面上仿射的''N''个投影值。这些投影值可以通过解广义特征值问题来找到，分子是以均值为样本构成的协方差矩阵，分母是共有协方差矩阵。详情参见上述“多类LDA”。

== 应用领域 ==
除了下面给出的实例，LDA应用于[[市场定位|市场定位]]和产品管理。

=== 破产预测 ===
在基于[[财务比率|财务比率]]和其他金融变量的破产预测中，LDA是第一个用来系统解释公司进入破产或存活的统计学工具。尽管受到财务比率不遵守LDA正态分布假设的限制，Edward Altman的1968年模型仍然是实际应用的领先者。

=== 脸部识别 ===
在计算机化的脸部识别中，每一张脸由大量像素值表达。 LDA在这里的主要作用是把特征的数量降到可管理的数量后再进行分类。每一个新的维度都是模板里像素值的线性组合。使用费舍尔线性判别得到的线性组合称为''费舍尔脸''，而通过主成分分析（PCA）得到称为''特征脸''。

=== 市场营销 ===
在[[市场营销|市场营销]]，判别分析曾经常用于通过市场调查或其他数据收集手段，找出那些能区分不同客户或产品类型的多个因素。如今用的更多的是[[逻辑回归|逻辑回归]]或其他方法。在市场营销中使用判别分析的具体步骤如下描述：
# 制定问题并收集数据 -- 识别消费者评估产品的一些显著属性 一 用定量市场研究技术（例如市场调查）从潜在消费者中收集关于他们对产品所有属性的评分数据。数据收集阶段通常是由专业的市场调查公司完成的。调查人员选择一系列属性，请参与者对他们给出1到5（或者1到7，1到10）的评分。通常选5到20个属性。通常包括：易用度，重量，准确度，耐用性，色彩度，价格，或尺寸。根据研究的产品选择不同的属性。在调查中对所有的产品都问相同的问题。多个产品的数据编码后，输入统计分析程序，比如[[R|R]]，[[SPSS|SPSS]]或[[SAS|SAS]]。（这一步与因子分析法一样）。
# 估计的判别函数的系数并确定统计显著水平和有效性 -- 选择适当的判别分析方法。直接的方法涉及估计判别函数，以便所有的预测器同时被评估。逐步的方法顺序进入的预测器。当因变量只有两类或状态时，适用两组的方法。因变量多于三个是，用多类判别方法。在SPSS或者SAS里的F统计包，用Wilks's Lambda检验显著水平。最常用的有效性检验方法是，将样本分为两组：估计/分析样本和验证样本。估计样本用来构建判别函数，验证样本构建分类矩阵，其中包括正确分类和不正确分类的数量。正确分类案例的百分比称为命中率。
# 将结果画在二维图里，定义维度，并解释结果。统计程序（或相关模块）会将结果做出图。图中包括每个产品（通常为二维空间）。每个产品之间的距离表明他们不同的程度。研究者必须给每个维度注明标签。整个过程需要主观判断，非常具有挑战性。参见感知映射。

=== 生物医学研究 ===
判别分析在医学的主要应用是评估患者的严重程度和对疾病结果的预后判断。比如，在回顾分析中，根据患者的病情分为几组：轻微，中度和严重。通过对临床和实验室分析结果的研究，揭示被研究组中哪些变量是统计上不同的。使用这些变量建立判别函数，帮助将未来的患者客观的划分到轻微，中度和严重的类别里。

在生物学中，类似的原则被用以划分和定义不同的生物对象。例如，用傅立叶变换红外光谱定义沙门氏菌的噬菌体类别，<ref>Preisner O, Guiomar R, Machado J, Menezes JC, Lopes JA. Application of Fourier transform infrared spectroscopy and chemometrics for differentiation of Salmonella enterica serovar Enteritidis phage types. Appl Environ Microbiol. 2010;76(11):3538–3544.</ref> 检测大肠杆菌的动物来源以研究它的毒力因子<ref>David DE, Lynne AM, Han J, Foley SL. Evaluation of virulence factor profiling in the characterization of veterinary Escherichia coli isolates. Appl Environ Microbiol. 2010;76(22):7509–7513.</ref> 等。

=== 地球科学 ===
这种方法可用于区分蚀变带。例如，当很多带的不同数据都现成时，判别分析可以从数据中找到模式并有效的对它分类。<ref>Tahmasebi, P., Hezarkhani, A., & Mortazavi, M. (2010). [http://ajbasweb.com/old/ajbas/2010/564-576.pdf Application of discriminant analysis for alteration separation] {{Wayback|url=http://ajbasweb.com/old/ajbas/2010/564-576.pdf |date=20150526070648 }}; sungun copper deposit, East Azerbaijan, Iran. Australian Journal of Basic and Applied Sciences, 6(4), 564–576.</ref>

== 参考 ==
* [[数据挖掘|数据挖掘]]
* [[决策树学习|决定树学习]]
* [[Factor_analysis|因子分析]]
* 核函数费舍尔判别分析
* [[邏輯迴歸|逻辑回归]]
* [[多维标度|多维标度]]
* [[模式识别|模式识别]]
* [[感知器|感知]]
* [[Preference_regression|偏好回归]]
* [[二次分類器|二次分類器]]

== 引用 ==
<div class="reflist columns references-column-count references-column-count-3" style="-moz-column-count: 3; -webkit-column-count: 3; column-count: 3; list-style-type: decimal;">
<references /></div>

== 扩展阅读 ==
* <cite class="citation book">Duda, R. O.; Hart, P. E.; Stork, D. H. (2000). ''Pattern Classification'' (2nd ed.). Wiley Interscience. [[国际标准书号|ISBN]] 0-471-05669-3. </cite><cite class="citation book">[[數學評論|MR]] [//www.ams.org/mathscinet-getitem?mr=1802993 1802993].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALinear+discriminant+analysis&rft.aufirst=R.+O.&rft.au=Hart%2C+P.+E.&rft.aulast=Duda&rft.au=Stork%2C+D.+H.&rft.btitle=Pattern+Classification&rft.date=2000&rft.edition=2nd&rft.genre=book&rft_id=%2F%2Fwww.ams.org%2Fmathscinet-getitem%3Fmr%3D1802993&rft.isbn=0-471-05669-3&rft.pub=Wiley+Interscience&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook"> </span>
* <cite class="citation book">Hilbe, J. M. (2009). ''Logistic Regression Models''. Chapman & Hall/CRC Press. </cite><cite class="citation book">[[国际标准书号|ISBN]] 978-1-4200-7575-5.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALinear+discriminant+analysis&rft.aufirst=J.+M.&rft.aulast=Hilbe&rft.btitle=Logistic+Regression+Models&rft.date=2009&rft.genre=book&rft.isbn=978-1-4200-7575-5&rft.pub=Chapman+%26+Hall%2FCRC+Press&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook"> </span>
* <cite class="citation journal">Mika, S.;  et al. (1999). [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.9904 "Fisher Discriminant Analysis with Kernels"] {{Wayback|url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.9904 |date=20090816194710 }}. </cite><cite class="citation journal">''IEEE Conference on Neural Networks for Signal Processing IX'': 41–48. [[DOI|doi]]:[[doi:10.1109/NNSP.1999.788121|10.1109/NNSP.1999.788121]].</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&rfr_id=info%3Asid%2Fen.wikipedia.org%3ALinear+discriminant+analysis&rft.atitle=Fisher+Discriminant+Analysis+with+Kernels&rft.au=Mika%2C+S.&rft.date=1999&rft.genre=article&rft_id=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.35.9904&rft_id=info%3Adoi%2F10.1109%2FNNSP.1999.788121&rft.jtitle=IEEE+Conference+on+Neural+Networks+for+Signal+Processing+IX&rft.pages=41-48&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal"> </span>
* Mark Burdon and Paul Harpur, ‘Re-Conceptualising Privacy and Discrimination in an Age of Talent Analytics’ (2014) 37 University of New South Wales Law Journal, 2, 679–712.1
* Miranda Terry and Paul Harpur, ‘The New Era of Segmenting Society on Ability Lines: Workplace Analytics and Disability Discrimination’ (Society for Disability Studies, Atlanta USA, 10–13 June 2015).
* H. Richard McFarland and Donald St. P. Richards, “Exact Misclassification Probabilities for Plug-In Normal Quadratic Discriminant Functions. I. The Equal-Means Case” Journal of Multivariate Analysis, 2001, vol. 77, issue 1, pages 21–53 [http://www.sciencedirect.com/science/article/pii/S0047259X00919249 link to article]
* H. Richard McFarland and Donald St. P. Richards, “Exact Misclassification Probabilities for Plug-In Normal Quadratic Discriminant Functions. II. The Heterogeneous Case” Journal of Multivariate Analysis, 2002, vol. 82, issue 2, pages 299-330 [http://www.sciencedirect.com/science/article/pii/S0047259X01920342 link to article]

== 外部链接 ==
* [http://www.alglib.net/dataanalysis/lineardiscriminantanalysis.php ALGLIB] contains open-source LDA implementation in C# / C++ / Pascal / VBA.
* [http://www.psychometrica.de/lds.html Psychometrica.de]{{dead link|date=2018年1月 |bot=InternetArchiveBot |fix-attempted=yes }} open-source LDA implementation in Java
* [http://people.revoledu.com/kardi/tutorial/LDA/index.html LDA tutorial using MS Excel]
* [https://web.archive.org/web/20150405124836/http://biostat.katerynakon.in.ua/en/prognosis/discriminant-analysis.html Biomedical statistics. Discriminant analysis]
* [http://www.powercam.cc/chli www.powercam.cc/chli] (中文的影片介紹包含Kernel Method, PCA, KPCA, LDA, GDA, and SVMs)
[[Category:分類演算法|Category:分類演算法]]
[[Category:多變量統計|Category:多變量統計]]
[[Category:统计分类|Category:统计分类]]
[[Category:市場學|Category:市場學]]