{{多個問題|
{{cleanup-jargon|time=2013-10-05T05:21:47+00:00}}
{{roughtranslation|time=2013-10-05T05:21:47+00:00}}
}}
{{noteTA
|T=zh-tw:線性迴歸;zh-cn:线性回归;
|G1=Math
|1=zh-hant:回歸;zh-tw:迴歸;zh-cn:回归;
|2=zh-hant:觀測值;zh-tw:觀察值;
|3=zh-tw:最小平方法;zh-cn:=最小二乘法;
}}

{{迴归侧栏}}
在[[统计学|统计学]]中，'''线性回归'''（{{lang-en|linear regression}}）是利用称为线性回归方程的[[最小二乘法|最小平方]]函數对一个或多个[[自变量|自变量]]和[[因变量|因变量]]之间关系进行建模的一种[[回归分析|回归分析]]。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做[[一般线性模型|多元回归]]（{{lang|en|multivariable linear regression}}）。<ref>{{citation|title=Methods of Multivariate Analysis|volume=709|series=Wiley Series in Probability and Statistics|first1=Alvin C.|last1=Rencher|first2=William F.|last2=Christensen|edition=3rd|publisher=John Wiley & Sons|year=2012|isbn=9781118391679|page=19|url=https://books.google.com/books?id=0g-PAuKub3QC&pg=PA19|contribution=Chapter 10, Multivariate regression – Section 10.1, Introduction|accessdate=2019-05-14|archive-date=2019-06-15|archive-url=https://web.archive.org/web/20190615204909/https://books.google.com/books?id=0g-PAuKub3QC&pg=PA19|dead-url=no}}.</ref>

在线性回归中，数据使用线性预测函数来建模，并且未知的模型参数也是通过数据来估计。这些模型被叫做线性模型。<ref>{{cite journal |author= Hilary L. Seal|year=1967 |title=The historical development of the Gauss linear model |jstor=2333849 |journal=Biometrika |publisher= |volume=54 |issue=1/2 |pages=1–24 |doi=10.1093/biomet/54.1-2.1|pmc= |pmid= }}</ref>最常用的线性回归建模是给定X值的y的条件均值是X的仿射函数。不太一般的情况，线性回归模型可以是一个中位数或一些其他的给定X的条件下y的条件分布的分位数作为X的线性函数表示。像所有形式的回归分析一样，线性回归也把焦点放在给定X值的y的条件概率分布，而不是X和y的联合概率分布（多元分析领域）。

线性回归是回归分析中第一种经过严格研究并在实际应用中广泛使用的类型。<ref>{{citation|title=Linear Regression Analysis: Theory and Computing|first=Xin|last=Yan|publisher=World Scientific|year=2009|isbn=9789812834119|pages=1–2|url=https://books.google.com/books?id=MjNv6rGv8NIC&pg=PA1|quote=Regression analysis ... is probably one of the oldest topics in mathematical statistics dating back to about two hundred years ago. The earliest form of the linear regression was the least squares method, which was published by Legendre in 1805, and by Gauss in 1809 ... Legendre and Gauss both applied the method to the problem of determining, from astronomical observations, the orbits of bodies about the sun.|accessdate=2019-05-14|archive-date=2019-06-08|archive-url=https://web.archive.org/web/20190608180433/https://books.google.com/books?id=MjNv6rGv8NIC&pg=PA1|dead-url=no}}</ref>这是因为线性依赖于其未知参数的模型比非线性依赖于其未知参数的模型更容易拟合，而且产生的估计的统计特性也更容易确定。

线性回归有很多实际用途。分为以下两大类：

# 如果目标是预测或者映射，线性回归可以用来对观测数据集的和X的值拟合出一个预测模型。当完成这样一个模型以后，对于一个新增的X值，在没有给定与它相配对的y的情况下，可以用这个拟合过的模型预测出一个y值。
# 给定一个变量y和一些变量<math>X_1</math>,...,<math>X_p</math>，这些变量有可能与y相关，线性回归分析可以用来量化y与Xj之间相关性的强度，评估出与y不相关的<math>X_j</math>，并识别出哪些<math>X_j</math>的子集包含了关于y的冗余信息。

线性回归模型经常用最小二乘逼近来拟合，但他们也可能用别的方法来拟合，比如用最小化“拟合缺陷”在一些其他规范里（比如最小绝对误差回归），或者在桥回归中最小化最小二乘损失函数的惩罚。相反，最小二乘逼近可以用来拟合那些非线性的模型。因此，尽管“最小二乘法”和“线性模型”是紧密相连的，但他们是不能划等号的。

{{机器学习导航栏}}

== 簡介 ==
[[File:Linear_least_squares.svg|thumb]]
=== 理論模型 ===
給一個随機樣本<math> (Y_i, X_{i1}, \ldots, X_{ip}), \, i = 1, \ldots, n </math>，一個線性回歸模型假設回歸子<math> Y_i </math>和回歸量<math> X_{i1}, \ldots, X_{ip} </math>之間的關係是除了X的影響以外，還有其他的變數存在。我們加入一個誤差項<math> \varepsilon_i </math>（也是一個随機變量）來捕獲除了<math>X_{i1}, \ldots, X_{ip} </math>之外任何對<math> Y_i </math>的影響。所以一個多變量線性回歸模型表示為以下的形式：
:<math> Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots + \beta_p X_{ip} + \varepsilon_i, \qquad i = 1, \ldots, n </math>
其他的模型可能被認定成非線性模型。一個線性回歸模型不需要是自變量的線性函數。線性在這裡表示<math> Y_i </math>的條件均值在參數<math>\beta</math>裡是線性的。例如：模型<math> Y_i = \beta_1 X_i + \beta_2 X_i^2 + \varepsilon_i </math>在<math> \beta_1 </math>和<math> \beta_2 </math>裡是線性的，但在<math> X_i^2 </math>裡是非線性的，它是<math> X_i </math>的非線性函數。

===數據和估計===
區分随機變量和這些變量的觀測值是很重要的。通常來說，觀測值或數據（以小寫字母表記）包括了''n''個值 <math> (y_i, x_{i1}, \ldots, x_{ip}), \, i = 1, \ldots, n </math>.

我們有<math> p + 1 </math>個參數<math> \beta_0, \ldots, \beta_p </math>需要決定，為了估計這些參數，使用[[矩陣|矩陣]]表記是很有用的。

:<math> Y = X \beta + \varepsilon \,</math> <!--- Do not delete "\,", it improves the display of the formula on certain browsers ---> 

其中''Y''是一個包括了觀測值<math> Y_1, \ldots, Y_n </math>的列向量，<math> \varepsilon </math>包括了未觀測的随機成份<math> \varepsilon_1, \ldots, \varepsilon_n </math>以及回歸量的觀測值矩陣<math> X </math>：

:<math> X = \begin{pmatrix} 1 & x_{11} & \cdots & x_{1p} \\ 1 & x_{21} & \cdots & x_{2p}\\ \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n1} & \cdots & x_{np} \end{pmatrix}</math>

''X''通常包括一個常數項。

如果''X''列之間存在[[線性相關|線性相關]]，那麽參數向量<math>\beta</math>就不能以最小二乘法估計除非<math>\beta</math>被限制，比如要求它的一些元素之和為0。

===古典假設===
* '''樣本'''是在'''母體'''之中'''随機抽取'''出來的。
* 因變量Y在[[實直線|實直線]]上是'''連續的'''，
* 殘差項是'''獨立'''且'''相同'''分佈的(iid)，也就是說，殘差是独立随机的，且服從'''高斯分佈'''。
這些假設意味著'''殘差項不依賴自變量的值'''，所以<math> \varepsilon_i </math>和自變量X（预測變量）之間是相互獨立的。

在這些假設下，建立一個顯式線性回歸作為條件预期模型的'''簡單線性回歸'''，可以表示為：

:<math> \mbox{E}(Y_i \mid X_i = x_i) = \alpha + \beta x_i \, </math>

==最小二乘法分析==
{{main|最小二乘法}}
===最小二乘法估計===
回歸分析的最初目的是估計模型的參數以便達到對數據的最佳拟合。在決定一個最佳拟合的不同標準之中，最小二乘法是非常優越的。這種估計可以表示為：
:<math> \hat\beta = (X^T X)^{-1}X^T y \, </math>

===迴歸推論===
對於每一個<math> i=1,\ldots,n </math>，我們用<math> \sigma^2 </math>代表誤差項<math> \varepsilon </math>的方差。一個無偏誤的估計是：
:<math>\hat \sigma^2  = \frac {S} {n-p} , </math>
其中<math> S := \sum_{i=1}^n \hat{\varepsilon}_i^2 </math>是誤差平方和（殘差平方和）。估計值和實際值之間的關係是：
:<math>\hat\sigma^2 \cdot \frac{n-p}{\sigma^2} \sim  \chi_{n-p}^2 </math>
其中<math> \chi_{n-p}^2 </math>服從[[卡方分佈|卡方分佈]]，自由度是<math>n-p</math>

對普通方程的解可以冩為：
:<math>\hat{\boldsymbol\beta}=(\mathbf{X^TX)^{-1}X^Ty}.</math>
這表示估計項是因變量的線性組合。進一步地說，如果所觀察的誤差服從[[正態分佈|正態分佈]]。參數的估計值將服從聯合正態分佈。在當前的假設之下，估計的參數向量是精確分佈的。
:<math> \hat\beta \sim N ( \beta, \sigma^2 (X^TX)^{-1} ) </math>
其中<math>N(\cdot)</math>表示[[多變量正態分佈|多變量正態分佈]]。

參數估計值的[[標準差|標準差]]是：
:<math>\hat\sigma_j=\sqrt{ \frac{S}{n-p}\left[\mathbf{(X^TX)}^{-1}\right]_{jj}}.</math>

參數<math>\beta_j </math>的<math>100(1-\alpha)\% </math>[[置信區間|置信區間]]可以用以下式子來計算：
:<math>\hat \beta_j  \pm t_{\frac{\alpha }{2},n - p} \hat \sigma_j. </math>

誤差項可以表示為：
:<math>\mathbf{\hat r =  y-X \hat{\boldsymbol{\beta}}= y-X(X^TX)^{-1}X^Ty}.\,</math>

====單變量線性回歸====
單變量線性回歸，又稱簡單線性回歸（simple linear regression, SLR），是最簡單但用途很廣的回歸模型。其回歸式為：

:<math> Y = \alpha + \beta X + \varepsilon </math>

為了從一組樣本<math>(y_i, x_i)</math>（其中<math>i = 1,\  2, \ldots, n </math>）之中估計最合適（誤差最小）的<math> \alpha </math>和<math> \beta </math>，通常採用最小二乘法，其計算目標為最小化殘差平方和：

:<math> \sum_{i = 1}^n \varepsilon_i^2 = \sum_{i = 1}^n (y_i - \alpha - \beta x_i)^2 </math>

使用微分法求極值：將上式分别對<math>\alpha</math>和<math>\beta</math>做一階偏微分，並令其等於0：
:<math>\left\{\begin{array}{lcl}
n\  \alpha + \sum\limits_{i = 1}^n x_i\  \beta = \sum\limits_{i = 1}^n y_i \\
\sum\limits_{i = 1}^n x_i\  \alpha + \sum\limits_{i = 1}^n x_i^2\  \beta = \sum\limits_{i = 1}^n x_i y_i
\end{array}\right.</math>

此二元一次線性方程組可用[[克萊姆法則|克萊姆法則]]求解，得解<math>\hat\alpha,\  \hat\beta</math>：

:<math>\hat\beta = \frac {n \sum\limits_{i = 1}^n x_i y_i - \sum\limits_{i = 1}^n x_i \sum\limits_{i = 1}^n y_i} {n \sum\limits_{i = 1}^n x_i^2 - \left(\sum\limits_{i = 1}^n x_i\right)^2}
=\frac{\sum\limits_{i = 1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum\limits_{i = 1}^n(x_i-\bar{x})^2}
\,</math>

:<math>\hat\alpha = \frac {\sum\limits_{i = 1}^n x_i^2 \sum\limits_{i = 1}^n y_i - \sum\limits_{i = 1}^n x_i \sum\limits_{i = 1}^n x_iy_i} {n \sum\limits_{i = 1}^n x_i^2 - \left(\sum\limits_{i = 1}^n x_i\right)^2}= \bar y-\bar x \hat\beta </math>

:<math>S = \sum\limits_{i = 1}^n (y_i - \hat{y}_i)^2 
= \sum\limits_{i = 1}^n y_i^2 - \frac {n (\sum\limits_{i = 1}^n x_i y_i)^2 + (\sum\limits_{i = 1}^n y_i)^2 \sum\limits_{i = 1}^n x_i^2 - 2 \sum\limits_{i = 1}^n x_i \sum\limits_{i = 1}^n y_i \sum\limits_{i = 1}^n x_i y_i } {n \sum\limits_{i = 1}^n x_i^2 - \left(\sum\limits_{i = 1}^n x_i\right)^2}
</math>

:<math>\hat \sigma^2  = \frac {S} {n-2}. </math>

[[協方差矩陣|協方差矩陣]]是： 

:<math>\frac{1}{n \sum_{i = 1}^n x_i^2 - \left(\sum_{i = 1}^n x_i\right)^2}\begin{pmatrix}
  \sum x_i^2 & -\sum x_i \\
  -\sum x_i & n   
\end{pmatrix}</math>

[[平均響應|平均響應]]置信區間為：
:<math>y_d = (\alpha+\hat\beta x_d) \pm t_{ \frac{\alpha }{2} ,n-2} \hat \sigma \sqrt {\frac{1}{n} + \frac{(x_d - \bar{x})^2}{\sum (x_i - \bar{x})^2}}</math>

[[預報響應|預報響應]]置信區間為：
:<math>y_d = (\alpha+\hat\beta x_d) \pm t_{ \frac{\alpha }{2} ,n-2} \hat \sigma \sqrt {1+\frac{1}{n} + \frac{(x_d - \bar{x})^2}{\sum (x_i - \bar{x})^2}}</math>

===方差分析===
在[[方差分析|方差分析]]（ANOVA）中，總平方和分解為兩個或更多部分。

'''總平方和'''SST (sum of squares for total) 是：

: <math> \text{SST} = \sum_{i=1}^n (y_i - \bar y)^2</math>　，其中：　<math> \bar y = \frac{1}{n} \sum_i y_i</math>

同等地：

: <math> \text{SST}  = \sum_{i=1}^n y_i^2 - \frac{1}{n}\left(\sum_i y_i\right)^2 </math>

'''回歸平方和'''SSReg (sum of squares for regression。也可寫做'''模型平方和'''，SSM，sum of squares for model) 是：

:<math>\text{SSReg} = \sum \left( \hat y_i  - \bar y  \right)^2
= \hat{\boldsymbol\beta}^T \mathbf{X}^T
\mathbf y - \frac{1}{n}\left( \mathbf {y^T u u^T y} \right), </math>

'''殘差平方和'''SSE (sum of squares for error) 是：

:<math>\text{SSE} = \sum_i {\left( {y_i  - \hat y_i} \right)^2 }
= \mathbf{ y^T y - \hat{\boldsymbol\beta}^T X^T y}.</math>

總平方和SST又可寫做SSReg和SSE的和：

:<math>\text{SST} = \sum_i \left( y_i-\bar y \right)^2 = \mathbf{ y^T y}-\frac{1}{n}\left( \mathbf{y^Tuu^Ty}\right)=\text{SSReg}+ \text{SSE}.</math>

'''回歸係數'''R<sup>2</sup>是：

:<math>R^2  = \frac{\text{SSReg}}{{\text{SST}}}
= 1 - \frac{\text{SSE}}{\text{SST}}.</math>

==其他方法==
===廣義最小二乘法===
[[廣義最小二乘法|廣義最小二乘法]]可以用在當觀測誤差具有[[異方差|異方差]]或者[[自相關|自相關]]的情況下。

===總體最小二乘法===
[[總體最小二乘法|總體最小二乘法]]用於當自變量有誤時。

===廣義線性模式===
[[廣義線性模式|廣義線性模式]]應用在當誤差分佈函數不是正態分佈時。比如[[指數分佈|指數分佈]]，[[伽瑪分佈|伽瑪分佈]]，[[逆高斯分佈|逆高斯分佈]]，[[泊松分佈|泊松分佈]]，[[二項式分佈|二項式分佈]]等。

===穩健回歸===
將平均[[絕對誤差|絕對誤差]]最小化，不同於在線性回歸中是將均方誤差最小化。

==線性回歸的應用==
===趨勢線===
一條趨勢線代表著[[時間序列|時間序列]]數據的長期走勢。它告訴我們一組特定數據（如GDP、石油價格和股票價格）是否在一段時期内增長或下降。雖然我們可以用肉眼觀察數據點在坐標系的位置大體畫出趨勢線，更恰當的方法是利用線性回歸計算出趨勢線的位置和斜率。

===流行病学===
有关[[吸烟|吸烟]]对[[死亡率|死亡率]]和[[发病率|发病率]]影响的早期证据来自采用了回归分析的[[观察性研究|观察性研究]]。为了在分析观测数据时减少[[偽關係|伪相关]]，除最感兴趣的变量之外,通常研究人员还会在他们的回归模型里包括一些额外变量。例如，假设有一个回归模型，在这个回归模型中吸烟行为是我们最感兴趣的独立变量，其相关变量是经数年观察得到的吸烟者寿命。研究人员可能将社会经济地位当成一个额外的独立变量，已确保任何经观察所得的吸烟对寿命的影响不是由于教育或收入差异引起的。然而，我们不可能把所有可能混淆结果的变量都加入到实证分析中。例如，某种不存在的基因可能会增加人死亡的几率，还会让人的吸烟量增加。因此，比起采用观察数据的回归分析得出的结论，[[随机对照试验|随机对照试验]]常能产生更令人信服的因果关系证据。当[[可控实验|可控实验]]不可行时，回归分析的衍生，如[[工具变量|工具变量]]回归，可尝试用来估计观测数据的因果关系。

===金融===
[[資本資產定價模型|資本資產定價模型]]利用線性回歸以及[[Beta係數|Beta係數]]的概念分析和計算投資的系統風險。這是從聯繫投資回報和所有風險性資產回報的模型Beta係數直接得出的。

===经济学===
线性回归是经济学的主要实证工具。例如，它是用来预测消费支出，<ref>{{cite book |last=Deaton |first=Angus |year=1992 |title=Understanding Consumption |url=https://archive.org/details/understandingcon0000deat |publisher=Oxford University Press |isbn=978-0-19-828824-4 }}</ref>固定投资支出，存货投资，一国出口产品的购买，<ref name=Krugman/>进口支出，<ref name=Krugman>{{cite book |last=Krugman |first=Paul R. |authorlink=Paul Krugman |first2=M. |last2=Obstfeld |authorlink2=Maurice Obstfeld |first3=Marc J. |last3=Melitz |authorlink3=Marc Melitz |year=2012 |title=International Economics: Theory and Policy |edition=9th global |location=Harlow |publisher=Pearson |isbn=9780273754091 }}</ref>要求持有流动性资产，<ref>{{cite book |last=Laidler |first=David E. W. |year=1993 |title=The Demand for Money: Theories, Evidence, and Problems |edition=4th |location=New York |publisher=Harper Collins |isbn=978-0065010985 }}</ref>劳动力需求、<ref name=Ehrenberg/>劳动力供给。<ref name=Ehrenberg>{{cite book |last=Ehrenberg |last2=Smith |title=Modern Labor Economics |publisher=Addison-Wesley |location=London |edition=10th international |year=2008 |isbn=9780321538963 }}</ref>

== 参考文献 ==
=== 引用 ===
{{Reflist|30em}}

=== 来源 ===
; 书籍
{{reflist|list=
* {{cite book |author = Cohen, J., Cohen P., West, S.G., & Aiken, L.S. |title = ''Applied multiple regression/correlation analysis for the behavioral sciences |year = 2003 |publisher = Lawrence Erlbaum Associates |location = Hillsdale, NJ|isbn =  }} 
* {{cite book |author = Draper, N.R. and Smith, H  |title = Applied Regression Analysis|year = 1998|publisher = Wiley Series in Probability and Statistics|location =  |ISBN =  }} 
* {{cite book |last =  |first =  |author = Robert S. Pindyck and Daniel L. Rubinfeld|authorlink =  |coauthors =  |editor =  |others =  |title = Econometric Models and Economic Forecasts|origdate =  |origyear =  |origmonth =  |url =  |format =  |accessdate =  |accessyear =  |accessmonth =  |edition =  |series =  |date =  |year = 1998 |month =  |publisher =  |location =  |language =  |isbn =  |oclc =  |doi =  |id =  |pages =  |chapter = Chapter One|chapterurl =  |quote =  }} 
* [[查尔斯·达尔文|Charles Darwin]]. ''The Variation of Animals and Plants under Domestication''. (1868) ''(Chapter XIII describes what was known about reversion in Galton's time. Darwin uses the term "reversion".)''
}}

; 刊物文章
{{reflist|list=
* {{cite journal en |last = Galton |first = Francis|authorlink = |coauthors =  |year = 1886 |month =  |title = Regression Towards Mediocrity in Hereditary Stature |journal = Journal of the Anthropological Institute |volume = 15 |issue =  |pages = 246-263 |url = http://www.mugu.com/galton/essays/1880-1889/galton-1886-jaigi-regression-stature.pdf |accessdate = 2008-12-30 |quote =  }}
}}

== 延伸阅读 ==
{{refbegin|2}}
* {{Cite document |date = 1982 |author = Pedhazur, Elazar J |title = Multiple regression in behavioral research: Explanation and prediction |url = https://archive.org/details/rj0000unse |edition = 2nd|place = New York |publisher = Holt, Rinehart and Winston |isbn = 0-03-041760-0 |postscript = <!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->}}
* {{Cite book |last = Barlow |first = Jesse L. |author-link = |chapter = Chapter 9: Numerical aspects of Solving Linear Least Squares Problems |editor-last = Rao |editor-first = C.R. |title = Computational Statistics |series = Handbook of Statistics |volume = 9 |publisher = North-Holland |publication-date = 1993 |isbn = 0-444-88096-8 }}
* {{Cite book |last1 = Björck |first1 =  Åke |authorlink = |coauthors = |title = Numerical methods for least squares problems |year = 1996 |publisher = SIAM |location = Philadelphia |isbn = 0-89871-360-9 |pages = }}
* {{Cite book |last = Goodall |first = Colin R. |author-link = |chapter = Chapter 13: Computation using the QR decomposition |editor-last = Rao |editor-first = C.R. |title = Computational Statistics | series = Handbook of Statistics |volume = 9 |publisher = North-Holland |publication-date = 1993 |isbn = 0-444-88096-8 }}
* {{Cite book |last = National Physical Laboratory |first = |chapter = Chapter 1: Linear Equations and Matrices: Direct Methods |title = Modern Computing Methods |edition = 2nd |series = Notes on Applied Science |volume = 16 |publisher = Her Majesty's Stationery Office |publication-date = 1961 }}
{{refend}}

== 参见 ==
{{div col|2}}
* [[方差分析|方差分析]]
* [[安斯库姆四重奏|安斯库姆四重奏]]
* [[横截面回归|横截面回归]]
* [[曲线拟合|曲线拟合]]
* [[经验贝叶斯方法|经验贝叶斯方法]]
* [[逻辑斯蒂回归|逻辑斯蒂回归]]
* [[M估计|M估计]]
* [[非线性回归|非线性回归]]
* [[非参数回归|非参数回归]]
* [[多元自适应回归样条|多元自适应回归样条]]
* [[Lack-of-fit_sum_of_squares|Lack-of-fit sum of squares]]<!--没有什么好的译名-->
* [[截断回归模型|截断回归模型]]
* [[删失回归模型|删失回归模型]]
* [[简单线性回归|简单线性回归]]
* [[分段回归|分段线性回归]]
{{div col end}}

==外部連結==
{{Commons category|Linear regression}}
* [https://phet.colorado.edu/en/simulation/least-squares-regression Least-Squares Regression], [[PhET|PhET]] Interactive simulations, University of Colorado at Boulder

{{-}}
{{Experimental design}}
{{Statistics}}

[[Category:迴归分析|Category:迴归分析]]
[[Category:估计理论|Category:估计理论]]