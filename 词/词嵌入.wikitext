{{机器学习导航栏}}

'''词嵌入'''（Word embedding）是[[自然语言处理|自然语言处理]]（NLP）中[[語言模型|语言模型]]与[[表征学习|表征学习]]技术的统称。概念上而言，它是指把一个维数为所有词的数量的高维空间[[嵌入_(数学)|嵌入]]到一个维数低得多的连续[[向量空间|向量空间]]中，每个单词或词组被映射为[[实数|实数]][[数域|域]]上的向量。

词嵌入的方法包括[[人工神经网络|人工神经网络]]<ref>{{Cite arXiv|title=Distributed Representations of Words and Phrases and their Compositionality|last1=Mikolov|last2=Sutskever|last3=Chen|last4=Corrado|last5=Dean|first1=Tomas|first2=Ilya|first3=Kai|first4=Greg|first5=Jeffrey|year=2013|eprint=1310.4546|class=cs.CL}}</ref>、对词语{{Link-en|同现矩阵|co-occurrence matrix}}[[降维|降维]]<ref>{{Cite arXiv|title=Word Emdeddings through Hellinger PCA|last1=Lebret|last2=Collobert|first1=Rémi|first2=Ronan|year=2013|eprint=1312.5542|class=cs.CL}}</ref><ref>{{Cite conference |url=http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf |title=Neural Word Embedding as Implicit Matrix Factorization |last=Levy |first=Omer |conference=NIPS |year=2014 |last2=Goldberg |first2=Yoav |access-date=2016-12-28 |archive-date=2016-11-14 |archive-url=https://web.archive.org/web/20161114071800/http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf/ |dead-url=no }}</ref><ref>{{Cite conference |url=http://ijcai.org/papers15/Papers/IJCAI15-513.pdf |title=Word Embedding Revisited: A New Representation Learning and Explicit Matrix Factorization Perspective |last=Li |first=Yitan |conference=Int'l J. Conf. on Artificial Intelligence (IJCAI) |year=2015 |last2=Xu |first2=Linli |access-date=2016-12-28 |archive-date=2015-09-06 |archive-url=https://web.archive.org/web/20150906072818/http://ijcai.org/papers15/Papers/IJCAI15-513.pdf |dead-url=yes }}</ref>、[[機率模型|機率模型]]<ref>{{Cite journal|title=Euclidean Embedding of Co-occurrence Data|url=http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/34951.pdf|last=Globerson|first=Amir|date=2007|journal=Journal of Machine learning research|access-date=2016-12-28|archive-date=2016-09-21|archive-url=https://web.archive.org/web/20160921091822/http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/34951.pdf|dead-url=yes}}</ref>以及单词所在上下文的显式表示等。<ref>{{cite conference |last1=Levy |first1=Omer |last2=Goldberg |first2=Yoav |title=Linguistic Regularities in Sparse and Explicit Word Representations |conference=CoNLL |pages=171–180 |year=2014 |url=https://levyomer.files.wordpress.com/2014/04/linguistic-regularities-in-sparse-and-explicit-word-representations-conll-2014.pdf |access-date=2016-12-28 |archive-date=2017-09-25 |archive-url=https://web.archive.org/web/20170925181854/https://levyomer.files.wordpress.com/2014/04/linguistic-regularities-in-sparse-and-explicit-word-representations-conll-2014.pdf |dead-url=yes }}</ref>

在底层输入中，使用词嵌入来表示词组的方法极大提升了NLP中[[語法分析器|语法分析器]]<ref>{{cite conference |last1=Socher |first1=Richard |last2=Bauer |first2=John |last3=Manning |first3=Christopher |last4=Ng |first4=Andrew |title=Parsing with compositional vector grammars |conference=Proc. ACL Conf. |year=2013 |url=http://www.socher.org/uploads/Main/SocherBauerManningNg_ACL2013.pdf |access-date=2016-12-28 |archive-date=2016-08-11 |archive-url=https://web.archive.org/web/20160811041232/http://www.socher.org/uploads/Main/SocherBauerManningNg_ACL2013.pdf |dead-url=yes }}</ref>和[[文本情感分析|文本情感分析]]等的效果。<ref>{{cite conference |last1=Socher |first1=Richard |last2=Perelygin |first2=Alex |last3=Wu |first3=Jean |last4=Chuang |first4=Jason |last5=Manning |first5=Chris |last6=Ng |first6=Andrew |last7=Potts |first7=Chris |title=Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank |conference=EMNLP |year=2013 |url=http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf |access-date=2016-12-28 |archive-date=2016-12-28 |archive-url=https://web.archive.org/web/20161228100300/http://nlp.stanford.edu/%7Esocherr/EMNLP2013_RNTN.pdf |dead-url=no }}</ref>

== 发展历程 ==
词嵌入技术起源于2000年。[[约书亚·本希奥|约书亚·本希奥]]等人在一系列论文中使用了神经概率语言模型（Neural probabilistic language models）使机器“习得词语的分布式表示（learning a distributed representation for words）”，从而达到将词语空间降维的目的。<ref>{{Cite journal|title=A Neural Probabilistic Language Model|url=http://link.springer.com/chapter/10.1007/3-540-33486-6_6#page-1|doi=10.1007/3-540-33486-6_6|page=1|journal=|access-date=2016-12-28|archive-date=2017-01-17|archive-url=https://web.archive.org/web/20170117015626/http://link.springer.com/chapter/10.1007/3-540-33486-6_6#page-1|dead-url=no}}</ref>罗维斯（Roweis）与索尔（Saul）在<span>《[[科学_(期刊)|科学]]》上发表了用</span>[[非线性降维|局部线性嵌入]]（LLE）来学习高维数据结构的低维表示方法<ref>{{cite journal|title=Nonlinear Dimensionality Reduction by Locally Linear Embedding|journal=Science|volume=290|issue=5500|pages=2323|url=http://science.sciencemag.org/content/290/5500/2323|bibcode=2000Sci...290.2323R|author1=Roweis|first1=Sam T.|last2=Saul|first2=Lawrence K.|year=2000|doi=10.1126/science.290.5500.2323|pmid=11125150|access-date=2016-12-28|archive-date=2016-12-06|archive-url=https://web.archive.org/web/20161206175042/http://science.sciencemag.org/content/290/5500/2323|dead-url=no}}</ref>。这个领域开始时稳步发展，在2010年后突飞猛进；一定程度上而言，这是因为这段时间里向量的质量与模型的训练速度有极大的突破。

词嵌入领域的分支繁多，有许多学者致力于其研究。2013年，[[Google|谷歌]]一个托马斯·米科洛维（Tomas Mikolov）领导的团队发明了一套工具[[word2vec|word2vec]]来进行词嵌入，训练向量空间模型的速度比以往的方法都快。<ref>{{Cite web |url=http://deeplearning4j.org/word2vec |title=word2vec |access-date=2016-12-28 |archive-url=https://web.archive.org/web/20170211040312/https://deeplearning4j.org/word2vec |archive-date=2017-02-11 |dead-url=yes }}</ref>许多新兴的词嵌入基于[[人工神经网络|人工神经网络]]，而不是过去的[[n元语法|n元语法]]模型和[[非監督式學習|非监督式学习]]。<ref>{{Cite journal|title=A Scalable Hierarchical Distributed Language Model|url=http://papers.nips.cc/paper/3583-a-scalable|deadurl=yes|archiveurl=https://web.archive.org/web/20160806151418/http://papers.nips.cc/paper/3583-a-scalable|archivedate=2016-08-06|journal=|access-date=2016-12-28}}</ref>

== 生物序列中的应用：BioVectors ==
阿斯加里（Asgari）和莫夫拉德（Mofrad）提出了[[生物信息学|生物信息学]]中生物序列（DNA、RNA和蛋白质等）基于n元语法的词嵌入技术。<ref name=":0">{{cite journal|title=Continuous Distributed Representation of Biological Sequences for Deep Proteomics and Genomics|url=http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0141287|last2=Mofrad|first2=Mohammad R.K.|date=2015|journal=PloS one|issue=11|doi=10.1371/journal.pone.0141287|volume=10|page=e0141287|bibcode=2015PLoSO..1041287A|last1=Asgari|first1=Ehsaneddin|access-date=2020-09-19|archive-date=2020-08-15|archive-url=https://web.archive.org/web/20200815082026/https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0141287|dead-url=no}}</ref>bio-vectors（BioVec）表示生物序列的统称，protein-vectors（ProtVec）表示蛋白质（[[氨基酸|氨基酸]]序列），gene-vectors（GeneVec）表示基因序列。BioVec在[[蛋白质组学|蛋白质组学]]与[[基因組學|基因组学]]的[[深度学习|深度学习]]中有广泛应用。他们提出的结果表明，BioVectors可描述[[生物化学|生物化学]]与[[生物物理学|生物物理学]]意义下生物序列的基本模式。<ref name=":0" />

== Thought vectors ==
<span>将词嵌入扩展到对句子或整个文本的嵌入后得到的结果称为</span>'''Thought vectors'''。部分研究者期望用Thought vectors来提升[[机器翻译|机器翻译]]的质量。<ref>{{Cite arXiv|title=skip-thought vectors|last1=Kiros|last2=Zhu|last3=Salakhutdinov|last4=Zemel|last5=Torralba|last6=Urtasun|last7=Fidler|first1=Ryan|first2=Yukun|first3=Ruslan|first4=Richard S.|first5=Antonio|first6=Raquel|first7=Sanja|year=2015|eprint=1506.06726|class=cs.CL}}</ref><ref>{{Cite web|url=http://deeplearning4j.org/thoughtvectors|title=thoughtvectors|access-date=2016-12-28|archive-url=https://web.archive.org/web/20170211043631/https://deeplearning4j.org/thoughtvectors|archive-date=2017-02-11|dead-url=yes}}</ref>

== 软件实现 ==
使用词嵌入技术的训练软件包括托马斯·米科洛维的[[Word2vec|Word2vec]]、[[斯坦福大学|斯坦福大学]]的{{Link-en|GloVe|GloVe (machine learning)}}<ref>{{Cite web|url=http://nlp.stanford.edu/projects/glove/|title=GloVe|accessdate=2016-12-28|archive-date=2016-12-19|archive-url=https://web.archive.org/web/20161219132150/http://nlp.stanford.edu/projects/glove/|dead-url=no}}</ref>和[[Deeplearning4j|Deeplearning4j]]。[[主成分分析|主成分分析]]（PCA）和{{Link-en|t-分布邻域嵌入算法|t-distributed stochastic neighbor embedding}}（t-SNE）也可以用来对词语空间降维，并实现词嵌入的可视化与{{Link-en|词义感应|Word-sense induction}}。<ref>{{Cite journal|title=A Visualization of Evolving Clinical Sentiment Using Vector Representations of Clinical Notes|url=http://www.cinc.org/archives/2015/pdf/0629.pdf|last=Ghassemi|first=Mohammad|last2=Mark|first2=Roger|date=2015|journal=Computing in Cardiology|last3=Nemati|first3=Shamim|access-date=2016-12-28|archive-date=2016-05-31|archive-url=https://web.archive.org/web/20160531160610/http://www.cinc.org/archives/2015/pdf/0629.pdf|dead-url=yes}}</ref>

== 参见 ==
* [[自然语言处理|自然语言处理]]
* [[人工神经网络|人工神经网络]]
* [[機率模型|機率模型]]

== 参考文献 ==
{{Reflist}}

[[Category:人工神经网络|Category:人工神经网络]]
[[Category:计算语言学|Category:计算语言学]]
[[Category:自然語言處理|Category:自然語言處理]]