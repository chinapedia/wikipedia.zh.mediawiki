[[File:Sigmoid_function_01.png|thumb]]
[[File:Sigmoid_function_02.png|thumb]]
'''S型函数'''（{{lang-en|sigmoid function}}，或稱'''乙狀函數'''）是一種[[函数|函数]]，因其[[函數圖像|函數圖像]]形状像字母'''[[S|S]]'''得名。其形狀曲線至少有2個焦點，也叫“二焦點曲線函數”。S型函数是[[有界函数|有界]]、[[可微函数|可微]]的实函数，在实数范围内均有取值，且导数恒为非负<ref name=":0">{{Cite book |last1=Han |first1=Jun |last2=Morag |first2=Claudio |title=From Natural to Artificial Neural Computation |volume=930 |chapter=The influence of the sigmoid function parameters on the speed of backpropagation learning |editor1-last=Mira |editor1-first=José |editor2-last=Sandoval |editor2-first=Francisco |pages=[https://archive.org/details/fromnaturaltoart1995inte/page/195 195–201] |year=1995 |doi=10.1007/3-540-59497-3_175 |series=Lecture Notes in Computer Science |isbn=978-3-540-59497-0 |chapter-url=https://archive.org/details/fromnaturaltoart1995inte/page/195 }}</ref>，有且只有一个[[拐点|拐点]]。S型函数和S型曲线指的是同一事物。

[[逻辑斯谛函数|逻辑斯谛函数]]是一种常见的S型函数，其公式如下：<ref name=":0"/>

: <math>S(t) = \frac{1}{1 + e^{-t}}.</math>

其级数展开为：

: <math>s := 1/2+\frac{1}{4}t-\frac{1}{48}t^3+\frac{1}{480}t^5-\frac{17}{80640}t^7+\frac{31}{1451520}t^9-\frac{691}{319334400}t^{11}+O(t^{12})</math>

其他S型函數案例見下。在一些學科領域，特別是[[人工神经网络|人工神经网络]]中，S型函數通常特指邏輯斯諦函數。

==常見的S型函數==
[[File:Gjl-t(x).svg|thumb]]

*[[逻辑斯谛函数|逻辑斯谛函数]]
::<math> f(x) = \frac{1}{1 + e^{-x}} </math>

*[[双曲函数|雙曲正切函數]]（等價於[[逻辑斯谛函数|逻辑斯谛函数]]的平移與縮放）
::<math> f(x) = \tanh x = \frac{e^x-e^{-x}}{e^x+e^{-x}} </math>

*[[反三角函数|反正切函數]]
::<math> f(x) = \arctan x </math>

*[[古德曼函數|古德曼函數]]

::<math> f(x) = \operatorname{gd}(x) = \int_{0}^{x} \frac{1}{\cosh t} \, dt = 2\arctan\left(\tanh\left(\frac{x}{2}\right)\right)
 </math>

*[[误差函数|误差函数]]
::<math> f(x) = \operatorname{erf}(x) = \frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2} \, dt </math>

*{{link-en|廣義邏輯斯諦函數|Generalised logistic function}}
::<math> f(x) = (1+e^{-x})^{-\alpha}, \quad \alpha > 0 </math>

*{{link-en|平滑階躍函數|Smoothstep}}

::<math> f(x) = \begin{cases}
\displaystyle{
\frac{\int_{0}^{x} \bigl(1 - u^2 \bigr)^N \ du}
{\int_{0}^{1} {\bigl(1 - u^2 \bigr)^N \ du}}
}, & |x| \le 1 \\
\sgn(x) & |x| \ge 1 \\
\end{cases} \, \quad N \ge 1 </math>

*一些[[代數函數|代數函數]], 例如
::<math> f(x) = \frac{x}{\sqrt{1+x^2}} </math>

所有連續非負的凸形函數的積分都是S型函數，因此許多常見[[概率分布|概率分布]]的[[累积分布函数|累积分布函数]]會是S型函數。一個常見的例子是[[误差函数|误差函数]]，它是[[正态分布|正态分布]]的累积分布函数。

== 参考文献 ==
{{reflist}}

* {{ cite book | first1=Tom M. |last1= Mitchell | title=Machine Learning | url=https://archive.org/details/machinelearning0000mitc | publisher=WCB–McGraw–Hill |year=1997
|isbn=0-07-042807-7}}. In particular see "Chapter 4: Artificial Neural Networks" (in particular pp. 96–97) where Mitchell uses the word "logistic function" and the "sigmoid function" synonymously – this function he also calls the "squashing function" – and the sigmoid (aka logistic) function is used to compress the outputs of the "neurons" in multi-layer neural nets.
* {{cite web |first1= Mark |last1= Humphrys |url= http://www.computing.dcu.ie/~humphrys/Notes/Neural/sigmoid.html |title= Continuous output, the sigmoid function |access-date= 2015-02-01 |archive-url= https://web.archive.org/web/20150202160017/http://www.computing.dcu.ie/~humphrys/Notes/Neural/sigmoid.html |archive-date= 2015-02-02 |dead-url= yes }} Properties of the sigmoid, including how it can shift along axes and how its domain may be transformed.

== 参见 ==
{{commons category|Sigmoid functions}}
{{div col|colwidth=30em}}
* [[单位阶跃函数|单位阶跃函数]]
* [[邏輯迴歸|邏輯迴歸]]
* {{tsl|en|Logit|分对数}}
* [[线性整流函数|线性整流函数]]
* [[Softmax函数|Softmax函数]]
* [[韦伯分布|韦伯分布]]
* [[费米-狄拉克统计|费米-狄拉克统计]]
{{div col end}}

{{Differentiable computing}}

[[Category:基本特殊函数|Category:基本特殊函数]]
[[Category:人工神经网络|Category:人工神经网络]]