{{机器学习导航栏}}

'''提升方法'''（Boosting）是一种[[机器学习|机器学习]]中的[[集成学习|集成学习]][[元启发算法|元启发算法]]，主要用来减小[[監督式學習|監督式學習]]中[[偏差|偏差]]并且也减小[[方差|方差]]<ref>{{cite web|url=http://oz.berkeley.edu/~breiman/arcall96.pdf|archive-url=https://web.archive.org/web/20150119081741/http://oz.berkeley.edu/~breiman/arcall96.pdf|url-status=dead|archive-date=2015-01-19|title=BIAS, VARIANCE, AND ARCING CLASSIFIERS|last1=Leo Breiman|author-link=Leo Breiman|date=1996|publisher=TECHNICAL REPORT|quote=Arcing [Boosting] is more successful than bagging in variance reduction|access-date=19 January 2015}}</ref>，以及一系列将弱学习器转换为强学习器的机器学习算法<ref>{{cite book |last=Zhou Zhi-Hua |author-link=Zhou Zhihua |date=2012 |title=Ensemble Methods: Foundations and Algorithms |publisher= Chapman and Hall/CRC |page=23 |isbn=978-1439830031 |quote=The term boosting refers to a family of algorithms that are able to convert weak learners to strong learners }}</ref>。面對的问题是邁可·肯斯（Michael Kearns）和[[莱斯利·瓦利安特|莱斯利·瓦利安特]](Leslie Valiant)提出的：<ref name="Kearns88">Michael Kearns (1988); [http://www.cis.upenn.edu/~mkearns/papers/boostnote.pdf ''Thoughts on Hypothesis Boosting''] {{Wayback|url=http://www.cis.upenn.edu/~mkearns/papers/boostnote.pdf |date=20190713142916 }}, Unpublished manuscript (Machine Learning class project, December 1988)</ref>一組“弱学习者”的集合能否生成一个“强学习者”？弱学习者一般是指一个分类器，它的结果只比随机分类好一点点；强学习者指分类器的结果非常接近真值。

Robert Schapire在1990年的一篇论文中<ref name="Schapire90">{{cite journal | first = Robert E. | last = Schapire | year = 1990 | citeseerx = 10.1.1.20.723 | url = http://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf | title = The Strength of Weak Learnability | journal = Machine Learning | volume = 5 | issue = 2 | pages = 197–227 | doi = 10.1007/bf00116037 | s2cid = 53304535 | access-date = 2012-08-23 | archive-url = https://web.archive.org/web/20121010030839/http://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf | archive-date = 2012-10-10 | url-status = dead }}</ref>对肯斯和瓦利安特的问题<!--Please do not cite only one, because "Kearns and Valiant" is used as a convention to denote this question.-->的肯定回答在机器学习和统计方面产生了重大影响，最显着的是导致了[[提升方法|提升方法]]的发展<ref>{{cite journal |last = Leo Breiman |author-link = Leo Breiman |date = 1998|title = Arcing classifier (with discussion and a rejoinder by the author)|url = https://archive.org/details/sim_annals-of-statistics_1998-06_26_3/page/801 |journal = Ann. Stat.|volume = 26|issue = 3|pages = 801–849|doi = 10.1214/aos/1024691079|quote = Schapire (1990) proved that boosting is possible. (Page 823)|doi-access = free}}</ref> <!--{{citation needed|date=July 2014}} Could use secondary source to back up this claim. -->。

==提升算法==
大多数提升算法包括由迭代使用弱学习分類器組成，並將其結果加入一個最終的成强学习分類器。加入的过程中，通常根据它们的分类准确率给予不同的权重。加和弱学习者之后，数据通常会被重新加权，来强化对之前分类错误数据点的分类。

[[File:Ensemble_Boosting.svg|thumb]]

一个经典的提升算法例子是[[AdaBoost|AdaBoost]]。一些最近的例子包括[[LPBoost|LPBoost]]、[[TotalBoost|TotalBoost]]、[[BrownBoost|BrownBoost]]、[[MadaBoost|MadaBoost]]及[[LogitBoost|LogitBoost]]。许多提升方法可以在[[AnyBoost|AnyBoost]]框架下解释为在[[函数空间|函数空间]]利用一个凸的误差函数作[[梯度下降|梯度下降]]。

==批评==
2008年，[[谷歌|谷歌]]的菲利普·隆（Phillip Long）與[[哥倫比亞大學|哥倫比亞大學]]的羅可·A·瑟維迪歐（Rocco A. Servedio）发表论文指出这些方法是有缺陷的：在训练集有错误的标记的情况下，一些提升算法雖會尝试提升这种样本点的正确率，但卻無法产生一个正确率大于1/2的模型。<ref>{{Cite web |url=http://www.phillong.info/publications/LS10_potential.pdf |title=Philip M. Long, Rocco A. Servedio, "Random Classiﬁcation Noise Defeats All Convex Potential Boosters" |access-date=2014-04-17 |archive-date=2021-01-18 |archive-url=https://web.archive.org/web/20210118185614/http://phillong.info/publications/LS10_potential.pdf |dead-url=no }}</ref>

==相關條目==
* [[AdaBoost|AdaBoost]]
* [[随机森林|随机森林]]
* [[Logit模型|Logit模型]]
* [[人工神经网络|人工神经网络]]
* [[支持向量機|支持向量機]]
* [[机器学习|机器学习]]

==实现==
* [[Orange_(software)|Orange]], a free data mining software suite, module [http://orange.biolab.si/docs/latest/reference/rst/Orange.ensemble Orange.ensemble] {{Wayback|url=http://orange.biolab.si/docs/latest/reference/rst/Orange.ensemble |date=20140419013935 }}
* [[Weka_(machine_learning)|Weka]] is a machine learning set of tools that offers variate implementations of boosting algorithms like AdaBoost and LogitBoost
* R package [http://cran.r-project.org/web/packages/gbm/index.html GBM] {{Wayback|url=http://cran.r-project.org/web/packages/gbm/index.html |date=20210307233135 }} (Generalized Boosted Regression Models) implements extensions to Freund and Schapire's AdaBoost algorithm and Friedman's gradient boosting machine.
* jboost; AdaBoost, LogitBoost, RobustBoost, Boostexter and alternating decision trees

== 参考文献 ==
===腳註===
{{Reflist}}

===其他參考資料===
* Yoav Freund and Robert E. Schapire (1997); [http://www.cse.ucsd.edu/~yfreund/papers/adaboost.pdf ''A Decision-Theoretic Generalization of On-line Learning and an Application to Boosting''] {{Wayback|url=http://www.cse.ucsd.edu/~yfreund/papers/adaboost.pdf |date=20081012172439 }}, Journal of Computer and System Sciences, 55(1):119-139
* Robert E. Schapire and Yoram Singer (1999); [http://citeseer.ist.psu.edu/schapire99improved.html ''Improved Boosting Algorithms Using Confidence-Rated Predictors''] {{Wayback|url=http://citeseer.ist.psu.edu/schapire99improved.html |date=20080820080511 }}, Machine Learning, 37(3):297-336

== 外部链接 ==
* Robert E. Schapire (2003); [http://www.cs.princeton.edu/courses/archive/spr08/cos424/readings/Schapire2003.pdf ''The Boosting Approach to Machine Learning: An Overview''] {{Wayback|url=http://www.cs.princeton.edu/courses/archive/spr08/cos424/readings/Schapire2003.pdf |date=20200920123710 }}, MSRI (Mathematical Sciences Research Institute) Workshop on Nonlinear Estimation and Classification
* [https://web.archive.org/web/20140419015925/http://www.cs.princeton.edu/~schapire/boost.html An up-to-date collection of papers on boosting]

{{Authority control}}

[[Category:分類演算法|Category:分類演算法]]
[[Category:集成学习|Category:集成学习]]