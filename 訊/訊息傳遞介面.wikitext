{{NoteTA|G1=IT;G2=USState
|1=zh-cn:明尼阿波利斯; zh-tw:明尼亞波利斯; zh-hk:明尼阿波利斯;}}
'''訊息傳遞介面'''（{{lang-en|Message Passing Interface}}，縮寫MPI）是一個[[平行計算|平行計算]]的[[應用程式接口|應用程式接口]]（API），常在[[超級電腦|超級電腦]]、[[電腦叢集|電腦叢集]]等非共享內存環境程序設計。

==历史==
建立信息传递接口的努力始于1991夏天一小群研究员在奥地利的一个度假山庄开始的讨论。那次讨论之后，于1992年4月29-30号于[[維吉尼亞州|維吉尼亞州]][[威廉斯堡_(弗吉尼亚州)|威廉斯堡]]召开了一次关于分布式内存环境下的信息传递标准设置研讨会。在这次研讨会上讨论了对标准信息传递接口至关重要的一些基本特征，并建立了一个继续标准化此过程的工作组。[[杰克·唐加拉|杰克·唐加拉]], Rolf Hempel, {{le|托尼·黑|Tony Hey}}, and David W. Walker于1992年11月提出了一些初始草稿提议，后被称为MPI1。在1992年11月，一个MPI的工作组会议在[[明尼亞波利斯|明尼亞波利斯]]召开，他们决定了为此标准化过程建立一个更正式的标注。MPI工作组在1993年的头九个月每6个星期见面一次。MPI标准草稿在93年11月的超级计算机会议上提出。在经过一阵子的公众论议后，MPI修改了一些部分，并于1994年6月发布了MPI1.0版本。这些会议和邮件共同建立了MPI论坛，此论坛后来开放至所有高性能计算的成员。

MPI包含了80个人40个组织的共同努力，他们主要都在美国和欧洲。主要的时下电脑供应商也涉入MPI，还有大学的研究员，政府公务员和产业界。

MPI标准定义了核心函式庫的语法和语义，这个函式庫可以被Fortran和C调用构成可移植的信息传递程序。MPI提供了适应各种并行硬件商的基础集，他们都被有效的实现。这导致了是硬件商可以基于这一系列底层标准来建立高层次的惯例，从而为分布式内存交互系统提供他们的并行机。MPI提供了一个简单易用的可移植接口，足够强大到程序员可以用它在高级机器上进行进行高性能信息传递操作。

在建立“真正”的MPI标准过程中，研究员们整合了几个系统最有用的特征到MPI中，而不是用一个系统来适应标准。其特征为IBM，Intel, nCUBE, PVM, Express, P4 and PARMACS等系统所用。

信息传递模式非常之吸引人，皆因它的广泛可移植性，以及能被用于分布式内存/共享内存的多核处理器，工作站网络，和这些架构的组合。信息传递模式可用于多重设定，独立于网络速度和内存架构。

==概述==
MPI是一个跨语言的通讯协议，用于编写并行计算机。支持点对点和广播。MPI是一个信息传递应用程序接口，包括协议和和语义说明，他们指明其如何在各种实现中发挥其特性。MPI的目标是高性能，大规模性，和可移植性。MPI在今天仍为高性能计算的主要模型。

主要的MPI-1模型不包括共享内存概念，MPI-2-{只}-有有限的分布共享内存概念。
但是MPI程序经常在共享内存的机器上运行。在MPI模型周边设计程序比在NUMA架构下设计要好因为MPI鼓励内存本地化。

尽管MPI属于OSI参考模型的第五层或者更高，他的实现可能通过传输层的sockets和Transmission Control Protocol (TCP)覆盖大部分的层。大部分的MPI实现由一些指定惯例集（API）组成，可由C,C++,Fortran,或者有此类库的语言比如C#, Java or Python直接调用。MPI优于老式信息传递库是因为他的可移植性和速度。

== 特色 ==
大多數訊息傳遞介面的實現為[[函式庫|函式庫]]，亦不需要[[編譯器|編譯器]]支持。

== 例子 ==
由多[[行程|行程]]來執行Hello World：

<syntaxhighlight lang=c>
#include <stdio.h>
#include <mpi.h>

int main(int argc, char *argv[])
{
 char processor_name[MPI_MAX_PROCESSOR_NAME];
 int len;

 MPI_Init(&argc, &argv);

 MPI_Get_processor_name(processor_name, &len);
 printf("Hello World from %s\n", processor_name);

 MPI_Finalize();
 
 return 0;
}
</syntaxhighlight>

執行結果：
<pre>
% mpicc hello.c

% cat nodefile
node1
node2

% mpirun -np 1 -hostfile nodefile a.out（由1節點來執行）
Hello World from node1

% mpirun -np 2 -hostfile nodefile a.out（由2節點來執行）
Hello World from node1
Hello World from node2
</pre>

== 實現 ==
* Open MPI－ 是[[自由軟件|自由軟件]]和[[開放源碼|開放源碼]]實現。<ref>{{Cite web |url=http://www.open-mpi.org/ |title=Open MPI: Open Source High Performance Computing |access-date=2011-04-10 |archive-date=2006-07-02 |archive-url=https://web.archive.org/web/20060702080439/http://www.open-mpi.org/ |dead-url=no }}</ref> [[走鵑_(超級電腦)|走鵑]]（2008年6月-2009年11月[[TOP500|TOP500]]第一快的超級電腦）<ref>{{cite web
| url=http://www.open-mpi.org/papers/sc-2008/jsquyres-cisco-booth-talk-1up.pdf
| format=PDF
| author=Jeff Squyres
| publisher=[[Open_MPI|Open MPI]]
| title=Open MPI: 10^15 Flops Can't Be Wrong
| accessdate=2008-11-22
| archive-date=2021-02-23
| archive-url=https://web.archive.org/web/20210223014051/https://www.open-mpi.org/papers/sc-2008/jsquyres-cisco-booth-talk-1up.pdf
| dead-url=no
}}</ref>及[[京_(超級電腦)|京]]（2011年6月至今第一快的超級電腦）也使用Open MPI。<ref>{{cite web
| url=http://www.fujitsu.com/downloads/TC/sc10/programming-on-k-computer.pdf
| publisher=Fujitsu
| title=Programming on K computer
| accessdate=2011-06-24
| archive-date=2020-07-02
| archive-url=https://web.archive.org/web/20200702201411/https://www.fujitsu.com/downloads/TC/sc10/programming-on-k-computer.pdf
| dead-url=no
}}</ref> <ref>{{cite web
| url=http://blogs.cisco.com/performance/open-mpi-powers-8-petaflops/
| publisher=[[思科系統|思科系統]]
| title=Open MPI powers 8 petaflops
| accessdate=2011-06-24
| archive-url=https://web.archive.org/web/20110628064742/http://blogs.cisco.com/performance/open-mpi-powers-8-petaflops/
| archive-date=2011-06-28
| dead-url=yes
}}</ref>
* Intel MPI－[[Intel|Intel]]基於開放源碼的MPICH2與MVAPICH2研發成的MPI。<ref>{{Cite web |url=http://www.intel.com/go/mpi/ |title=Intel MPI Library - Intel Software Network |access-date=2011-04-10 |archive-date=2011-03-12 |archive-url=https://web.archive.org/web/20110312075618/http://www.intel.com/go/mpi/ |dead-url=no }}</ref>
* Platform MPI－Platform公司收購Scali MPI及HP MPI，研發成Platform MPI。<ref>{{Cite web |url=http://www.platform.com/cluster-computing/platform-mpi |title=Platform MPI |access-date=2011-04-10 |archive-url=https://web.archive.org/web/20110711194159/http://www.platform.com/cluster-computing/platform-mpi |archive-date=2011-07-11 |dead-url=yes }}</ref>

== 参考文献 ==
{{Reflist|2}}

== 外部連結 ==
* {{en}}[http://www.mpi-forum.org/ 訊息傳遞介面] {{Wayback|url=http://www.mpi-forum.org/ |date=20210505021518 }}
* {{en}}[https://web.archive.org/web/20120520011000/http://www.aosabook.org/en/openmpi.html Open MPI內部結構]（The Architecture of Open Source Applications, Volume II - ISBN 9781105571817）

{{-}}
{{并行计算}}

[[Category:并发计算|Category:并发计算]]
[[Category:应用程序接口|Category:应用程序接口]]