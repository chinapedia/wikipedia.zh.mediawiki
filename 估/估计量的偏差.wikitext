{{about|统计估计量的偏差|统计学中其他用法|偏差}}

{{NoteTA
|G1 = Math
|T=zh-tw:估計量的偏誤;zh-cn:估计量的偏差
|1 = zh-cn:方差; zh-tw:變異數;
|2= zh-cn:无偏; zh-tw: 不偏
|3= zh-cn:有偏; zh-tw:偏誤
|4= zh-cn: 贝塞尔修正; zh-tw:自由度修正
|5= zh-cn:参数; zh-tw:母數
|6= zh-cn:变量;zh-hk:變量;zh-tw: 變數
|7=zh-cn: 偏差;zh-tw:偏誤
}}

在[[统计学|统计学]]中，[[估计量|估计量]]的'''偏差'''（或'''偏差函数'''）是此估计量的[[期望值|期望值]]与估计参数的真值之差。偏差为零的估计量或决策规则称为'''无偏的'''。否则该估计量是'''有偏的'''。在统计中，“偏差”是一个函数的客观陈述。

偏差也可以相对于[[中位數|中位數]]来衡量，而非相对于均值（期望值），在这种情况下为了与通常的“均值”无偏性区别，称作“中值”无偏。偏差与[[一致估计量|一致性]]相关联，一致估计量都是收敛并且''渐进''无偏的（因此会收敛到正确的值），虽然一致序列中的个别估计量可能是有偏的（只要偏差收敛于零）；参见[[一致估计量|偏差与一致性]]。

当其他量相等时，无偏估计量比有偏估计量更好一些，但在实践中，并不是所有其他统计量的都相等，于是也经常使用有偏估计量，一般偏差较小。当使用一个有偏估计量时，也会估计它的偏差。有偏估计量可能用于以下原因：由于如果不对总体进一步假设，无偏估计量不存在或很难计算（如{{le|标准差的无偏估计|unbiased estimation of standard deviation}}）；由于估计量是中值无偏的，却不是均值无偏的（或反之）；由于一个有偏估计量较之无偏估计量（特别是{{le|收缩估计量|shrinkage estimator}}）可以减小一些[[损失函数|损失函数]]（尤其是[[均方差|均方差]]）；或者由于在某些情况下，无偏的条件太强，而这些无偏估计量没有太大用处。此外，在非线性变换下均值无偏性不会保留，不过中值无偏性会保留（参见[[#变换的效应|变换的效应]]）；例如[[方差|样本方差]]是总体方差的无偏估计量，但它的平方根[[標準差|標準差]]则是总体标准差的有偏估计量。下面会进行说明。

==定义==

设我们有一个参数为实数 ''θ'' 的[[概率模型|概率模型]]，产生观测数据的概率分布 <math>P_\theta(x) = P(x\mid\theta)</math>，而统计量 <math>\hat\theta</math> 是基于任何观测数据 <math>x</math> 下 ''θ'' 的[[估计量|估计量]]。也就是说，我们假定我们的数据符合某种未知分布 <math>P_\theta(x) = P(x\mid\theta)</math>（其中 ''θ'' 是一个固定常数，而且是该分布的一部分，但具体值未知），于是我们构造估计量 <math>\hat\theta</math>，该估计量将观测数据与我们希望的接近 ''θ'' 的值对应起来。因此这个估量的（相对于参数 ''θ''的）'''偏差'''定义为
:<math> \operatorname{Bias}_\theta[\,\hat\theta\,] = \operatorname{E}_\theta[\,\hat{\theta}\,]-\theta = \operatorname{E}_\theta[\, \hat\theta - \theta \,],</math>

其中 <math>\operatorname{E}_\theta</math> 表示分布 <math>P_\theta(x) = P(x\mid\theta)</math> 的[[期望值|期望值]]，即对所有可能的观测值 <math>x</math> 取平均。由于 ''θ'' 对于条件分布 <math>P(x\mid\theta)</math> 是可测的，就有了第二个等号。

对于参数 ''θ'' 的所有值的偏差都等于零的估计量称为'''无偏'''估计量。

在一次关于估计量性质的模拟实验中，估计量的偏差可以用{{le|平均有符号离差|mean signed difference}}来评估。

==例子==

===样本方差===
{{main|样本方差}}
随机变量的[[方差|样本方差]]从两方面说明了估计量偏差：首先，自然估计量（{{lang|en|naive estimator}}）是有偏的，可以通过比例因子校正；其次，无偏估计量的[[均方差|均方差]]（MSE）不是最优的，可以用一个不同的比例因子来最小化，得到一个比无偏估计量的MSE更小的有偏估计量。

具体地说，自然估计量就是将离差平方和加起来然后除以 ''n''，是有偏的。不过除以 ''n'' − 1 会得到一个无偏估计量。相反，MSE可以通过除以另一个数来最小化（取决于分布），但这会得到一个有偏估计量。这个数总会比 ''n'' − 1 大，所以这就叫做{{le|收缩估计量|shrinkage estimator}}，因为它把无偏估计量向零“收缩”；对于正态分布，最佳值为 ''n'' + 1。

设 ''X''<sub>1</sub>, ..., ''X''<sub>''n''</sub> 是[[期望值|期望]]为 ''μ''、[[方差|方差]]为 ''σ''<sup>2</sup> 的[[独立同分布|独立同分布]]（i.i.d.）随机变量。如果[[样本均值|样本均值]]与未修正[[方差|样本方差]]定义为

:<math>\overline{X}=\frac{1}{n}\sum_{i=1}^nX_i, \qquad S^2=\frac{1}{n}\sum_{i=1}^n\left(X_i-\overline{X}\,\right)^2,</math>

则 ''S''<sup>2</sup> 是 ''σ''<sup>2</sup> 的一个有偏估计量，因为
:<math>
    \begin{align}
    \operatorname{E}[S^2]
        &= \operatorname{E}\left[ \frac 1 n \sum_{i=1}^n \big(X_i-\overline{X}\big)^2 \right]
         = \operatorname{E}\bigg[ \frac 1 n \sum_{i=1}^n \bigg((X_i-\mu)-(\overline{X}-\mu)\bigg)^2 \bigg] \\[8pt]
        &= \operatorname{E}\bigg[ \frac 1 n \sum_{i=1}^n \bigg((X_i-\mu)^2 -
                                  2(\overline{X}-\mu)(X_i-\mu) +
                                  (\overline{X}-\mu)^2\bigg) \bigg] \\[8pt]
        &= \operatorname{E}\bigg[ \frac 1 n \sum_{i=1}^n (X_i-\mu)^2 -
                                  \frac 2 n (\overline{X}-\mu) \sum_{i=1}^n (X_i-\mu) +
                                  \frac 1 n (\overline{X}-\mu)^2 \sum_{i=1}^n 1  \bigg] \\[8pt]
        &= \operatorname{E}\bigg[ \frac 1 n \sum_{i=1}^n (X_i-\mu)^2 -
                                  \frac 2 n (\overline{X}-\mu)\sum_{i=1}^n (X_i-\mu) +
                                  \frac 1 n (\overline{X}-\mu)^2 \cdot n\bigg] \\[8pt]
        &= \operatorname{E}\bigg[ \frac 1 n \sum_{i=1}^n (X_i-\mu)^2 -
                                  \frac 2 n (\overline{X}-\mu)\sum_{i=1}^n (X_i-\mu) +
                                  (\overline{X}-\mu)^2 \bigg] \\[8pt]
    \end{align}
</math>

换句话说，未修正的样本方差的期望值不等于总体方差 ''σ''<sup>2</sup>，除非乘以归一化因子。而样本均值是总体均值 ''μ'' 的无偏<ref name="JohnsonWichern2007">{{cite book|author1=Richard Arnold Johnson|author2=Dean W. Wichern|title=Applied Multivariate Statistical Analysis|url=https://books.google.com/books?id=gFWcQgAACAAJ|accessdate=10 August 2012|year=2007|publisher=Pearson Prentice Hall|isbn=978-0-13-187715-3|archive-date=2016-05-29|archive-url=https://web.archive.org/web/20160529154017/https://books.google.com/books?id=gFWcQgAACAAJ|dead-url=no}}</ref>估计量。

''S''<sup>2</sup> 是有偏的原因源于样本均值是 ''μ'' 的{{le|普通最小二乘|ordinary least squares}}（OLS）估计量这个事实：<math>\overline{X}</math> 是令 <math>\sum_{i=1}^n (X_i-\overline{X})^2</math> 尽可能小的数。也就是说，当任何其他数代入这个求和中时，这个和只会增加。尤其是，在选取 <math>\mu \ne \overline{X}</math> 就会得出，

:<math>
        \frac{1}{n}\sum_{i=1}^n (X_i-\overline{X})^2  <  \frac{1}{n}\sum_{i=1}^n (X_i-\mu)^2,
 </math>
于是
:<math>
    \begin{align}
    \operatorname{E}[S^2]
        &= \operatorname{E}\bigg[ \frac{1}{n}\sum_{i=1}^n (X_i-\overline{X})^2 \bigg]
         < \operatorname{E}\bigg[ \frac{1}{n}\sum_{i=1}^n (X_i-\mu)^2 \bigg]  = \sigma^2.
    \end{align}
  </math>

注意到，通常的样本方差定义为
:<math>s^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\overline{X}\,)^2,</math>

而这时总体方差的无偏估计量。可以由下式看出：
:<math>\operatorname{E}\big[ (\overline{X}-\mu)^2 \big] = \frac{1}{n}\sigma^2 .</math>
方差的有偏（未修正）与无偏估计之比称为{{le|贝塞尔修正|Bessel's correction}}。

==参见==
*[[点估计|點估計]]
* {{le|忽略变量偏差|Omitted-variable bias}}
* [[一致估计量|一致估计量]]
* [[估计理论|估计理论]]
* {{le|期望损失|Expected loss}}
* [[期望值|期望值]]
* [[损失函数|损失函数]]
* [[中位數|中位數]]
* [[决策论|决策论]]
* {{le|乐观偏差|Optimism bias}}

{{portal box|科学|统计学}}

==参考文献==
* [https://web.archive.org/web/20110310043642/http://www.universityofcalifornia.edu/senate/inmemoriam/georgewbrown.htm Brown, George W.] "On Small-Sample Estimation." ''The Annals of Mathematical Statistics'', vol. 18, no. 4 (Dec., 1947), pp. 582–585. {{JSTOR|2236236}}.
* {{tsl|en|Erich Leo Lehmann||Lehmann, E. L.}} "A General Concept of Unbiasedness" ''The Annals of Mathematical Statistics'', vol. 22, no. 4 (Dec., 1951), pp. 587–592. {{JSTOR|2236928}}.
* {{tsl|en|Allan Birnbaum||Allan Birnbaum}}, 1961. "A Unified Theory of Estimation, I", ''The Annals of Mathematical Statistics'', vol. 32, no. 1 (Mar., 1961), pp. 112–135.
* Van der Vaart, H. R., 1961. "Some Extensions of the Idea of Bias" ''The Annals of Mathematical Statistics'', vol. 32, no. 2 (June 1961), pp. 436–447.
* Pfanzagl, Johann. 1994. ''Parametric Statistical Theory''. Walter de Gruyter.
* {{cite book  | last1 = Stuart  | first1 = Alan  | first2 = Keith  | last2 = Ord   | first3=Steven [F.]  | last3=Arnold  | title=Classical Inference and the Linear Model  | series = Kendall's Advanced Theory of Statistics  | volume = 2A  | year = 2010  | publisher = Wiley  | isbn=0-4706-8924-2 }}.
* {{cite book |last1 = Voinov |first1 = Vassily [G.] |last2= Nikulin |first2= Mikhail [S.] |title= Unbiased estimators and their applications| volume = 1: Univariate case |year = 1993 |publisher = Kluwer Academic Publishers | location= Dordrect | ISBN= 0-7923-2382-3}}
* {{cite book |last1 = Voinov |first1 = Vassily [G.] |last2= Nikulin |first2= Mikhail [S.] |title= Unbiased estimators and their applications| volume = 2: Multivariate case |year = 1996 |publisher = Kluwer Academic Publishers | location= Dordrect| ISBN= 0-7923-3939-8}}
* {{cite book | last1 =  Klebanov |first1 = Lev [B.] | last2 = Rachev |first2 = Svetlozar [T.] | last3 = Fabozzi |first3 =  Frank [J.] | title = Robust and Non-Robust Models in Statistics | year = 2009 |publisher = Nova Scientific Publishers | location = New York | ISBN = 978-1-60741-768-2}}

==外部链接==
* {{springer|title=Unbiased estimator|id=p/u095070}}

{{認知偏誤}}

[[Category:統計理論|Category:統計理論]]
[[Category:估计理论|Category:估计理论]]
[[category:認知偏誤|category:認知偏誤]]