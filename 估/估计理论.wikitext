{{expert|time=2018-07-12T08:18:42+00:00}}
{{Cleanup-rewrite}}

'''估计理论'''是[[统计学|统计学]]和[[信号处理|信号处理]]中的一个分支，主要是通过测量或经验数据来估计[[概率分布|概率分布]][[参数|参数]]的数值。这些参数描述了实质情况或实际对象，它们能够回答[[估计函数|估计函数]]提出的问题。

例如，估计投票人总体中，给特定候选人投票的人的比例。这个比例是一个不可观测的参数，因为投票人总体很大；估计值建立在投票者的一个小的随机采样上。

又如，雷达的目的是物体（飞机、船等）的定位。这种定位是通过分析收到的回声（回波）来实现的，定位提出的问题是“飞机在哪里？”为了回答这个问题，必须估计飞机到雷达之间的距离。如果雷达的绝对位置是已知的，那么飞机的绝对位置也是可以确定的。

在估计理论中，通常假定信息隐藏在包含[[雜訊_(通訊學)|雜訊]]的[[信号|信号]]中。噪声增加了[[不确定性|不确定性]]，如果没有不确定性，那么也就没有必要估计了。

== 使用估计理论的领域==
有非常多的领域使用参数估计理论。这些领域包括（当然不局限于以下列出的领域）:

* 信号处理
** [[X射線斷層成像|X射線斷層成像]]
** [[脑电图|脑电图]]
** [[心电图|心电图]]
** [[核磁共振成像|核磁共振]]
** [[医学超声波扫描术|医学超声波扫描术]]
** [[雷达|雷达]]、[[声纳|声纳]]、[[地震學|地震學]]——物件的定位
** 噪声[[方差|方差]]
** 参数化（例如[[周期图|周期图]]和[[相关图|相关图]]谱）分析
** 非参数化（例如[[MUSIC|MUSIC]]、[[Root-MUSIC|Root-MUSIC]]和[[ESPRIT|ESPRIT]]）谱分析 
** [[维纳滤波|维纳滤波]]
** [[粒子滤波器|粒子滤波器]]
* [[临床试验|临床试验]]
* [[民意调查|民意调查]]
* [[质量控制|质量控制]]
* [[通讯|通讯]]
** [[信道_(通讯)|信道]]参数
** DC增益（请看下边的例子）
* [[控制理论|控制理论]]
** [[卡尔曼滤波|卡尔曼滤波]]
** 随时间改变的[[执行器|执行器]]（英文：{{lang|en|Actuator}}）
* [[网络入侵侦查系统|网络入侵侦查系统]]

测量参数包含噪声或者其他不确定性。通过统计[[概率|概率]]，可以求得最优化的解，用来从数据中提取尽可能多的[[信息|信息]]。

== 估计过程 ==

估计理论的全部目的都是获取一个估计函数，最好是一个可以实现的估计函数。估计函数输入测量数据，输出相应参数的估计。

我们通常希望估计函数能最优，一个最优的估计意味着所有的信息都被提取出来了；如果还有信息没有提取出来，那就意味着它不是最优的。

一般来说，求估计函数需要三步：

* 为了实现一个预测单个或者多个参数的所期望的估计器，首先需要确定系统的模型。这个模型需要将需要建模的过程以及不确定性和和噪声融合到一起，这个模型将描述参数应用领域的物理场景。
* 在确定模型之后，需要确定估计器的限制条件。这些限制条件可以通过如[[Cramér-Rao不等式|Cramér-Rao不等式]]这样的方法找到。
* 下一步，需要开发一个估计器或者应用一个已知的对于模型有效的估计器。这个估计器需要根据限制条件进行测试以确定它是否是最优估计器，如果是的话，它就是最好的估计器。
* 最后，在估计器上运行试验或者仿真以测试性能。

当实现一个估计器之后，实际的数据有可能证明推导出估计器的模型是不正确的，这样的话就需要重复上面的过程重新寻找估计器。不能实现的估计器需要抛弃，然后开始一个新的过程。总的来说，估计器根据实际测量的数据预测物理模型的参数。

== 基础 ==

为了建立一个模型，需要知道几项统计“因素”。为了保证预测在数学上是可以追踪的而不是仅仅基于“内心感受”来说这是必需的。

第一个是从大小为 <math>N</math> 的[[随机矢量|随机矢量]]中取出的[[统计采样|统计采样]]，将它们放到一个[[矢量|矢量]]中，

: <math>\mathbf{x} = \begin{bmatrix} x[0] \\ x[1] \\ \vdots \\ x[N-1] \end{bmatrix}</math>.

第二，有相应的 <math>M</math> 参数

: <math>\mathbf{\theta} = \begin{bmatrix} \theta_1 \\ \theta_2 \\ \vdots \\ \theta_M \end{bmatrix}</math>,

它需要根据[[概率密度函数|概率密度函数]]（pdf）或者[[概率聚集函数|概率聚集函数]]（:en:[[probability_mass_function|probability mass function]]）（pmf）建立

: <math>p(\mathbf{x} | \mathbf{\theta})</math>.

参数本身还可能有一个概率分布（[[Bayesian_statistics|Bayesian statistics]]），需要定义[[epistemic_probability|epistemic probability]]

: <math>\pi( \mathbf{\theta})</math>.

模型形成之后的目标就是预测参数，通常表示为 <math>\hat{\mathbf{\theta}}</math>，其中“hat”表示预测值。

一个普通的估计器是[[最小均方误差|最小均方误差]]（MMSE）估计器，它利用了参数估计值与实际值之间的误差

: <math>\mathbf{e} = \hat{\mathbf{\theta}} - \mathbf{\theta}</math>

作为优化的基础。在最小均方误差估计器中误差进行取平方、最小化。

==估计函数（估计子）==
以下是一些相关的[[估计函数|估计函数]]以及相关的主题
*[[最大似然估計|最大似然估計]]（Maximum likelihood estimation，簡稱MLE）
*[[矩估計|矩估計]]（Method of moments estimators，簡稱MME）
*[[Cramér-Rao不等式|Cramér-Rao不等式]]
*[[最小均方差|最小均方差]]（Minimum mean squared error，简称MMSE）
*[[最大后验概率|最大后验概率]]（Maximum a posteriori probability，簡稱MAP）
*[[最小方差非偏估计|最小方差非偏估计]]（Minimum variance unbiased estimator，简称MVUE）
*[[最佳线性非偏估计|最佳线性非偏估计]]（BLUE）
*非偏估计，见[[偏差|偏差]] ([[统计学|统计学]])。
*Particle filter
*Markov chain Monte Carlo，简称MCMC
*[[卡尔曼滤波|卡尔曼滤波]]
*[[维纳滤波|维纳滤波]]

==例子：高斯白噪声中的直流增益==

让我们来看一个接收到的<math>N</math>个[[statistical_independence|独立]][[采样点|采样点]]的[[离散信号|离散信号]]<math>x[n]</math>，它由一个[[直流|直流]]增益<math>A</math>和''已知''[[方差|方差]]为<math>\sigma^2</math> (例如，<math>\mathcal{N}(0, \sigma^2)</math>）的[[叠加白噪声|叠加白噪声]]<math>w[n]</math>组成。

由于方差已经知道，所以仅有的未知参数就是<math>A</math>。

于是信号的模型是
: <math>x[n] = A + w[n] \quad n=0, 1, \dots, N-1</math>

两个可能的估计器是：
* <math>\hat{A}_1 = x[0]</math>
* <math>\hat{A}_2 = \frac{1}{N} \sum_{n=0}^{N-1} x[n]</math>它是[[采样平均|采样平均]]

这两个估计器都有一个[[平均值|平均值]]<math>A</math>，这可以通过代入每个估计器的[[期望|期望]]得到

:<math>\mathrm{E}\left[\hat{A}_1\right] = \mathrm{E}\left[ x[0] \right] = A</math>
和
:<math>
\mathrm{E}\left[ \hat{A}_2 \right]
=
\mathrm{E}\left[ \frac{1}{N} \sum_{n=0}^{N-1} x[n] \right]
=
\frac{1}{N} \left[ \sum_{n=0}^{N-1} \mathrm{E}\left[ x[n] \right] \right]
=
\frac{1}{N} \left[ N A \right]
=
A
</math>

在这一点上，这两个估计器看起来是一样的。但是，当比较方差部分的时候它们之间的不同就很明显了。

:<math>\mathrm{var} \left( \hat{A}_1 \right) = \mathrm{var} \left( x[0] \right) = \sigma^2</math>
和
:<math>
\mathrm{var} \left( \hat{A}_2 \right)
=
\mathrm{var} \left( \frac{1}{N} \sum_{n=0}^{N-1} x[n] \right)
=
\frac{1}{N^2} \left[ \sum_{n=0}^{N-1} \mathrm{var}(x[n]) \right]
=
\frac{1}{N^2} \left[ N \sigma^2 \right]
=
\frac{\sigma^2}{N}
</math>

看起来采样平均是一个更好的估计器，因为方差部分<math>N \to \infty</math>趋向于0。

===最大似然估计===

使用[[最大似然估计|最大似然估计]]继续上面的例子，噪声在一个采样点<math>w[n]</math>的[[概率密度函数|概率密度函数]]（pdf）是

:<math>p(w[n]) = \frac{1}{\sigma \sqrt{2 \pi}} \exp\left(- \frac{1}{2 \sigma^2} w[n]^2 \right)</math>

这样<math>x[n]</math>的概率变为（<math>x[n]</math>可以认为是<math>\mathcal{N}(A, \sigma^2)</math>）

:<math>p(x[n]; A) = \frac{1}{\sigma \sqrt{2 \pi}} \exp\left(- \frac{1}{2 \sigma^2}(x[n] - A)^2 \right)</math>

由于相互独立，<math>\mathbf{x}</math>的概率变为

:<math>
p(\mathbf{x}; A)
=
\prod_{n=0}^{N-1} p(x[n]; A)
=
\frac{1}{\left(\sigma \sqrt{2\pi}\right)^N}
\exp\left(- \frac{1}{2 \sigma^2} \sum_{n=0}^{N-1}(x[n] - A)^2 \right)
</math>

概率密度函数取[[自然对数|自然对数]]

:<math>
\ln p(\mathbf{x}; A)
=
-N \ln \left(\sigma \sqrt{2\pi}\right)
- \frac{1}{2 \sigma^2} \sum_{n=0}^{N-1}(x[n] - A)^2
</math>

于是最大似然估计器是

:<math>\hat{A} = \arg \max \ln p(\mathbf{x}; A)</math>

对数最大似然函数取一阶[[导数|导数]]

:<math>
\frac{\partial}{\partial A} \ln p(\mathbf{x}; A)
=
\frac{1}{\sigma^2} \left[ \sum_{n=0}^{N-1}(x[n] - A) \right]
=
\frac{1}{\sigma^2} \left[ \sum_{n=0}^{N-1}x[n] - N A \right]
</math>

并且将它赋值为0

:<math>
0
=
\frac{1}{\sigma^2} \left[ \sum_{n=0}^{N-1}x[n] - N A \right]
=
\sum_{n=0}^{N-1}x[n] - N A
</math>

这就得到最大似然估计器

:<math>
\hat{A} = \frac{1}{N} \sum_{n=0}^{N-1}x[n]
</math>

它是一个简单的采样平均。

从这个例子中，我们发现对于带有固定未知直流增益的AWGN的<math>N</math>个采样点来说采样平均就是最大似然估计器。

===Cramér-Rao下限===

为了找到采样平均估计器的[[Cramér-Rao下限|Cramér-Rao下限]]（CRLB），需要找到Fisher information数

:<math>
\mathcal{I}(A)
=
\mathrm{E}
\left(
 \left[
  \frac{\partial}{\partial\theta} \ln p(\mathbf{x}; A)
 \right]^2
\right)
=
-\mathrm{E}
\left[
 \frac{\partial^2}{\partial\theta^2} \ln p(\mathbf{x}; A)
\right]
</math>

从上面得到

:<math>
\frac{\partial}{\partial A} \ln p(\mathbf{x}; A)
=
\frac{1}{\sigma^2} \left[ \sum_{n=0}^{N-1}x[n] - N A \right]
</math>

取二阶导数
:<math>
\frac{\partial^2}{\partial A^2} \ln p(\mathbf{x}; A)
=
\frac{1}{\sigma^2}(- N)
=
\frac{-N}{\sigma^2}
</math>

发现负的期望值是无关紧要的（{{lang|en|trivial}}），因为它现在是一个确定的常数

<math>
-\mathrm{E}
\left[
 \frac{\partial^2}{\partial A^2} \ln p(\mathbf{x}; A)
\right]
=
\frac{N}{\sigma^2}
</math>

最后，将Fisher information代入 

:<math>
\mathrm{var}\left( \hat{A} \right)
\geq
\frac{1}{\mathcal{I}}
</math>

得到

:<math>
\mathrm{var}\left( \hat{A} \right)
\geq
\frac{\sigma^2}{N}
</math>

将这个值与前面确定的采样平均的变化比较显示对于所有的<math>N</math>和<math>A</math>来说采样平均都是''等于''Cramér-Rao下限。

采样平均除了是[[最大似然|最大似然]]估计器之外还是[[最小变化无偏估计器|最小变化无偏估计器]]（MVUE）。

这个直流增益 + WGN的例子是Kay的''统计信号处理基础''中一个例子的再现。

==相关书籍==
* ''Fundamentals of Statistical Signal Processing: Estimation Theory'' by Steven M. Kay (ISBN 0-13-345711-7)
* ''An Introduction to Signal Detection and Estimation'' by H. Vincent Poor (ISBN 0-38-794173-8)
* ''Detection, Estimation, and Modulation Theory, Part 1'' by Harry L. Van Trees (ISBN 0-47-109517-6; [https://web.archive.org/web/20050428233957/http://gunston.gmu.edu/demt/demtp1/ website])

==参见==
* [[偏差|偏差]]
* [[检测理论|检测理论]]
* [[信息论|信息论]]
* [[最大似然估计|最大似然估计]]
* [[矩方法|矩方法]]
* [[最小均方差|最小均方差]]（MMSE）
* [[最大后验概率|最大后验概率]]（MAP）
* [[卡尔曼滤波|卡尔曼滤波]]
* [[维纳滤波|维纳滤波]]
* [[最小平方頻譜分析法|最小平方頻譜分析法]]（LSSA）
{{DSP}}

[[Category:估计理论|*]]
[[Category:信号处理|Category:信号处理]]
[[Category:统计学|Category:统计学]]