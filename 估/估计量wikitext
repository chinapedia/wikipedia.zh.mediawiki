{{NoteTA
|G1=Math
|1=zh-cn: 参数;zh-tw:母數;zh-hant:參數
}}
{{Expand English|Estimator}}
在[[统计学|统计学]]中，'''估計量'''(Estimator)，亦稱'''推定量'''，是基于观测数据计算一个已知量的估计值的法则：于是[[估计量|估计量]]（estimator）、[[被估量|被估量]]（estimand）和[[估计值|估计值]]（estimate）是有区别的。

估计量用来估计未知总体的[[母數|母數]]，它有时也被称为'''估计子'''；一次'''估计'''是指把这个函数应用在一组已知的数据集上，求函数的结果。对于给定的参数，可以有许多不同的估计量。我们通过一些选择标准从它们中选出较好的估计量，但是有时候很难说选择这一个估计量比另外一个好。

== 定义 ==
假设存在一个固定的待估''参数''<math> \theta</math>。那么"估计量"是[[样本空间|样本空间]]映射到''样本估计值''的一个函数。<math> \theta</math>的一个估计量记为<math>\widehat{\theta}</math>。

易用随机变量的代数来阐述这个理论：如果用''X''来标记对应观测数据的[[随机变量|随机变量]]，估计量（本身视为随机变量）的符号表示为该随机变量的函数，<math>\widehat{\theta}(X)</math>。对特定观测数据集（即对于''X''=''x''）而言，其估计值为一固定值<math>\widehat{\theta}(x)</math>。通常使用简化标记<math>\widehat{\theta}</math>表示随机变量，但这容易造成误解。

== 量化特性 ==
以下定义和属性是相关的。<ref>Jaynes (2007), p.172.</ref>

===误差===
对于一个给定样本<math> x </math>，估计量<math>\widehat{\theta}</math>的「[[误差|误差]]」定义为
:<math>e(x)=\widehat{\theta}(x) - \theta,</math>
其中<math>\theta</math>是待估参数。注意误差''e''不仅取决于估计量（估计公式或过程），还取决于样本。
===均方误差===
估计量<math>\hat{\theta}</math>的均方误差被定义为误差的平方的期望值，即为：

: <math>MSE(\hat{\theta})=E[(\hat{\theta}(x)-\theta)^2]</math>

它用来显示估计值的集合与被估计单个参数的平均差异。试想下面的类比：假设“参数”是靶子的靶心，“估计量”是向靶子射箭的过程，而每一支箭则是“估计值”（样本）。那么，高均方误差就意味着每一支箭离靶心的平均距离较大，低均方误差则意味着每一支箭离靶心的平均距离较小。箭支可能集聚，也可能不。比如说，即使所有箭支都射中了同一个点，同时却严重偏离了靶子，均方误差相对来说依然很大。然而要注意的是，如果均方误差相对较小，箭支则更有可能集聚（而不是离散）。<!--
The ''[[mean_squared_error|mean squared error]]'' of <math>\widehat{\theta}</math> is defined as the expected value(probability-weighted average, over all samples) of the squared errors; that is,
:<math>\operatorname{MSE}(\widehat{\theta}) = \operatorname{E}[(\widehat{\theta}(X) - \theta)^2]</math>。
It is used to indicate how far, on average, the collection of estimates are from the single parameter being estimated. Consider the following analogy. Suppose the parameter is the bull's-eye of a target, the estimator is the process of shooting arrows at the target, and the individual arrows are estimates (samples). Then high MSE means the average distance of the arrows from the bull's-eye is high, and low MSE means the average distance from the bull's-eye is low. The arrows may or may not be clustered. For example, even if all arrows hit the same point, yet grossly miss the target, the MSE is still relatively large. Note, however, that if the MSE is relatively low, then the arrows are likely more highly clustered (than highly dispersed).-->

===抽样偏差===<!--
For a given sample <math> x \ </math>, the ''[[sampling_deviation|sampling deviation]]'' of the estimator <math>\widehat{\theta}</math> is defined as
:<math>d（x）=\widehat{\theta}（x） - \operatorname{E}( \widehat{\theta}（X） ) =\widehat{\theta}(x) - \operatorname{E}( \widehat{\theta} ),</math>
where <math> \operatorname{E}( \widehat{\theta}(X) ) </math> is the [[expected_value|expected value]] of the estimator. Note that the sampling deviation, ''d'', depends not only on the estimator, but on the sample.-->
===方差===<!--
The ''[[variance|variance]]'' of <math>\widehat{\theta}</math> is simply the expected value of the squared sampling deviations; that is, <math>\operatorname{var}(\widehat{\theta}) = \operatorname{E}[(\widehat{\theta} - \operatorname{E}(\widehat{\theta}) )^2]</math>. It is used to indicate how far, on average, the collection of estimates are from the ''expected value'' of the estimates. Note the difference between MSE and variance. If the parameter is the bull's-eye of a target, and the arrows are estimates, then a relatively high variance means the arrows are dispersed, and a relatively low variance means the arrows are clustered. Some things to note: even if the variance is low, the cluster of arrows may still be far off-target, and even if the variance is high, the diffuse collection of arrows may still be unbiased. Finally, note that even if all arrows grossly miss the target, if they nevertheless all hit the same point, the variance is zero.-->
===偏差===<!--
The ''[[bias_of_an_estimator|bias]]'' of <math>\widehat{\theta}</math> is defined as <math>B(\widehat{\theta}) = \operatorname{E}(\widehat{\theta}) - \theta</math>. It is the distance between the average of the collection of estimates, and the single parameter being estimated. It also is the expected value of the error, since <math> \operatorname{E}(\widehat{\theta}) - \theta = \operatorname{E}(\widehat{\theta} - \theta ) </math>. If the parameter is the bull's-eye of a target, and the arrows are estimates, then a relatively high absolute value for the bias means the average position of the arrows is off-target, and a relatively low absolute bias means the average position of the arrows is on target. They may be dispersed, or may be clustered.  The relationship between bias and variance is analogous to the relationship between [[accuracy_and_precision|accuracy and precision]].

The estimator <math>\widehat{\theta}</math> is an ''[[estimator_bias|unbiased estimator]]'' of <math> \theta \ </math> [[if_and_only_if|if and only if]] <math>B(\widehat{\theta}) = 0</math>.  Note that bias is a property of the estimator, not of the estimate. Often, people refer to a "biased estimate" or an "unbiased estimate," but they really are talking about an "estimate from a biased estimator," or an "estimate from an unbiased estimator." Also, people often confuse the "error" of a single estimate with the "bias" of an estimator. Just because the error for one estimate is large, does not mean the estimator is biased. In fact, even if all estimates have astronomical absolute values for their errors, if the expected value of the error is zero, the estimator is unbiased. Also, just because an estimator is biased, does not preclude the error of an estimate from being zero (we may have gotten lucky). The ideal situation, of course, is to have an unbiased estimator with low variance, and also try to limit the number of samples where the error is extreme(that is, have few outliers). Yet unbiasedness is not essential. Often, if just a little bias is permitted, then an estimator can be found with lower MSE and/or fewer outlier sample estimates.

An alternative to the version of "unbiased" above, is "median-unbiased", where the [[median|median]] of the distribution of estimates agrees with the true value; thus, in the long run half the estimates will be too low and half too high. While this applies immediately only to scalar-valued estimators, it can be extended to any measure of [[central_tendency|central tendency]] of a distribution: see [[Bias_of_an_estimator#Median-unbiased_estimators,_and_bias_with_respect_to_other_loss_functions|median-unbiased estimators]].

===Relationships among the quantities===
*The MSE, variance, and bias, are related: <math>\operatorname{MSE}(\widehat{\theta}) = \operatorname{var}(\widehat\theta) + (B(\widehat{\theta}))^2,</math> i.e.  mean squared error = variance + square of bias. In particular, for an unbiased estimator, the variance equals the MSE.
*The [[standard_deviation|standard deviation]] of an estimator of ''θ''(the [[square_root|square root]] of the variance), or an estimate of the standard deviation of an estimator of ''θ'', is called the ''[[Standard_error_(statistics)|standard error]]'' of ''θ''.-->
<math>bias(\hat{\theta}(x))=E[\hat{\theta}(x)]-{\theta}</math>

== 行为特性 ==
=== 一致性 ===
一致估计量序列是一列随着序号（通常是样本容量）无限增大时[[依概率收敛|依概率收敛]]于被估量的估计量序列。换句话说，增加样本容量增大了估计量接近总体参数的概率。

在数学上，一个估计量序列{{nowrap|{''t<sub>n</sub>''; ''n'' ≥ 0}}}是参数''θ''的一致估计量当且仅当对于所有{{nowrap|''ϵ'' > 0}}，不管多小，我们都有
:<math>
\lim_{n\to\infty}\Pr\left\{
\left
|t_n-\theta\right|<\epsilon
\right\}=1.
</math>

就如，一个人不断地抛硬币，随着次数的增多，任何一面出现的概率（机率）就会趋于0.5。那么这个0.5就是这个抛硬币事件中任何一面出现概率的一致估计量，或者说一致估计值。

== 参见 ==
*[[点估计|點估計]]
*[[估計量的偏誤|估計量的偏誤]]
* [[最大似然估计|最大似然估计]]
* [[矩估计|動差法]]、[[广义矩估计|廣義動差法]]
* [[最小均方误差|最小均方誤差法]]（MMSE）
* [[最小方差無偏估計|最小方差无偏估計式]]（MVUE）
* [[最佳线性无偏预测|最佳線性无偏估計]]（BLUE）
* [[卡尔曼滤波|卡尔曼滤波]]
* [[维纳滤波|维纳滤波]]

== 外部链接 ==
* [https://web.archive.org/web/20060512225028/http://www.ds.unifi.it/VL/VL_EN/point/point1.html 关于统计函数的课程]

[[Category:统计学|Category:统计学]]
[[Category:函数|Category:函数]]