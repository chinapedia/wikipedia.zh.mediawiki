'''問答系統'''(Question Answering System，QA System)，是未來[[自然語言處理|自然語言處理]]的明日之星。問答系統外部的行為上來看，其與目前主流[[資訊檢索|資訊檢索]]技術有兩點不同：首先是查詢方式為完整而口語化的問句，再來則是其回傳的為高精準度網頁結果或明確的答案字串。以Ask Jeeves<ref>{{Cite web |url=http://www.ask.com/ |title=Ask Jeeves |accessdate=2008-08-21 |archive-date=2011-02-10 |archive-url=https://web.archive.org/web/20110210233339/http://www.ask.com/ |dead-url=no }}</ref>為例，使用者不需要思考該使用甚麼樣的問法才能夠得到理想的答案，只需要用口語化的方式直接提問如「請問誰是[[美國|美國]][[總統|總統]]？」即可。而系統在瞭解使用者問句後，會非常清楚地回答「[[唐納德·川普|特朗普]]是[[美國|美國]][[總統|總統]]」。面對這種系統，使用者不需要費心去一一檢視[[搜尋引擎|搜尋引擎]]回傳的網頁，對於資訊檢索的效率與資訊的普及都有很大幫助。從系統內部來看，問答系統使用了大量有別於傳統[[資訊檢索|資訊檢索]]系統[[自然語言處理|自然語言處理]]技術，如自然語言剖析（Natural Language Parsing）、問題分類（Question Classification）、專名辨識（Named Entity Recognition）等等。少數系統<ref>{{Cite web |url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.6.538 |title=Harrabagiu, S., Moldovan, D., Clark, C., Bowden, M., Williams, J., and Bensley, J. "Answer Mining by Combining Extraction Techniques with Abductive Reasoning," Proceedings of TREC, 2003. |accessdate=2008-08-21 |archive-date=2020-12-14 |archive-url=https://web.archive.org/web/20201214142655/http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.6.538 |dead-url=no }}</ref>甚至會使用複雜的邏輯推理機制，來區隔出需要推理機制才能夠區隔出來的答案。在系統所使用的資料上，除了傳統[[資訊檢索|資訊檢索]]會使用到的資料外（如字典），問答系統還會使用[[本体论_(信息科学)|本體論]]等語义資料，或者利用網頁來增加資料的豐富性。

截至目前為止，最著名的問答系統應屬IBM的[[沃森_(人工智能程序)|-{zh-cn:沃森; zh-tw:華生;}-]]系統。該系統在2011年於Jeopardy節目中，與人類同場較勁，並獲得最後的勝利。

==前言==
早在1961年，Green <ref>[http://portal.acm.org/citation.cfm?id=21922.24354 Green, B., Wolf, A., Chomsky, C., and Laughery, K. "BASEBALL: an automatic question answerer," in: Readings in natural language processing, Morgan Kaufmann Publishers Inc., 1986, pp. 545-549.]</ref>就發展了第一個問答系統，用來回答單季[[美國職棒大聯盟|美國職棒大聯盟]]相關比賽問題。該系統執行於[[IBM|IBM]] 7090平台，以今日的觀點來看，其硬體資源相當貧乏，但由於問答的範圍狹小，系統正確率尚能達到令人滿意的地步。近年來，網際網路成長快速，在資訊、流量、使用人數、以及應用領域上都有驚人的發展。截至目前為止<ref>[http://portal.acm.org/citation.cfm?id=1062789 Gulli, A., and Signorini, A. "The Indexable Web is More than 11.5 billion pages," Poster proceedings of the 14th international conference on World Wide Web, 2004.]</ref>，[[Google|Google]]已經索引了超過八十億個網頁資料；MSN BETA、[[Yahoo|Yahoo]]也分別有四十億與二十億個網頁資料。整個網際網路總索引量則高達一百一十五億個網頁，頗為驚人。此趨勢帶動了近幾年問答系統的研究風潮，盼能解決網路搜尋如大海撈針的困境。報導指出<ref>{{Cite web |url=http://www.internetworldstats.com/stats7.htm |title=Internet Users By Language, Internet World Stats |accessdate=2008-08-21 |archive-date=2012-04-26 |archive-url=https://web.archive.org/web/20120426122721/http://www.internetworldstats.com/stats7.htm |dead-url=no }}</ref>，從2000年到2005年，網路人口成長了1.7倍，在前十名的語言中，中文人口成長率為284.8%，高達一億兩千萬，遠遠超過英文人口的成長率。

==問答系統的分類==
我們可以從知識領域、答案來源等角度來替問答系統做分類。從知識領域來看，可分為「封閉領域」以及「開放領域」兩類系統。封閉領域系統專注於回答特定領域的問題，如醫藥或特定公司等。由於問題領域受限，系統有比較大的發揮空間，可以導入如專屬本體論等知識，或將答案來源全部轉換成結構性資料，來有效提升系統的表現。開放領域系統則希望不設限問題的內容範圍，[[天文|天文]][[地理|地理]]無所不問。系統中所有知識與元件都必須儘量做到與領域不相關，當然難度也相對地提高。

若根據答案來源來區分，可分為「資料庫問答」、「常問問題問答」、「新聞問答」、「網際網路問答」等系統。[[資料庫|資料庫]]是最常見的結構化資料儲存媒介。雖然透過操控[[SQL|SQL]]語言便能夠有效率地存取資料，但有些系統試圖提供更直覺的自然語言查詢介面，希望能進一步降低學習門檻。1970年代的LUNAR系統<ref>{{Cite web |url=http://citeseerx.ist.psu.edu/showciting;jsessionid=5A4AD06E732A7D14D8E7CE76573FC6C8?cid=1083695 |title=Woods, W.A. "Progress in Natural Language Understanding - an application to lunar geology," American Federation of Information Processing Societies, 1973, pp. 441-450. |accessdate=2008-08-21 |archive-date=2021-01-10 |archive-url=https://web.archive.org/web/20210110003205/http://citeseerx.ist.psu.edu/showciting;jsessionid=5A4AD06E732A7D14D8E7CE76573FC6C8?cid=1083695 |dead-url=no }}</ref>算是早期成功的案例，其正確答題率可以達到百分之七十，可回答[[月球|月球]][[隕石|隕石]]相關資料。[[微軟|微軟]]的English Query<ref>{{Cite web |url=http://www.amazon.co.uk/Server-Developers-Analysis-Services-Guides/dp/0782129579 |title=Gunderloy, M., and Sneath, T. SQL Server Developer's Guide to OLAP with Analysis Services Sybex, 2001. |accessdate=2008-08-21 |archive-date=2021-01-10 |archive-url=https://web.archive.org/web/20210110003212/https://www.amazon.co.uk/Server-Developers-Analysis-Services-Guides/dp/0782129579 |dead-url=no }}</ref>則是近期的一個商業產品。English Query在剖析完英文問句後，會根據底層資料庫結構，自動產生出相對應的SQL查詢。雖然有這些成功系統案例，但資料庫問答系統似乎很難被大眾所接受，其中一個因素可能是因為對於結構化資料來說，結構化的查詢介面在查詢上更為方便。常問問題（Frequently Asked Questions, FAQs）是公司或者長期經營領域中常見的重要資源。一份FAQ資料包含了一個問句以及相對應的答案描述。FAQ問答系統的主要責任在比對使用者問句與現有FAQ問句的相似度，此與其他問答系統著重在答案語料中擷取答案的作法不同。另一種重要的系統為新聞問答系統。今日[[新聞|新聞]][[傳播媒體|媒體]]都已經[[數位化|數位化]]了，每日累積所產生的新聞資訊量是相當可觀的，加上新聞的內容廣泛豐富，作為開放領域問答系統的答案來源是最適合不過的。這樣的特性使得此類系統的評估較為容易，因此稍後會提到的國際評估會議都是採用此類系統作為評估對象。最後一類的是網際網路問答系統，這些系統利用搜尋引擎回傳的結果網頁，從中擷取答案。主要挑戰在於如何處理網路多異質性的資料，以及高雜訊網頁過濾等問題。
==問題類型==
問答系統接受的是自然語言問句，為了有效控制研究變因，多會訂定可接受的問題類型來限制研究範圍。最基本的類型為「仿真陳述問答」（Factoid Question Answering），此類系統根據答案語料所述資訊，取出一小段字串作為答案。由於答案的正確與否是根據答案語料的內容來決定，在現實生活中不一定為真，故稱為仿真陳述問答。有些系統把問答範圍進一步縮小，限定在人、地、組織等明確的專有名詞上。若此類系統有能力回答如「請列舉美國歷屆總統」這種清單型的問句，則稱為「清單問答」（List Question Answering）；若能回答定義問題，則稱為「定義問答」（Definition Question Answering）；以此類推還能定義出其他類型的問題。除了這些與問句資訊內容有關的類型外，最近評鑑會議引進如「時間限制問題」（Temporally Restricted Questions）與「序列問題」（Series of Questions）等複雜的問題類型。時間限制型的問題會在問句中明確指出答案的時間範圍限制，比如說以「民國九十年時的[[國民黨|國民黨]]主席是誰」這問句來說，系統必須有根據答案語料結構化資料，或上下文來推論正確答案的能力。序列問題則把問答系統未來的應用定位在互動式的系統上。經過來回多次問答的方式來滿足使用者的資訊需求。瞭解這些問題類型分類，有助於研究範圍界定，同時在分析比較上也比較有依據。
==國際性評估會議==
截至目前為止，世界主要語言都有問答系統發表在[[文獻|文獻]]上，甚至還有少數跨語言的案例。在過去問答系統的研究中，所有研究都是在各自的假設下進行，加上系統複雜度高，不同單位的研究成果很難拿來做客觀的評估與比較。除此之外，這類系統的評估是非常消耗人力的，事前的準備包含要產生足夠多且合適的問題題目，同時每一題可能出現的答案都必須以人工方式從比賽語料中挑選出來。以上所述對問答系統的研究發展非常不利。有鑑於此，由單一組織舉辦、多個研究單位共同參與的問答系統比賽應運而生。

英文問答系統早在1999年就開始由TREC (Text REtreival Conference)<ref>{{Cite web |url=http://trec.nist.gov/ |title=TREC (Text REtreival Conference) |accessdate=2008-08-21 |archive-date=2007-06-24 |archive-url=https://web.archive.org/web/20070624093459/http://trec.nist.gov/ |dead-url=no }}</ref>會議主辦進行這類型的比賽；日文的比賽於2003年由[[日本國立情報學研究所|日本國立情報學研究所]]NII的NTCIR會議（NTCIR Workshop）<ref>[http://research.nii.ac.jp/ntcir/NTCIR]{{dead link|date=2017年12月 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>所主辦；歐洲同樣於2003年由CLEF (Cross Language Evaluation Forum)<ref>{{Cite web |url=http://www.clef-campaign.org/ |title=CLEF |accessdate=2008-08-21 |archive-date=2007-06-23 |archive-url=https://web.archive.org/web/20070623121601/http://www.clef-campaign.org/ |dead-url=no }}</ref>會議主辦歐洲語言的比賽。根據2004年的報告<ref>{{Cite web |url=http://trec.nist.gov/pubs/trec13/papers/QA.OVERVIEW.pdf |title=Voorhees, E.M. "Overview of the TREC 2004 Question Answering Track," Text REtreival Conference (TREC), 2004. |accessdate=2008-08-21 |archive-date=2021-01-10 |archive-url=https://web.archive.org/web/20210110003237/https://trec.nist.gov/pubs/trec13/papers/QA.OVERVIEW.pdf |dead-url=no }}</ref>，目前最佳英文問答系統的水準已經可以達到70%左右的正確率。也就是說，一百個自然語言問句中，有七十題可以直接回答精準而正確的答案。此最佳英文系統由Language Computer Corporation所發展，邏輯推理能力為其致勝關鍵。在日文系統方面，正確率稍微低了些，但也有51%。日本電信電話公司（NTT）<ref>{{Cite web |url=http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings4/ntc4-pr-toc.html |title=Isozaki, H. "NTT's Question Answering System for NTCIR QAC2," NTCIR Workshop, 2004. |accessdate=2008-08-21 |archive-date=2021-01-10 |archive-url=https://web.archive.org/web/20210110003232/http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings4/ntc4-pr-toc.html |dead-url=no }}</ref>是目前成績最好的團隊。歐洲方面，QA@CLEF在規模上相當大，參與比賽的語言高達九種，加上跨語言問答的項目，比賽內容最為豐富。其中[[法文|法文]]、[[葡萄牙文|葡萄牙文]]等語言系統於2005年<ref>{{Cite web |url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.3625 |title=Vallin, A., Giampiccolo, D., Aunimo, L., Ayache, C., Osenova, P., Penas, A., Rijke, M.d., Sacaleanu, B., Santos, D., and Sutcliffe, R. "Overview of the CLEF 2005 Multilingual Question Answering Track," Cross Language Evaluation Forum (CLEF), 2005. |access-date=2008-08-21 |archive-date=2021-10-10 |archive-url=https://web.archive.org/web/20211010104816/http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.3625 |dead-url=no }}</ref>都已經可以達到六成多的正確率。相較於其他語言，中文雖然是世界上第二大語言，但中文問答系統比賽直到2005年才開始由日本NICIR會議所主辦，目前最佳的正確率為中研院的55%<ref>{{Cite web |url=http://iasl.iis.sinica.edu.tw/webpdf/paper-2007-Chinese-Chinese_and_English-Chinese_Question_Answering_with_ASQA_at_NTCIR-6_CLQA.pdf |title=Cheng-Wei Lee, Min-Yuh Day, Cheng-Lung Sung, Yi-Hsun Lee, Tian-Jian Jiang, Chia-Wei Wu, Cheng-Wei Shih, Yu-Ren Chen, Wen-Lian Hsu, "Chinese-Chinese and English-Chinese Question Answering with ASQA at NTCIR-6 CLQA", in Proceedings of NTCIR-6 Workshop Meeting, Tokyo, Japan, May 15-18, 2007, pp. 175-181.（2007）. |access-date=2008-08-21 |archive-url=https://web.archive.org/web/20130926211005/http://iasl.iis.sinica.edu.tw/webpdf/paper-2007-Chinese-Chinese_and_English-Chinese_Question_Answering_with_ASQA_at_NTCIR-6_CLQA.pdf |archive-date=2013-09-26 |dead-url=yes }}</ref>。

閱讀這些評鑑會議數據時必須注意評鑑方式間的差異。[[TREC|TREC]]會議主要的評鑑項目有「仿真陳述」、「列舉」、以及「定義」問題，各類型又有其特定的評鑑標準。而[[CLEF|CLEF]]看似與TREC的「仿真陳述」類型相同，但最近特別強調「時間限制問題」，使得問題更有挑戰性。而[[NTCIR|NTCIR]]的2005年的日文題目則全為「序列問題」。就算題目類型相同，評鑑方式仍可能不同。TREC使用三位評鑑者來評估每一結果，而CLEF依照語言的不同，使用一或兩位來評鑑每一題。2005新引進的NTCIR中文問答則使用了兩位評鑑者。評鑑標準最大的差異在於是否有考慮「文章支持度」的問題，TREC、CLEF以及NTCIR的中文問答都會考慮答案所在的文章是否「支持」該答案為真，若證據不明確，就算答案字串正確，該題仍會被視為是錯誤的。早期NTCIR日文問答則沒有考慮文章支持度的問題。根據TREC的評鑑結果，有考慮跟沒考慮文章支持度的評鑑結果差距可達十幾的百分比之多。以上說明顯示了問答系統在評鑑與解讀上到處充滿陷阱。

==線上問答系統==
*[https://web.archive.org/web/20080705080411/http://asqa.iis.sinica.edu.tw/ 智慧型中文問答系統 (中研院)]
* [http://wikiferret.com askEd! - a multilingual question answering system]{{Wayback|url=http://wikiferret.com/ |date=20201214142806 }}（[http://wikiferret.com English], [https://web.archive.org/web/20080531161024/http://wikiferret.com/edw/pc/index_j.html Japanese], [https://web.archive.org/web/20080531161017/http://wikiferret.com/edw/pc/index_cn.html Chinese],  and {{Wayback|url=http://wikiferret.com/ |date=20080531161030 }}）
*[http://www.ask.com/ Ask Jeeves]{{Wayback|url=http://www.ask.com/ |date=20110210233339 }}
*[http://www.wolframalpha.com/ Wolfram|Alpha]{{Wayback|url=http://www.wolframalpha.com/ |date=20110224050246 }}
* [https://web.archive.org/web/20080820041101/http://brainboost.com/ Automatic question answering engine]
* [http://start.csail.mit.edu/ START Web-based Question Answering system at MIT]{{Wayback|url=http://start.csail.mit.edu/ |date=20210110003305 }}
* [https://web.archive.org/web/20080215232851/http://demos.inf.ed.ac.uk:8080/qualim/ University of Edinburgh QA system - Search Wikipedia]
* [http://sourceforge.net/projects/openephyra/ OpenEphyra open source question answering system]{{Wayback|url=http://sourceforge.net/projects/openephyra/ |date=20210110003306 }}
* [http://www.answerbus.com/ AnswerBus]{{Wayback|url=http://www.answerbus.com/ |date=20210110003347 }}
* [http://experimental-quetal.dfki.de/ DFKI Experimental Open Domain Web QA system]{{dead link|date=2017年12月 |bot=InternetArchiveBot |fix-attempted=yes }}
* [https://web.archive.org/web/20081009151901/http://qa.wpcarey.asu.edu/ ASU-QA prototype Web-based QA system]
* [https://web.archive.org/web/20081120015559/http://www.ics.mq.edu.au/~pizzato/TellMe TellMe QA: A prototype QA system]
* [https://web.archive.org/web/20080821064358/http://www.laancor.com/technology/quadra/ QUADRA: Question Answering Digital Research Assistant]

==參考來源==
<div class="references-small">
<references />
</div>

{{自然语言处理}}
[[Category:人工智能|Category:人工智能]]
[[Category:计算语言学|Category:计算语言学]]
[[Category:電腦科學|Category:電腦科學]]
[[Category:資訊科學|Category:資訊科學]]