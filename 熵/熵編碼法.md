{{noteTA
|G1=IT
}}
'''熵編碼法'''是一种独立于介质的具体特征的进行[[无损数据压缩|无损数据压缩]]的方案。

一种主要类型的熵编码方式是对输入的每一个符号，创建并分配一个唯一的[[前缀码|前缀码]]，然后，通过将每个固定长度的输入符号替换成相应的可变长度前缀无关（prefix-free）输出码字替换，从而达到压缩数据的目的。每个码字的长度近似与[[概率|概率]]的负[[对数|对数]]成比例。因此，最常见的符号使用最短的码。

根据[[香农|香农]]的[[信源编码定理|信源编码定理]]，一个符号的最佳码长是 −log''<sub>b</sub>P''，其中 ''b'' 是用来输出的码的数目，''P'' 是输入符号出现的概率。

[[霍夫曼编码|霍夫曼编码]]和[[算术编码|算术编码]]是两种最常见的熵编码技术。如果预先已知数据流的近似熵特性（尤其是对于信号压缩），可以使用简单的静态码。这些静态码，包括通用密码（如Elias gamma coding或[[斐波那契编码|斐波那契编码]]）和[[哥伦布编码|哥伦布编码]]（比如[[元编码|元编码]]或[[哥伦布编码|Rice编码]]）。

一般熵編碼器与其它编码器联合使用。比如[[LHA|LHA]]首先使用LZ编码，然后将其结果进行熵編碼。[[ZIP格式|Zip]]和[[Bzip|Bzip]]的最后一级编码也是熵編碼。

==编码==
使用長度不同的比特串對字母進行編碼有一定的困難。尤其是，幾乎所有幾率的熵都是一個有理數。

===使用整数[[位元|位元]]（bit）===
[[霍夫曼编码|霍夫曼编码]]建议了一种将位元进位成整数的[[演算法|演算法]]，但这个演算法在特定情况下無法達到最佳结果。为此有人加以改进，提供最佳整数位元数。这个演算法使用[[二元樹|二元樹]]来设立一个编码。这个二叉树的终端节点代表被编码的字母，根节点代表使用的位元。

除这个对每个要编码的数据产生一个特别的表格的方法外还有使用固定的编码表的方法。比如加入要编码的数据中符号出现的機率符合一定的规则的话就可以使用特别的[[变长编码表|变长编码表]]。这样的编码表具有一定的系数来使得它适应实际的字母出现機率。

===改进===
使用整数比特的方法往往無法获得使用熵计算的比特数，因此其压缩並非一定最佳。

比如字母列由两个不同的字母组成，其中一个字母的可能性是<math>\mathrm{p}(A)=0{.}75</math>，另一个字母的可能性是<math>\mathrm{p}(B)=0{.}25</math>。以上算法的结果是每个字母应该用一个比特来代表，因此其结果的比特数与字母数相同。

但扩展取样位数可以稍微弥补该破绽：上例的<math>\mathrm{p}(AA)=0{.}5625</math>、<math>\mathrm{p}(AB)=0{.}1875</math>、<math>\mathrm{p}(BA)=0{.}1875</math>、<math>\mathrm{p}(BB)=0{.}0625</math>，以[[霍夫曼编码|霍夫曼编码]]算法得结果为：每两个字母平均用<math>(0.5625*1+0.1875*2+0.1875*3+0.0625*3)=1.6875</math>个比特，即平均每个字母用0.84375个比特来代表，向最佳熵值踏近了一步。

最佳熵編碼器应该为第一个字母使用<math>-\log_2(0{.}75)\approx 0{.}41</math>个比特，为第二个字母使用<math>-\log_2(0{.}25)=2</math>个比特，因此整个结果是每个字母平均使用<math>-0{.}75*\log_2(0{.}75)-0{.}25*\log_2(0{.}25)\approx 0.81</math>个比特。

使用[[算术编码|算术编码]]可以改善这个结果，使得原信息按照熵最佳来编码。

==模型==
要确定每个字母的比特数算法需要尽可能精确地知道每个字母的出现機率。模型的任务是提供这个数据。模型的预言越好压缩的结果就越好。此外模型必须在压缩和恢复时提出同样的数据。在历史上有许多不同的模型。

===静态模型===
静态模型在压缩前对整个文字进行分析计算每个字母的機率。这个计算结果用于整个文字上。
*优点
**编码表只需计算一次，因此编码速度高
**除在解码时所需要的機率值外结果肯定不比原文长
*缺点
**计算的機率必须附加在编码后的文字上，这使得整个结果加长
**计算的機率是整个文字的機率，因此无法对部分地区的有序数列进行优化

===动态模型===
在这个模型里機率随编码过程而不断变化。多种算法可以达到这个目的：
*前向动态：機率按照已经被编码的字母来计算，每次一个字母被编码后它的機率就增高
*反向动态：在编码前计算每个字母在剩下的还未编码的部分的機率。随着编码的进行最后越来越多的字母不再出现，它们的機率成为0，而剩下的字母的機率升高，为它们编码的比特数降低。压缩率不断增高，以至于最后一个字母只需要0比特来编码
*优点
**模型按照不同部位的特殊性优化
**在前向模型中機率数据不需要输送
*缺点
**每个字母被处理后機率要重算，因此比较慢
**计算出来的機率与实际的機率不一样，在最坏状态下压缩的数据比实际原文还要长

一般在动态模型中不使用機率，而使用每个字母出现的次数。

除上述的前向和反向模型外还有其它的动态模型计算方法。

比如在前向模型中可以不时减半出现过的字母的次数来降低一开始的字母的影响力。

对于尚未出现过的字母的处理方法也有许多不同的手段：比如假设每个字母正好出现一次，这样所有的字母均可被编码。

===模型度===
模型度说明模型顾及历史上多少个字母。比如模型度0说明模型顾及整个原文。模型度1说明模型顾及原文中的上一个字母并不断改变其機率。模型度可以无限高，但是对于大的原文来说模型度越高其需要的计算内存也越多。

== 熵作为相似性的量度 ==
除了使用熵编码作为压缩数字数据一种方法外，熵编码器也可以用来测量数据流和已经存在的类的数据之间的相似程度。这是通过对每类数据产生一个熵编码器/压缩器；通过将未压缩的数据提供给每个压缩机，据该压缩机产生的最佳压缩[[分类问题|分类]]。具有最佳压缩率的编码器可能是用与未知数据最相似的数据训练的编码器。

==外部链接==
* [https://web.archive.org/web/20160217105359/http://www.inference.phy.cam.ac.uk/mackay/itila/book.html On-line textbook: Information Theory, Inference, and Learning Algorithms]，[[David_MacKay_(scientist)|David MacKay]]著，提供一个可访问的香农理论和数据压缩的介绍，包括[[霍夫曼编码|霍夫曼编码]]和[[算术编码|算术编码]]。
* [http://iphome.hhi.de/wiegand/assets/pdfs/VBpart1.pdf Source Coding Book], by Wiegand and Schwarz

{{压缩方法}}
[[Category:信息學熵|Category:信息學熵]]
[[Category:无损压缩算法|Category:无损压缩算法]]