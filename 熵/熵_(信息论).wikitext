{{refimprove|time=2018-02-25T13:29:53+00:00}}
{{noteTA
|G1=Communication
|1=zh-hans:自信息;zh-hant:資訊本體;
|2=zh-hans:概率;zh-hant:機率;
}}
{{Expand English}}
{{otheruses|subject=[[信息论|信息论]]的熵|other=「熵」的其他意思|熵 (消歧義)}}
[[File:Entropy_flip_2_coins.jpg|thumb]]
在[[信息论|信息论]]中，'''熵'''（{{lang-en|entropy}}）是接收的每条消息中包含的信息的平均量，又被稱為'''信息熵'''、'''信源熵'''、'''平均自信息量'''。这里，“消息”代表来自分布或数据流中的事件、样本或特征。（熵最好理解为不确定性的量度而不是确定性的量度，因为越随机的信源的熵越大。）来自信源的另一个特征是样本的概率分布。这里的想法是，比较不可能发生的事情，当它发生了，会提供更多的[[信息|信息]]。由于一些其他的原因，把信息（熵）定义为概率分布的对数的相反数是有道理的。事件的概率分布和每个事件的信息量构成了一个随机变量，这个随机变量的均值（即[[数学期望|期望]]）就是这个分布产生的信息量的平均值（即熵）。熵的单位通常为比特，但也用Sh、nat、Hart计量，取决于定义用到对数的底。

采用概率分布的对数作为信息的量度的原因是其可加性。例如，投掷一次硬币提供了1 Sh的信息，而掷m次就为m位。更一般地，你需要用log<sub>2</sub>(''n'')位来表示一个可以取''n''个值的变量。

在1948年，[[克劳德·艾尔伍德·香农|克劳德·艾尔伍德·香农]]將[[熱力學|熱力學]]的熵，引入到[[信息论|信息论]]，因此它又被稱為'''香农熵'''(Shannon entropy)<ref name=shannonPaper1>{{cite journal |last=Shannon |first=Claude E. |authorlink=Claude Shannon |title=A Mathematical Theory of Communication |journal=[[Bell_System_Technical_Journal|Bell System Technical Journal]] |volume=27 |issue=3 |pages=379–423 |date=July 1948 |doi=10.1002/j.1538-7305.1948.tb01338.x|title-link=A Mathematical Theory of Communication |hdl=10338.dmlcz/101429 }} ([https://web.archive.org/web/20120615000000*/https://www.alcatel-lucent.com/bstj/vol27-1948/articles/bstj27-3-379.pdf PDF], archived from [http://www.alcatel-lucent.com/bstj/vol27-1948/articles/bstj27-3-379.pdf here] {{Wayback|url=http://www.alcatel-lucent.com/bstj/vol27-1948/articles/bstj27-3-379.pdf |date=20140620153353 }})</ref><ref name=shannonPaper2>{{cite journal |last=Shannon |first=Claude E. |authorlink=Claude Shannon |title=A Mathematical Theory of Communication |journal=[[Bell_System_Technical_Journal|Bell System Technical Journal]] |volume=27 |issue=4 |pages=623–656 |date=October 1948 |doi=10.1002/j.1538-7305.1948.tb00917.x|title-link=A Mathematical Theory of Communication |hdl=11858/00-001M-0000-002C-4317-B }} ([https://web.archive.org/web/20120615000000*/https://www.alcatel-lucent.com/bstj/vol27-1948/articles/bstj27-4-623.pdf PDF], archived from [http://www.alcatel-lucent.com/bstj/vol27-1948/articles/bstj27-4-623.pdf here] {{Wayback|url=http://www.alcatel-lucent.com/bstj/vol27-1948/articles/bstj27-4-623.pdf |date=20130510074504 }})</ref>。

== 简介 ==
熵的概念最早起源于[[物理学|物理学]]，用于度量一个热力学系统的无序程度。在[[信息论|信息论]]里面，熵是对不确定性的测量。但是在信息世界，熵越高，则能传输越多的信息，熵越低，则意味着传输的信息越少。

[[英语|英语]]文本数据流的熵比较低，因为英语很容易读懂，也就是说很容易被预测。即便我们不知道下一段英语文字是什么内容，但是我们能很容易地预测，比如，字母e总是比字母z多，或者qu字母组合的可能性总是超过q与任何其它字母的组合。如果未经压缩，一段英文文本的每个字母需要8个比特来编码，但是实际上英文文本的熵大概只有4.7比特。這是由於英文的編碼包含了各式符號，如逗號、引號等。因此英文輸入法使用了8個位元來表達一共256個字母及符號。

如果压缩是无损的，即通过解压缩可以百分之百地恢复初始的消息内容，那么压缩后的消息携带的信息和未压缩的原始消息是一样的多。而压缩后的消息可以通过较少的比特传递，因此压缩消息的每个比特能携带更多的信息，也就是说压缩信息的熵更加高。熵更高意味着比较难于预测压缩消息携带的信息，原因在于压缩消息里面没有冗余，即每个比特的消息携带了一个比特的信息。香农的[[信源编码定理|信源编码定理]]揭示了，任何无损压缩技术不可能让一比特的消息携带超过一比特的信息。消息的熵乘以消息的长度决定了消息可以携带多少信息。

香农的信源编码定理同时揭示了，任何无损压缩技术不可能缩短任何消息。根据[[鸽笼原理|鸽笼原理]]，如果有一些消息变短，则至少有一条消息变长。在实际使用中，由于我们通常只关注于压缩特定的某一类消息，所以这通常不是问题。例如英语文档和随机文字，数字照片和噪音，都是不同类型的。所以如果一个压缩算法会将某些不太可能出现的，或者非目标类型的消息变得更大，通常是无关紧要的。但是，在我们的日常使用中，如果去压缩已经压缩过的数据，仍会出现问题。例如，将一个已经是[[FLAC|FLAC]]格式的音乐文件压缩为ZIP文件很难使它占用的空间变小。

===熵的计算===
如果有一枚理想的硬币，其出现正面和反面的机会相等，则抛硬币事件的熵等于其能够达到的最大值。我们无法知道下一个硬币抛掷的结果是什么，因此每一次抛硬币都是不可预测的。因此，使用一枚正常硬币进行若干次抛掷，这个事件的熵是一[[位元|比特]]，因为结果不外乎两个——正面或者反面，可以表示为<code>0, 1</code>编码，而且两个结果彼此之间相互独立。若进行<code>n</code>次[[统计独立性|独立实验]]，则熵为<code>n</code>，因为可以用长度为<code>n</code>的[[比特流|比特流]]表示。<ref name="crypto">{{Cite book |title=''Cryptography Theory and Practice'' |trans_title=密码学理论与实践 |author1=Douglas Robert Stinson |author2=Maura Paterson |isbn= |edition=2 |section=第2.4节“熵” |pages= |language= |year=}}</ref>但是如果一枚硬币的两面完全相同，那个这个系列抛硬币事件的熵等于零，因为结果能被准确预测。现实世界里，我们收集到的数据的熵介于上面两种情况之间。

另一个稍微复杂的例子是假设一个[[随机变量|随机变量]]<code>X</code>，取三种可能值{{smallmath|f = x_1, x_2, x_3}}，概率分别为{{smallmath|f = \frac{1}{2}, \frac{1}{4}, \frac{1}{4} }}，那么编码平均比特长度是：{{smallmath|f = \frac{1}{2} \times 1 + \frac{1}{4} \times 2 + \frac{1}{4} \times 2 = \frac{3}{2} }}。其熵为3/2。

因此熵实际是对随机变量的比特量和顺次发生概率相乘再总和的[[数学期望|数学期望]]。

== 定义 ==
依据Boltzmann's H-theorem，香农把[[随机变量|随机变量]]''X''的熵值 Η（希腊字母[[Eta|Eta]]）定义如下，其值域为{''x''<sub>1</sub>, ..., ''x''<sub>''n''</sub>}：

:<math>\Eta(X) = \mathrm{E}[\mathrm{I}(X)] = \mathrm{E}[-\ln(\mathrm{P}(X))]</math>。

其中，P为''X''的[[機率質量函數|機率質量函數]]（probability mass function），E为[[期望|期望]]函數，而I(''X'')是''X''的資訊量（又稱為[[資訊本體|資訊本體]]）。I(''X'')本身是個隨機變數。

当取自有限的样本时，熵的公式可以表示為：
:<math>\Eta(X) = \sum_{i} {\mathrm{P}(x_i)\,\mathrm{I}(x_i)} = -\sum_{i} {\mathrm{P}(x_i) \log_b \mathrm{P}(x_i)},</math>

在這裏''b''是[[對數|對數]]所使用的[[底數_(對數)|底]]，通常是2,自然常數[[e_(数学常数)|e]]，或是10。當''b'' = 2，熵的單位是[[位元|bit]]；當''b'' = e，熵的單位是[[奈特_(单位)|nat]]；而當''b'' = 10,熵的單位是Hart。

''p''<sub>''i''</sub> = 0时，对於一些''i''值，对应的被加数0 log<sub>''b''</sub> 0的值将会是0，这与[[函数极限|极限]]一致。

:<math>\lim_{p\to0+}p\log p = 0</math>。
还可以定义事件 ''X'' 与 ''Y'' 分别取 ''x<sub>i</sub>'' 和 ''y<sub>j</sub>'' 时的[[条件熵|条件熵]]为

:<math>\Eta(X|Y)=-\sum_{i,j}p(x_{i},y_{j})\log\frac{p(x_{i},y_{j})}{p(y_{j})}</math>

其中''p''(''x<sub>i</sub>'', ''y<sub>j</sub>'')为 ''X'' = ''x<sub>i</sub>'' 且 ''Y'' = ''y<sub>j</sub>'' 时的概率。这个量应当理解为你知道''Y''的值前提下随机变量 ''X'' 的随机性的量。

== 範例 ==
[[File:Binary_entropy_plot.svg|thumb]][[自信息|自信息]]），以[[位元|位元]]度量，與之相對的是硬幣的公正度Pr(X=1).<br /><br />注意图的最大值取决於分布；在這裡，要傳達一個公正的拋硬幣結果至多需要1位元，但要傳達一個公正的拋[[骰子|骰子]]結果至多需要log2(6)位元。]]

如果有一个系统S内存在多个事件S = {E<sub>1</sub>,...,E<sub>n</sub>}，每个事件的機率分布P = {p<sub>1</sub>, ..., p<sub>n</sub>}，则每个事件本身的訊息（[[資訊本體|資訊本體]]）为：

:<math>I_e = -\log_2 {p_i} </math>（对数以2为底，单位是[[位元|位元]]（bit））

:<math>I_e = -\ln {p_i} </math>（对数以<math>e</math>为底，单位是[[纳特|纳特]]/nats）

如英语有26个字母，假如每个字母在文章中出现次数平均的话，每个字母的訊息量为：

:<math>I_e = -\log_2 {1\over 26} = 4.7 </math>

以日文五十音平假名作為相對範例，假設每個平假名日語文字在文章中出現的機率相等，每個平假名日語文字可攜帶的資訊量為：

:<math>I_e = -\log_2 {1\over 50} = 5.64 </math>

而汉字常用的有2500个，假如每个[[汉字|汉字]]在文章中出现次数平均的话，每个汉字的信息量为：

:<math>I_e = -\log_2 {1\over 2500} = 11.3 </math>

实际上每个字母和每个汉字在文章中出现的次数并不平均，比方说较少见字母（如z）和罕用汉字就具有相对高的信息量。但上述计算提供了以下概念：使用书写单元越多的[[文字|文字]]，每个单元所包含的訊息量越大。

熵是整个系统的平均消息量，即：

:<math>H_s = \sum_{i=1}^n p_i I_e = -\sum_{i=1}^n p_i \log_2 p_i</math>

因为和[[热力学|热力学]]中描述[[热力学熵|热力学熵]]的[[玻尔兹曼|玻尔兹曼]]公式本质相同（仅仅单位不同，一[[纳特|纳特]]的信息量即相当于[[玻尔兹曼常量|k]][[焦耳|焦耳]]每[[开尔文|开尔文]]的热力学熵），所以也称为“熵”。

如果两个系统具有同样大的消息量，如一篇用不同文字写的同一文章，由于汉字的信息量较大，中文文章应用的汉字就比英文文章使用的字母要少。所以汉字印刷的文章要比其他应用总体数量少的字母印刷的文章要短。即使一个汉字占用两个字母的空间，汉字印刷的文章也要比英文字母印刷的用纸少。

== 熵的特性 ==
可以用很少的标准来描述香农熵的特性，将在下面列出。任何满足这些假设的熵的定义均正比以下形式
:<math>-K\sum_{i=1}^np_i\log (p_i)</math>
其中''，K''是与选择的度量单位相对应的一个正比常数。

下文中，''p<sub>i</sub>'' = Pr(''X'' = ''x<sub>i</sub>'')且<math>\Eta_n(p_1,\ldots,p_n)=\Eta(X)</math>

===连续性===
该量度应[[连续函数|连续]]，概率值小幅变化只能引起熵的微小变化。

===对称性===
符号''x<sub>i</sub>''重新排序后，该量度应不变。
:<math>\Eta_n\left(p_1, p_2, \ldots \right) = \Eta_n\left(p_2, p_1, \ldots \right)</math>等。

===极值性===
当所有符号有同等機會出现的情况下，熵达到最大值（所有可能的事件同等概率时不确定性最高）。
:<math> \Eta_n(p_1,\ldots,p_n) \le \Eta_n\left(\frac{1}{n}, \ldots, \frac{1}{n}\right) = \log_b (n)</math>。

等概率事件的熵应随符号的数量增加。
:<math>\Eta_n\bigg(\underbrace{\frac{1}{n}, \ldots, \frac{1}{n}}_{n}\bigg) = \log_b(n) < \log_b (n+1) = \Eta_{n+1}\bigg(\underbrace{\frac{1}{n+1}, \ldots, \frac{1}{n+1}}_{n+1}\bigg).</math>

===可加性===
熵的量与该过程如何被划分无关。

最后给出的这个函数关系刻画了一个系统与其子系统的熵的关系。如果子系统之间的相互作用是已知的，则可以通过子系统的熵来计算一个系统的熵。

给定''n''个均匀分布元素的集合，分为''k''个箱（子系统），每个里面有  ''b''<sub>1</sub>, ..., ''b<sub>k</sub>''  个元素，合起来的熵应等于系统的熵与各个箱子的熵的和，每个箱子的权重为在该箱中的概率。

对于[[正整数|正整数]]''b<sub>i</sub>''其中''b''<sub>1</sub> + ... + ''b<sub>k</sub>'' = ''n''来说，
:<math>\Eta_n\left(\frac{1}{n}, \ldots, \frac{1}{n}\right) = \Eta_k\left(\frac{b_1}{n}, \ldots, \frac{b_k}{n}\right) + \sum_{i=1}^k \frac{b_i}{n} \, \Eta_{b_i}\left(\frac{1}{b_i}, \ldots, \frac{1}{b_i}\right)</math>。

选取''k'' = ''n''，''b''<sub>1</sub> = ... = ''b<sub>n</sub>'' = 1，这意味着确定符号的熵为零：Η<sub>1</sub>(1) = 0。这就是说可以用''n''进制熵来定义''n''个符号的信源符号集的效率。参见[[信息冗余|信息冗余]]。

==进一步性质==
香农熵满足以下性质，藉由將熵看成「在揭示随机变量''X''的值後，從中得到的信息量（或消除的不确定性量）」，可來幫助理解其中一些性質。

*增減一概率为零的事件不改变熵：
::<math>\Eta_{n+1}(p_1,\ldots,p_n,0) = \Eta_n(p_1,\ldots,p_n)</math>
*可用[[琴生不等式|琴生不等式]]证明
::<math>\Eta(X) = \operatorname{E}\left[\log_b \left( \frac{1}{p(X)}\right) \right] \leq \log_b \left( \operatorname{E}\left[ \frac{1}{p(X)} \right] \right) = \log_b(n)</math>
:具有均匀概率分布的信源符号集可以有效地达到最大熵log<sub>''b''</sub>(''n'')：所有可能的事件是等概率的时候，不确定性最大。
*计算 (''X'',''Y'')得到的熵或信息量（即同时计算''X''和''Y''）等于通过进行两个连续实验得到的信息：先计算''Y''的值，然后在你知道''Y''的值条件下得出''X''的值。写作
::<math> \Eta(X,Y)=\Eta(X|Y)+\Eta(Y)=\Eta(Y|X)+\Eta(X)</math>。
*如果''Y=f(X)''，其中''f''是确定性的，那么Η(''f''(''X'')|''X'') = 0。应用前一公式Η(''X'', ''f''(''X''))就会产生
::<math> \Eta(X)+\Eta(f(X)|X)=\Eta(f(X))+\Eta(X|f(X)),</math> 
:所以Η(''f''(''X'')) ≤ Η(''X'')，因此当后者是通过确定性函数传递时，变量的熵只能降低。
*如果''X''和''Y''是两个独立实验，那么知道''Y''的值不影响我们对''X''值的认知（因为两者独立，所以互不影响）：
::<math> \Eta(X|Y)=\Eta(X)</math>。
*两个事件同时发生的熵不大于每个事件单独发生的熵的总和，且仅当两个事件是独立的情况下相等。更具体地说，如果''X''和''Y''是同一概率空间的两个随机变量，而 ''(X,Y)''表示它们的笛卡尔积，则
::<math> \Eta(X,Y)\leq \Eta(X)+\Eta(Y)</math>。
:在前两条熵的性质基础上，很容易用数学证明这一点。

== 和热力学熵的联系 ==
物理学家和化学家对一个系统自发地从初始状态向前演进过程中，遵循热力学第二定律而发生的熵的变化更感兴趣。在传统热力学中，熵被定义为对系统的宏观测定，并没有涉及概率分布，而概率分布是信息熵的核心定义。
 
根据Jaynes（1957）的观点，热力学熵可以被视为香农信息理论的一个应用：
热力学熵被解釋成與定義系統的微態細節所需的進一步香农資訊量成正比，[[波茲曼常數|波茲曼常數]]為比例系數，其中系統與外界無交流，只靠古典熱力學的巨觀變數所描述。加熱系統會提高其热力学熵，是因為此行為增加了符合可測巨觀變數
的系統微態的數目，也使得所有系統的的完整敘述變得更長。（假想的）[[麦克斯韦妖|麦克斯韦妖]]可利用每個分子的状态信息，來降低热力学熵，但是{{le|羅夫·蘭道爾|Rolf Landauer}}（於1961年）和及其同事則證明了，让小妖精行使职责本身——即便只是了解和储存每个分子最初的香农信息——就会给系统带来热力学熵的增加，因此总的来说，系统的熵的总量没有减少。这就解决了Maxwell思想实验引发的[[悖论|悖论]]。[[蘭道爾原理|蘭道爾原理]]也為現代計算機處理大量資訊時所產生的熱量給出了下限，雖然現在計算機的廢熱遠遠比這個限制高。

== 逸闻 ==
[[贝尔实验室|贝尔实验室]]曾流传一则可信度不高的传闻：[[冯诺依曼|冯诺依曼]]建议香农为这个概念取名为“熵”，理由是这个热力学名词别人不懂，容易被唬住。<ref>{{cite book |title=''The Information: A History, a Theory, a Flood'' |trans_title=[[信息简史|信息简史]] |author=[[詹姆斯·格雷克|詹姆斯·格雷克]] |others=高博 (翻译), 楼伟珊 (审校), 高学栋 (审校), 李松峰 (审校) |publisher=[[人民邮电出版社|人民邮电出版社]] |ISBN=978-7-115-33180-9 |edition=1 |section=第9章“熵及其妖” |page=265 |language=zh-cn |year=2013 |quote="根据在贝尔实验室里流传的一个说法，是约翰·冯·诺依曼建议香农使用这个词，因为没有人懂这个词的意思，所以他与人争论时可以无往而不利。这件事虽然子虚乌有，但听起来似乎有点道理。"}}</ref>

== 参见 ==
* [[熵_(生态学)|熵 (生态学)]]
* [[熵_(熱力學)|熵 (熱力學)]]
* [[熵编码|熵编码]]
* [[麦克斯韦妖|麦克斯韦妖]]

== 参考 ==
{{reflist|2}}

==外部链接==
* {{springer|title=Entropy|id=p/e035740}}

{{Authority control}}

{{DEFAULTSORT:entropy}}
[[Category:信息论|Category:信息论]]
[[Category:信息學熵|*]]