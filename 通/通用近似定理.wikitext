在[[人工神经网络|人工神经网络]]的[[数学|数学]]理论中， '''通用近似定理'''（或稱'''萬能近似定理'''）指出人工神經網路近似任意函數的能力<ref>{{Cite book|title=Neural Networks and Deep Learning|last=Nielsen|first=Michael|publisher=Determination Press|year=2015|isbn=|location=|pages=|url=http://neuralnetworksanddeeplearning.com/|accessdate=2020-08-27|language=en|chapter=4|chapterurl=http://neuralnetworksanddeeplearning.com/chap4.html|archive-date=2017-07-29|archive-url=https://web.archive.org/web/20170729043304/http://neuralnetworksanddeeplearning.com/|dead-url=no}}</ref>。 通常此定理所指的神經網路爲[[前饋神經網路|前饋神經網路]]，並且被近似的目標函數通常爲輸入輸出都在[[歐幾里得空間|歐幾里得空間]]的連續函數。但亦有研究將此定理擴展至其他類型的神經網路，如[[卷積神經網路|卷積神經網路]]<ref>Zhou, Ding-Xuan (2020) Universality of deep convolutional neural networks; Applied and computational harmonic analysis 48.2 (2020): 787-794.</ref><ref>A. Heinecke, J. Ho and W. Hwang (2020); Refinement and Universal Approximation via Sparsely Connected ReLU Convolution Nets; IEEE Signal Processing Letters, vol. 27, pp. 1175-1179.</ref>、[[径向基函数网络|放射狀基底函數網路]]<ref>Park, Jooyoung, and Irwin W. Sandberg (1991); Universal approximation using radial-basis-function networks; Neural computation 3.2, 246-257.</ref>、或其他特殊神經網路<ref>Yarotsky, Dmitry (2018); Universal approximations of invariant maps by neural networks.</ref>。 

此定理意味着神經網路可以用來近似任意的復雜函數，並且可以達到任意近似精準度。但它並沒有說明要如何選擇神經網絡參數（權重、神經元數量、神經層層數等等）來達到想近似的目標函數。 

== 历史 ==
===1900年代===
{{main|希爾伯特第十三問題}}

===1950年代至60年代===

Kolmogorov與學生Arnold 在1950年代及60年代期間，證明多元函數可分解為以下形式（e.g. [[Kolmogorov–Arnold_表示定理|Kolmogorov–Arnold 表示定理]]）:

:<math> f(\mathbf x) = f(x_1,\ldots ,x_n) = \sum_{q=0}^{2n} \Phi_{q}\left(\sum_{p=1}^{n} \phi_{q,p}(x_{p})\right) </math>.

===1980年代後===
[[乔治·西本科|乔治·西本科]]于1989年证明了單一隱藏層、任意宽度、並使用[[S型函數|S型函數]]作爲[[激勵函數|激勵函數]]的[[前饋神經網路|前饋神經網路]]的通用近似定理<ref name="cyb">Cybenko, G. (1989) [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.441.7873&rep=rep1&type=pdf "Approximations by superpositions of sigmoidal functions"] {{Wayback|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.441.7873&rep=rep1&type=pdf |date=20210227132418 }}, ''[[控制，信号和系统数学|Mathematics of Control, Signals, and Systems]]'', 2(4), 303–314. {{Doi|10.1007/BF02551274}}</ref>。科特·霍尼克（{{Lang-en|Kurt Hornik}}）在1991年证明 ，激勵函數的選擇不是關鍵，前饋神經網路的多層神經層及多神經元架構才是使神经网络有成为通用逼近器的關鍵<ref name="horn">Kurt Hornik (1991) "", ''Neural Networks'', 4(2), 251–257. {{Doi|10.1016/0893-6080(91)90009-T}}</ref>。

== 參見 ==
* {{le|Kolmogorov–Arnold表示定理|Kolmogorov–Arnold representation theorem}} 
* {{le|代表定理|Representer theorem}} 
* {{en-link|没有免费的午餐定理|No free lunch in search and optimization}}
* [[魏尔施特拉斯逼近定理|Stone–Weierstrass定理]] 
* [[傅里叶级数|傅里叶级数]]
* [[希爾伯特第十三問題|希爾伯特第十三問題]]

== 参考文献 ==
{{Reflist}}
[[Category:网络|Category:网络]]
[[Category:網路結構|Category:網路結構]]
[[Category:人工神经网络|Category:人工神经网络]]