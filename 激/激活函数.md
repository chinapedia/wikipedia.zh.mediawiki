{{NoteTA
|G1 = IT
|1 = zh-cn:数字; zh-tw:數位; zh-hk:數碼;
|2 = zh-cn:激活函数; zh-tw:激勵函數; zh-hk:激活函数;
}}
{{Expand language|time=2017-02-04T07:06:30+00:00}}
{{translating|time=2018-06-14T07:06:30+00:00}}
在[[人工神经网络|计算网络]]中， 一个节点的'''激活函数'''定义了该节点在给定的输入或输入的集合下的输出。标准的[[集成电路|计算机芯片电路]]可以看作是根据输入得到'''开'''（1）或'''关'''（0）输出的[[数字电路|數位電路]]激活函数。这与神经网络中的[[感知器|线性感知机]]的行为类似。然而，只有[[非線性系統|非線性]]激活函数才允許這種網絡僅使用少量節點來計算非[[平凡_(數學)|平凡]]問題。 在[[人工神经网络|人工神經網絡]]中，這個功能也被稱為[[传递函数|傳遞函數]]。

== 函數 ==
下表列出了幾個激活函数，它們的輸入為一個[[变量|變量]]。
{| class="wikitable sortable"
|-
! 名稱
![[函数图形|函數圖形]]
![[方程|方程式]]
![[导数|導數]]
![[區間|區間]]
! [[Smoothness#Order_of_continuity|Order of continuity]]
![[单调函数|單調]]
! Derivative Monotonic
! Approximates identity near the origin
|-
|[[恆等函數|恆等函數]]
| [[File:Activation_identity.svg|File:Activation identity.svg]]
| <math>f(x)=x</math>
| <math>f'(x)=1</math>
| <math>(-\infty,\infty)</math>
| <math>C^\infty</math>
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
|[[单位阶跃函数|單位階躍函數]]
| [[File:Activation_binary_step.svg|File:Activation binary step.svg]]
| <math>f(x) = \begin{cases}
	0 & \text{for } x < 0\\
	1 & \text{for } x \ge 0\end{cases}</math>
| <math>f'(x) = \begin{cases}
	0 & \text{for } x \ne 0\\
	? & \text{for } x = 0\end{cases}</math>
| <math>\{0,1\}</math>
| <math>C^{-1}</math>
| {{Yes}}
| {{No}}
| {{No}}
|-
|[[邏輯函數|邏輯函數]] (也被稱為[[S函数|S函數]])
| [[File:Activation_logistic.svg|File:Activation logistic.svg]]
| <math>f(x)=\sigma(x)=\frac{1}{1+e^{-x}}</math>{{ref|logistic1}}
| <math>f'(x)=f(x)(1-f(x))</math>
| <math>(0,1)</math>
| <math>C^\infty</math>
| {{Yes}}
| {{No}}
| {{No}}
|-
|[[双曲正切函数|雙曲正切函數]]
| [[File:Activation_tanh.svg|File:Activation tanh.svg]]
| <math>f(x)=\tanh(x)=\frac{(e^{x} - e^{-x})}{(e^{x} + e^{-x})}</math>
| <math>f'(x)=1-f(x)^2</math>
| <math>(-1,1)</math>
| <math>C^\infty</math>
| {{Yes}}
| {{No}}
| {{Yes}}
|-
|[[反正切|反正切函數]]
| [[File:Activation_arctan.svg|File:Activation arctan.svg]]
| <math>f(x)=\tan^{-1}(x)</math>
| <math>f'(x)=\frac{1}{x^2+1}</math>
| <math>\left(-\frac{\pi}{2},\frac{\pi}{2}\right)</math>
| <math>C^\infty</math>
| {{Yes}}
| {{No}}
| {{Yes}}
|-
| Softsign函數<ref>{{Cite web|url=http://www.iro.umontreal.ca/~lisa/publications2/index.php/attachments/single/205|title=Quadratic polynomials learn better image features". Technical Report 1337|last=Bergstra|first=James|last2=Desjardins|first2=Guillaume|date=2009|website=Département d’Informatique et de Recherche Opérationnelle, Université de Montréal|archive-url=|archive-date=|dead-url=|access-date=|last3=Lamblin|first3=Pascal|last4=Bengio|first4=Yoshua}}</ref><ref>{{Citation|contribution-url=http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf|contribution=Understanding the difficulty of training deep feedforward neural networks|last=Glorot|first=Xavier|last2=Bengio|first2=Yoshua|date=2010|title= International Conference on Artificial Intelligence and Statistics (AISTATS’10) |publisher=Society for Artificial Intelligence and Statistics|archive-url=|archive-date=|dead-url=|access-date=}}</ref>
| [[File:Activation_softsign.png|File:Activation softsign.png]]
| <math>f(x)=\frac{x}{1+|x|}</math>
| <math>f'(x)=\frac{1}{(1+|x|)^2}</math>
| <math>(-1,1)</math>
| <math>C^1</math>
| {{Yes}}
| {{No}}
| {{Yes}}
|-
| 反平方根函數(ISRU)<ref name="isrlu">{{Cite arxiv|last=Carlile|first=Brad|last2=Delamarter|first2=Guy|last3=Kinney|first3=Paul|last4=Marti|first4=Akiko|last5=Whitney|first5=Brian|date=2017-11-09|title=Improving Deep Learning by Inverse Square Root Linear Units (ISRLUs)|eprint=1710.09967 |class=cs.LG}}</ref>
| [[File:Activation_ISRU.svg|File:Activation_ISRU.svg]]
| <math>f(x) = \frac{x}{\sqrt{1 + \alpha x^2}} </math>
| <math>f'(x) = \left(\frac{1}{\sqrt{1 + \alpha x^2}}\right)^3 </math>
| <math>\left(-\frac{1}{\sqrt{\alpha}},\frac{1}{\sqrt{\alpha}}\right)</math>
| <math>C^\infty</math>
| {{Yes}}
| {{No}}
| {{Yes}}
|-
|[[线性整流函数|線性整流函數]](ReLU)
| [[File:Activation_rectified_linear.svg|File:Activation rectified linear.svg]]
| <math>f(x) = \begin{cases}
	0 & \text{for } x < 0\\
	x & \text{for } x \ge 0\end{cases}</math>
| <math>f'(x) = \begin{cases}
	0 & \text{for } x < 0\\
	1 & \text{for } x \ge 0\end{cases}</math>
| <math>[0,\infty)</math>
| <math>C^0</math>
| {{Yes}}
| {{Yes}}
| {{No}}
|-
|[[线性整流函数#带泄露线性整流|帶泄露線性整流函數]](Leaky ReLU)
| [[File:Activation_prelu.svg|File:Activation prelu.svg]]
| <math>f(x) = \begin{cases}
	0.01x & \text{for } x < 0\\
	x     & \text{for } x \ge 0\end{cases}</math>
| <math>f'(x) = \begin{cases}
	0.01 & \text{for } x < 0\\
	1    & \text{for } x \ge 0\end{cases}</math>
| <math>(-\infty,\infty)</math>
| <math>C^0</math>
| {{Yes}}
| {{Yes}}
| {{No}}
|-
| 參數化線性整流函數(PReLU)<ref>{{Cite arxiv|last=He|first=Kaiming|last2=Zhang|first2=Xiangyu|last3=Ren|first3=Shaoqing|last4=Sun|first4=Jian|date=2015-02-06|title=Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification|eprint=1502.01852 |class=cs.CV}}</ref>
| [[File:Activation_prelu.svg|File:Activation prelu.svg]]
| <math>f(\alpha,x) = \begin{cases}
	\alpha x & \text{for } x < 0\\
	x & \text{for } x \ge 0\end{cases}</math>
| <math>f'(\alpha,x) = \begin{cases}
	\alpha & \text{for } x < 0\\
	1 & \text{for } x \ge 0\end{cases}</math>
| <math>(-\infty,\infty)</math>
| <math>C^0</math>
| {{Depends|Yes [[If_and_only_if|iff]] <math>\alpha \geq 0</math>}}
| {{Yes}}
| {{Depends|Yes [[If_and_only_if|iff]] <math>\alpha = 1</math>}}
|-
|[[线性整流函数#带泄露随机线性整流|帶泄露隨機線性整流函數]](RReLU)<ref>{{Cite arxiv|last=Xu|first=Bing|last2=Wang|first2=Naiyan|last3=Chen|first3=Tianqi|last4=Li|first4=Mu|date=2015-05-04|title=Empirical Evaluation of Rectified Activations in Convolutional Network|eprint=1505.00853 |class=cs.LG}}</ref>
| [[File:Activation_prelu.svg|File:Activation prelu.svg]]
| <math>f(\alpha, x) = \begin{cases}
	\alpha x & \text{for } x < 0\\
	x & \text{for } x \ge 0\end{cases}</math>{{ref|alpha_random}}
| <math>f'(\alpha, x) = \begin{cases}
	\alpha & \text{for } x < 0\\
	1 & \text{for } x \ge 0\end{cases}</math>
| <math>(-\infty,\infty)</math>
| <math>C^0</math>
| {{Yes}}
| {{Yes}}
| {{No}}
|-
| 指數線性函數(ELU)<ref>{{Cite arxiv|last=Clevert|first=Djork-Arné|last2=Unterthiner|first2=Thomas|last3=Hochreiter|first3=Sepp|date=2015-11-23|title=Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)|eprint=1511.07289 |class=cs.LG}}</ref>
| [[File:Activation_elu.svg|File:Activation elu.svg]]
| <math>f(\alpha,x) = \begin{cases}
	\alpha(e^x - 1) & \text{for } x < 0\\
	x & \text{for } x \ge 0\end{cases}</math>
| <math>f'(\alpha,x) = \begin{cases}
	f(\alpha,x) + \alpha & \text{for } x < 0\\
	1 & \text{for } x \ge 0\end{cases}</math>
| <math>(-\alpha,\infty)</math>
| <math>\begin{cases}
C_ 1
& \text{when }
\alpha=1
\\
C_0 & \text{otherwise }
\end{cases}
</math>
| {{Depends|Yes [[If_and_only_if|iff]] <math>\alpha \geq 0</math>}}
| {{Depends|Yes [[If_and_only_if|iff]] <math>0 \leq \alpha \leq 1</math>}}
| {{Depends|Yes [[If_and_only_if|iff]] <math>\alpha = 1</math>}}
|-
| 擴展指數線性函數(SELU)<ref>{{Cite arxiv|last=Klambauer|first=Günter|last2=Unterthiner|first2=Thomas|last3=Mayr|first3=Andreas|last4=Hochreiter|first4=Sepp|date=2017-06-08|title=Self-Normalizing Neural Networks|eprint=1706.02515 |class=cs.LG}}</ref>
|  <!-- [[File:Activation_selu.png|File:Activation selu.png]] -->
| <math>f(\alpha,x) = \lambda \begin{cases}
	\alpha(e^x - 1) & \text{for } x < 0\\
	x & \text{for } x \ge 0\end{cases}</math>
with <math>\lambda = 1.0507</math> and <math>\alpha = 1.67326</math>
| <math>f'(\alpha,x) = \lambda \begin{cases}
	\alpha(e^x) & \text{for } x < 0\\
	1 & \text{for } x \ge 0\end{cases}</math>
| <math>(-\lambda\alpha,\infty)</math>
| <math>C^0</math>
| {{Yes}}
| {{No}}
| {{No}}
|-
| S 型線性整流激活函數(SReLU)<ref>{{Cite arxiv|last=Jin|first=Xiaojie|last2=Xu|first2=Chunyan|last3=Feng|first3=Jiashi|last4=Wei|first4=Yunchao|last5=Xiong|first5=Junjun|last6=Yan|first6=Shuicheng|date=2015-12-22|title=Deep Learning with S-shaped Rectified Linear Activation Units|eprint=1512.07030|class=cs.CV}}</ref>
|
| <math>f_{t_l,a_l,t_r,a_r}(x) = \begin{cases}
	t_l + a_l (x - t_l) & \text{for } x \le t_l\\
	x & \text{for } t_l < x < t_r\\
	t_r + a_r (x - t_r) & \text{for } x \ge t_r\end{cases}</math><br><math>t_l, a_l, t_r, a_r</math> are parameters.
| <math>f'_{t_l,a_l,t_r,a_r}(x) = \begin{cases}
	a_l & \text{for } x \le t_l\\
	1   & \text{for } t_l < x < t_r\\
	a_r & \text{for } x \ge t_r\end{cases}</math>
| <math>(-\infty,\infty)</math>
| <math>C^0</math>
| {{No}}
| {{No}}
| {{No}}
|-
| 反平方根線性函數(ISRLU)<ref name="isrlu" />
| [[File:Activation_ISRLU.svg|File:Activation_ISRLU.svg]]
| <math>f(x) = \begin{cases}
	\frac{x}{\sqrt{1 + \alpha x^2}} & \text{for } x < 0\\
	x & \text{for } x \ge 0\end{cases}</math>
| <math>f'(x) = \begin{cases}
	\left(\frac{1}{\sqrt{1 + \alpha x^2}}\right)^3 & \text{for } x < 0\\
	1 & \text{for } x \ge 0\end{cases}</math>
| <math>\left(-\frac{1}{\sqrt{\alpha}},\infty\right)</math>
| <math>C^2</math>
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
| 自適應分段線性函數(APL)<ref>{{cite arxiv|author1=Forest Agostinelli|author2=Matthew Hoffman|author3=Peter Sadowski|author4=Pierre Baldi|title=Learning Activation Functions to Improve Deep Neural Networks|eprint=1412.6830|date=21 Dec 2014|class=cs.NE}}</ref>
| 
| <math>f(x) = \max(0,x) + \sum_{s=1}^{S}a_i^s \max(0, -x + b_i^s)</math>
| <math>f'(x) = H(x) - \sum_{s=1}^{S}a_i^s H(-x + b_i^s)</math>{{ref|heaviside}}
| <math>(-\infty,\infty)</math>
| <math>C^0</math>
| {{No}}
| {{No}}
| {{No}}
|-
| SoftPlus函數<ref>{{Cite web|url=http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf|title=Deep sparse rectifier neural networks|last=Glorot|first=Xavier|last2=Bordes|first2=Antoine|date=2011|work=International Conference on Artificial Intelligence and Statistics|archive-url=|archive-date=|dead-url=|access-date=|last3=Bengio|first3=Yoshua}}</ref>
| [[File:Activation_softplus.svg|File:Activation softplus.svg]]
| <math>f(x)=\ln(1+e^x)</math>
| <math>f'(x)=\frac{1}{1+e^{-x}}</math>
| <math>(0,\infty)</math>
| <math>C^\infty</math>
| {{Yes}}
| {{Yes}}
| {{No}}
|-
| 彎曲恆等函數
| [[File:Activation_bent_identity.svg|File:Activation bent identity.svg]]
| <math>f(x)=\frac{\sqrt{x^2 + 1} - 1}{2} + x</math>
| <math>f'(x)=\frac{x}{2\sqrt{x^2 + 1}} + 1</math>
| <math>(-\infty,\infty)</math>
| <math>C^\infty</math>
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
| Sigmoid-weighted linear unit (SiLU)<ref>[https://arxiv.org/abs/1702.03118 Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning]</ref> (也被稱為Swish<ref>[https://arxiv.org/abs/1710.05941 Searching for Activation Functions]</ref>)
| 
| <math>f(x)=x\cdot\sigma(x)</math>{{ref|logistic2}}
| <math>f'(x)=f(x)+\sigma(x)(1-f(x))</math>{{ref|logistic3}}
| <math>[\approx-0.28,\infty)</math>
| <math>C^\infty</math>
| {{No}}
| {{No}}
| {{No}}
|-
| SoftExponential函數<ref>{{cite journal|last=Godfrey|first=Luke B.|last2=Gashler|first2=Michael S.|date=2016-02-03|title=A continuum among logarithmic, linear, and exponential functions, and its potential to improve generalization in neural networks|arxiv=1602.01321 |volume=1602|pages=481–486|via=|work=7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management: KDIR|bibcode=2016arXiv160201321G}}</ref>
| [[File:Activation_soft_exponential.svg|File:Activation soft exponential.svg]]
| <math>f(\alpha,x) = \begin{cases}
	-\frac{\ln(1-\alpha (x + \alpha))}{\alpha} & \text{for } \alpha < 0\\
	x & \text{for } \alpha = 0\\
        \frac{e^{\alpha x} - 1}{\alpha} + \alpha & \text{for } \alpha > 0\end{cases}</math>
| <math>f'(\alpha,x) = \begin{cases}
	\frac{1}{1-\alpha (\alpha + x)} & \text{for } \alpha < 0\\
	e^{\alpha x} & \text{for } \alpha \ge 0\end{cases}</math>
| <math>(-\infty,\infty)</math>
| <math>C^\infty</math>
| {{Yes}}
| {{Yes}}
| {{Depends|Yes [[If_and_only_if|iff]] <math>\alpha = 0</math>}}
|-
|[[正弦函数|正弦函數]]
| [[File:Activation_sinusoid.svg|File:Activation sinusoid.svg]]
| <math>f(x)=\sin(x)</math>
| <math>f'(x)=\cos(x)</math>
| <math>[-1,1]</math>
| <math>C^\infty</math>
| {{No}}
| {{No}}
| {{Yes}}
|-
|[[Sinc函数|Sinc函數]]
| [[File:Activation_sinc.svg|File:Activation sinc.svg]]
| <math>f(x)=\begin{cases}
        1 & \text{for } x = 0\\
	\frac{\sin(x)}{x} & \text{for } x \ne 0\end{cases}</math>
| <math>f'(x)=\begin{cases}
	0 & \text{for } x = 0\\
	\frac{\cos(x)}{x} - \frac{\sin(x)}{x^2} & \text{for } x \ne 0\end{cases}</math>
| <math>[\approx-.217234,1]</math>
| <math>C^\infty</math>
| {{No}}
| {{No}}
| {{No}}
|-
|[[高斯函数|高斯函數]]
| [[File:Activation_gaussian.svg|File:Activation gaussian.svg]]
| <math>f(x)=e^{-x^2}</math>
| <math>f'(x)=-2xe^{-x^2}</math>
| <math>(0,1]</math>
| <math>C^\infty</math>
| {{No}}
| {{No}}
| {{No}}
|}
:{{note|heaviside}}此處{{mvar|H}}是[[单位阶跃函数|單位階躍函數]]。
:{{note|alpha_random}}{{mvar|α}}是在訓練時間從[[連續型均勻分布|均勻分佈]]中抽取的隨機變量，並且在測試時間固定為分佈的[[期望值|期望值]]。
:{{note|logistic1}}{{note|logistic2}}{{note|logistic3}}此處<math>\sigma</math>是[[邏輯函數|邏輯函數]]。

下表列出了幾個激活函数，它們的輸入為多個[[变量|變量]]。

{| class="wikitable sortable"
|-
! [[名稱|名稱]]
! [[方程式|方程式]]
! [[導數|導數]]
! [[區間|區間]]
! [[Smoothness#Order_of_continuity|Order of continuity]]
|-
|[[Softmax函数|Softmax函數]]
| <math>f_i(\vec{x}) = \frac{e^{x_i}}{\sum_{j=1}^J e^{x_j}}</math>    for {{mvar|i}} = 1, …, {{mvar|J}}
| <math>\frac{\partial f_i(\vec{x})}{\partial x_j} = f_i(\vec{x})(\delta_{ij} - f_j(\vec{x}))</math>{{ref|kronecker_delta}}
| <math>(0,1)</math>
| <math>C^\infty</math>
|-
| Maxout函數<ref>{{Cite journal|last=Goodfellow|first=Ian J.|last2=Warde-Farley|first2=David|last3=Mirza|first3=Mehdi|last4=Courville|first4=Aaron|last5=Bengio|first5=Yoshua|date=2013-02-18|title=Maxout Networks|journal=JMLR WCP |volume=28|issue=3|pages=1319–1327|arxiv=1302.4389 |bibcode=2013arXiv1302.4389G}}</ref>
| <math>f(\vec{x}) = \max_i x_i</math>
| <math>\frac{\partial f}{\partial x_j} = \begin{cases}
        1 & \text{for } j = \underset{i}{\operatorname{argmax}} \,  x_i\\
	0 & \text{for } j \ne\underset{i}{\operatorname{argmax}} \,   x_i\end{cases}</math>
| <math>(-\infty,\infty)</math>
| <math>C^0</math>
|}
{{note|}} 此處{{mvar|δ}}是[[克罗内克δ函数|克羅內克δ函數]]。

== 參見 ==
*[[邏輯函數|邏輯函數]]
*[[线性整流函数|線性整流函數]]
*[[Softmax函数|Softmax函數]]
*[[人工神经网络|人工神經網路]]
*[[深度学习|深度學習]]

==參考資料==

[[Category:人工神经网络|Category:人工神经网络]]

<references />