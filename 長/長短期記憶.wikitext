{{NoteTA
|G1 = Computer
|1 = zh-cn:变量; zh-tw:變數; zh-hk:變數;
|2 = zh-cn:网络; zh-tw:網路;
|3 = zh-cn:识别; zh-tw:辨識;
}}

{{机器学习导航栏}}
{{ request translation }}
'''长短期记忆'''（{{lang-en|Long Short-Term Memory}}，{{lang|en|LSTM}}）是一种[[循环神经网络|时间循环神经网络]]（RNN）<ref>S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–1780, 1997.</ref>，论文首次发表于1997年。由于独特的设计结构，LSTM适合于处理和预测[[时间序列|时间序列]]中间隔和延迟非常长的重要事件。

LSTM的表现通常比[[循环神经网络|时间循环神经网络]]及[[隐马尔科夫模型|隐马尔科夫模型]]（HMM）更好，比如用在不分段连续[[手写识别|手写识别]]上<ref>A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, J. Schmidhuber. A Novel Connectionist System for Improved Unconstrained Handwriting Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, 2009.</ref>。2009年，用LSTM构建的人工神经网络模型赢得过[[ICDAR|ICDAR]]手写识别比赛冠军。LSTM还普遍用于自主[[语音识别|语音识别]]，2013年運用[[TIMIT|TIMIT]]自然演講資料庫達成17.7%錯誤率的紀錄。作为[[非线性|非线性]]模型，LSTM可作为复杂的非线性单元用于构造更大型[[深度学习|深度神经网络]]。

== 历史 ==
1997年，[[Sepp_Hochreiter|Sepp Hochreiter]]和[[Jürgen_Schmidhuber|Jürgen Schmidhuber]]提出LSTM。版本包含了cells, input以及output gates。

2014年，Kyunghyun Cho et al.发明了{{Internal link helper/en|Gated recurrent unit|Gated recurrent unit}}（GRU）。<ref>{{Cite journal|title=Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation|url=http://arxiv.org/abs/1406.1078|last=Cho|first=Kyunghyun|last2=van Merrienboer|first2=Bart|date=2014-09-02|journal=arXiv:1406.1078 [cs, stat]|last3=Gulcehre|first3=Caglar|last4=Bahdanau|first4=Dzmitry|last5=Bougares|first5=Fethi|last6=Schwenk|first6=Holger|last7=Bengio|first7=Yoshua|access-date=2020-02-11|archive-date=2022-02-08|archive-url=https://web.archive.org/web/20220208192330/https://arxiv.org/abs/1406.1078|dead-url=no}}</ref>

2016年，谷歌用LSTM进行[[谷歌翻译|谷歌翻译]]。<ref>{{Cite journal|title=Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation|url=http://arxiv.org/abs/1609.08144|last=Wu|first=Yonghui|last2=Schuster|first2=Mike|date=2016-10-08|journal=arXiv:1609.08144 [cs]|last3=Chen|first3=Zhifeng|last4=Le|first4=Quoc V.|last5=Norouzi|first5=Mohammad|last6=Macherey|first6=Wolfgang|last7=Krikun|first7=Maxim|last8=Cao|first8=Yuan|last9=Gao|first9=Qin|access-date=2020-02-11|archive-date=2021-01-14|archive-url=https://web.archive.org/web/20210114150631/https://arxiv.org/abs/1609.08144|dead-url=no}}</ref> [[苹果公司|苹果公司]]、[[微软|微软]]和[[亞馬遜公司|亞馬遜公司]]也用LSTM生产产品，例如：[[iPhone|iPhone]]<ref>{{Cite web|title=Apple’s Machines Can Learn Too|url=https://www.theinformation.com/articles/apples-machines-can-learn-too|accessdate=2020-02-11|last=Amir@theinformation.cOm|work=The Information|archive-date=2021-01-15|archive-url=https://web.archive.org/web/20210115175905/https://www.theinformation.com/articles/apples-machines-can-learn-too|dead-url=no}}</ref>、[[Amazon_Alexa|Amazon Alexa]]<ref>{{Cite web|title=Bringing the Magic of Amazon AI and Alexa to Apps on AWS. - All Things Distributed|url=https://www.allthingsdistributed.com/2016/11/amazon-ai-and-alexa-for-all-aws-apps.html|accessdate=2020-02-11|work=www.allthingsdistributed.com|archive-date=2019-04-01|archive-url=https://web.archive.org/web/20190401134207/https://www.allthingsdistributed.com/2016/11/amazon-ai-and-alexa-for-all-aws-apps.html|dead-url=no}}</ref>、等。中国公司也正在用LSTM。

==结构==
LSTM是一種含有LSTM區塊（blocks）或其他的一種類神經網路，文獻或其他資料中LSTM區塊可能被描述成智慧型網路單元，因為它可以記憶不定時間長度的數值，區塊中有一個gate能夠決定input是否重要到能被記住及能不能被輸出output。

右圖底下是四個S函數單元，最左邊函數依情況可能成為區塊的input，右邊三個會經過gate決定input是否能傳入區塊，左邊第二個為input gate，如果這裡產出近似於零，將把這裡的值擋住，不會進到下一層。左邊第三個是forget gate，當這產生值近似於零，將把區塊裡記住的值忘掉。第四個也就是最右邊的input為output gate，他可以決定在區塊記憶中的input是否能輸出 。

LSTM有很多个版本，其中一个重要的版本是GRU（Gated Recurrent Unit）<ref>[http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:bahdanau-iclr2015.pdf Neural Machine Translation by Jointly Learning to Align and Translate] {{Wayback|url=http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:bahdanau-iclr2015.pdf |date=20160310113559 }}，Cho et al. 2014年。</ref>，根据谷歌的测试表明，LSTM中最重要的是Forget gate，其次是Input gate，最次是Output gate<ref>[http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf 递归神经网络结构经验之谈] {{Wayback|url=http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf |date=20170329161943 }}，2015年。</ref>。

== 方程 ==
<math>
\begin{align}
f_t &= \sigma_g(W_{f} x_t + U_{f} h_{t-1} + b_f) \\
i_t &= \sigma_g(W_{i} x_t + U_{i} h_{t-1} + b_i) \\
o_t &= \sigma_g(W_{o} x_t + U_{o} h_{t-1} + b_o) \\
c_t &= f_t \circ c_{t-1} + i_t \circ \sigma_c(W_{c} x_t + U_{c} h_{t-1} + b_c) \\
h_t &= o_t \circ \sigma_h(c_t)
\end{align}
</math>
[[File:Long_Short_Term_Memory.png|缩略图]]]]

=== 变量 ===
*<math>x_t \in \mathbb{R}^{d}</math>: LSTM的input（輸入）
*<math>f_t \in \mathbb{R}^{h}</math>: forget gate（遺忘閥）
*<math>i_t \in \mathbb{R}^{h}</math>: input gate（輸入閥）
*<math>o_t \in \mathbb{R}^{h}</math>: output gate（輸出閥）
*<math>h_t \in \mathbb{R}^{h}</math>: hidden state（隱藏狀態）
*<math>c_t \in \mathbb{R}^{h}</math>: cell state（單元狀態）
*<math>W \in \mathbb{R}^{h \times d}</math>、<math>U \in \mathbb{R}^{h \times h} </math>、<math>b \in \mathbb{R}^{h}</math>: 訓練中的矩阵，网络学习计算元值

<br />

==训练方法==
為了最小化訓練誤差，[[梯度下降法|梯度下降法]]（Gradient descent）如：{{tsl|en|Backpropagation through time|應用時序性倒傳遞演算法}}，可用來依據錯誤修改每次的權重。梯度下降法在循環神經網路（RNN）中主要的問題初次在1991年發現，就是誤差梯度隨著事件間的時間長度成指數般的消失。當設置了LSTM 區塊時，誤差也隨著倒回計算，從output影響回input階段的每一個gate，直到這個數值被過濾掉。因此正常的倒循環類神經是一個有效訓練LSTM區塊記住長時間數值的方法。

{{Internal link helper/en|Backpropagation through time|Backpropagation through time}}、BPTT <ref>{{Cite web|title=Problem Set 8|url=https://cos485.github.io/2019/04/24/pset8.html|accessdate=2020-02-11|date=2019-04-24|work=COS 485 Neural Networks: Theory and Applications|language=en}}</ref><ref>{{Cite web|title=Recurrent Neural Networks|url=https://nlp.cs.princeton.edu/cos484/lectures/lec9.pdf|accessdate=|author=Danqi Chen|date=|format=|publisher=|language=|archive-date=2020-09-02|archive-url=https://web.archive.org/web/20200902225927/https://nlp.cs.princeton.edu/cos484/lectures/lec9.pdf|dead-url=no}}</ref>[[Image:Lstm_block.svg|thumb]]
==应用==

* [[机器|机器]]控制<ref>{{Cite book|chapter=2006 IEEE/RSJ International Conference on Intelligent Robots and Systems : Beijing, China, 9-13 October 2006.|url=https://www.worldcat.org/oclc/812612388|publisher=IEEE|date=2006|location=Piscataway, NJ|isbn=1-4244-0258-1|oclc=812612388|last=Institute of Electrical and Electronics Engineers.}}</ref>
* [[時間序列|時間序列]]
* [[语音识别|语音识别]]
* 音乐
* [[自然语言处理|自然语言处理]]
* [[手写识别|手写识别]]
* 生物
* 飞机处理
* [[自動駕駛汽車|自動駕駛汽車]]
* [[自平衡滑行车|自平衡滑行车]]
* [[电脑游戏|电脑游戏]]
* [[动画|动画]]
* 即時天氣預報（ConvLSTM）

==参见==
* [[人工神经网络|人工神经网络]]
*[[深度学习|深度学习]]
* {{tsl|en|Prefrontal Cortex Basal Ganglia Working Memory|前额叶皮质基底节工作记忆}}（PBWM）
* [[循环神经网络|循环神经网络]]（RNN）
* [[时间序列|时间序列]]
*Seq2seq

==完整阅读==
* [http://colah.github.io/posts/2015-08-Understanding-LSTMs/ 理解LSTM网络] {{Wayback|url=http://colah.github.io/posts/2015-08-Understanding-LSTMs/ |date=20210523001042 }}，作者Christopher Olah，更新于2015年八月。
* http://people.idsia.ch/~juergen/rnn.html {{Wayback|url=http://people.idsia.ch/~juergen/rnn.html |date=20210505202602 }}
* https://www.youtube.com/watch?v=cTqVhcrilrE&list=WL&index=51

==参考==
{{reflist}}

[[Category:人工智能|Category:人工智能]]
[[Category:人工神经网络|Category:人工神经网络]]