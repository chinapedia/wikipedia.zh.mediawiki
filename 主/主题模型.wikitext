'''主题模型'''（Topic Model）在[[机器学习|机器学习]]和[[自然语言处理|自然语言处理]]等领域是用来在一系列文档中发现抽象主题的一种统计模型。直观来讲，如果一篇文章有一个中心思想，那么一些特定词语会更频繁的出现。比方说，如果一篇文章是在讲狗的，那“狗”和“骨头”等词出现的频率会高些。如果一篇文章是在讲猫的，那“猫”和“鱼”等词出现的频率会高些。而有些词例如“这个”、“和”大概在两篇文章中出现的频率会大致相等。但真实的情况是，一篇文章通常包含多种主题，而且每个主题所占比例各不相同。因此，如果一篇文章10%和猫有关，90%和狗有关，那么和狗相关的关键字出现的次数大概会是和猫相关的关键字出现次数的9倍。一个主题模型试图用数学框架来体现文档的这种特点。主题模型自动分析每个文档，统计文档内的词语，根据统计的信息来断定当前文档含有哪些主题，以及每个主题所占的比例各为多少。

主题模型最初是运用于自然语言处理相关方向，但目前以及延伸至例如[[生物信息学|生物信息学]]的其它领域。

== 历史 ==

Papadimitriou、Raghavan、Tamaki和Vempala在1998年发表的一篇论文中提出了[[潜在语义索引|潜在语义索引]]<ref name="PRTV1998">{{cite journal
 |last1        = Papadimitriou
 |first1       = Christos
 |last2        = Raghavan
 |first2       = Prabhakar
 |last3        = Tamaki
 |first3       = Hisao
 |last4        = Vempala
 |first4       = Santosh
 |title        = Latent Semantic Indexing: A probabilistic analysis
 |journal      = Proceedings of ACM PODS
 |year         = 1998
 |url          = http://www.cs.berkeley.edu/~christos/ir.ps
 |format       = Postscript
 |author       = 
 |access-date  = 2013-08-18
 |archive-url  = https://web.archive.org/web/20130509130907/http://www.cs.berkeley.edu/%7Echristos/ir.ps
 |archive-date = 2013-05-09
 |dead-url     = yes
}}</ref>。1999年，Thomas Hofmann又在此基础上，提出了[[概率性潜在语义索引|概率性潜在语义索引]]（Probabilistic Latent Semantic Indexing，简称PLSI）<ref name="hofmann1999">{{cite journal
 | last1 = Hofmann
 | first1 = Thomas
 | title = Probabilistic Latent Semantic Indexing
 | journal = Proceedings of the Twenty-Second Annual International SIGIR Conference on Research and Development in Information Retrieval
 | year = 1999
 | url = http://www.cs.brown.edu/~th/papers/Hofmann-SIGIR99.pdf
 | format = PDF
 | deadurl = yes
 | archiveurl = https://web.archive.org/web/20101214074049/http://www.cs.brown.edu/~th/papers/Hofmann-SIGIR99.pdf
 | archivedate = 2010-12-14
 | access-date = 2013-08-18
 }}</ref>。

[[隐含狄利克雷分配|隐含狄利克雷分配]]可能是最常见的主题模型，是一般化的PLSI，由Blei, David M.、[[吴恩达|吴恩达]]和Jordan, Michael I于2003年提出<ref name="blei2003">{{cite journal
 |last1        = Blei
 |first1       = David M.
 |last2        = Ng
 |first2       = Andrew Y.
 |last3        = Jordan
 |first3       = Michael I
 |authorlink3  = Michael I. Jordan
 |title        = Latent Dirichlet allocation
 |journal      = [[Journal_of_Machine_Learning_Research|Journal of Machine Learning Research]]
 |volume       = 3
 |pages        = 993–1022
 |url          = http://jmlr.csail.mit.edu/papers/v3/blei03a.html
 |doi          = 10.1162/jmlr.2003.3.4-5.993
 |last4        = Lafferty
 |first4       = John
 |date         = January 2003
 |author       = 
 |access-date  = 2013-08-18
 |archive-url  = https://web.archive.org/web/20120501152722/http://jmlr.csail.mit.edu/papers/v3/blei03a.html
 |archive-date = 2012-05-01
 |dead-url     = yes
}}</ref>。LDA允许文档拥有多种主题。其它主题模型一般是在LDA基础上改进的。例如Pachinko分布在LDA度量词语关联之上，还加入了主题的关联度。

==參見==
*[[範例理論|範例理論]]

== 参考 ==
{{Reflist|30em}}

{{自然语言处理}}

[[Category:机器学习|Category:机器学习]]
[[Category:自然語言處理|Category:自然語言處理]]