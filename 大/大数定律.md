{{noteTA
|T=zh-cn:大数定律;zh-tw:大數法則;zh-hk:大數法則;
|1=zh-cn:大数定律;zh-tw:大數法則;zh-hk:大數法則;
|G1=Math
}}
{{Probability fundamentals}}
[[File:Largenumbers.svg|以特定掷单个[[骰子]]的过程来展示大数定律。随着投掷次数的增加，所有结果的均值趋于3.5（骰子點數的期望值）。不同时候做的这个实验会在投掷次數较小的时候（左部）会表现出不同的形状，当次數变得很大（右部）的时候，它们将会非常相似。|thumb|right|400 px]]

在[[數學|數學]]與[[統計學|統計學]]中，'''大数定律'''又称'''-{zh-hans:大数法则; zh-hant:大數定律;}-'''、'''大数律'''，是描述相当多次数重复实验的结果的定律。根据这个定律知道，樣本數量越多，則其[[算术平均值|算术平均值]]就有越高的機率接近[[期望值|期望值]]。

大数定律很重要，因为它“說明”了一些随机事件的均值的长期稳定性。人们发现，在重複試驗中，随着试验次数的增加，事件发生的[[频率|频率]]趋于一个稳定值；人们同时也发现，在对物理量的测量实践中，测定值的[[算术平均|算术平均]]也具有稳定性。比如，我们向上抛一枚硬币，硬币落下后哪一面朝上是偶然的，但当我们上抛硬币的次数足够多后，达到上万次甚至几十万几百万次以后，我们就会发现，硬币每一面向上的次数约占总次数的二分之一，亦即偶然之中包含着必然。

[[切比雪夫不等式|切比雪夫不等式]]的一个特殊情况、[[辛钦定理|辛钦定理]]和[[伯努利大数定律|伯努利大数定律]]都概括了这一现象，都称为大数定律。

== 举例 ==
例如，抛掷一颗均匀的6面的骰子，1，2，3，4，5，6应等概率出现，所以每次扔出骰子後，出現點數的期望值是

<math> \frac{1+2+3+4+5+6}{6} = 3.5</math>

根据大数定理，如果多次抛掷骰子，随着抛掷次数的增加，平均值（样本平均值）应该接近3.5，根据大数定理，在多次伯努利实验中，实验概率最后收敛于理论推断的概率值，对于伯努利随机变量，理论推断的成功概率就是期望值，而若对n个相互独立的随机变量的平均值，频率越多则相对越精准。

例如硬币投掷即伯努利实验，当投掷一枚均匀的硬币，理论上得出的正面向上的概率应是1/2。因此，根据大数定理，正面朝上的比例在相对“大”的数字下，“理应”接近为1/2，尤其是正面朝上的概率在n次实验（n接近无限大时）后应几近收敛到1/2。

即使正面朝上（或背面朝上）的比例接近1/2，几乎很自然的正面与负面朝上的绝对差值（absolute difference差值范围）应该相应随着抛掷次数的增加而增加。换句话说，绝对差值的概率应该是会随着抛掷次数而接近于0。直观的来看，绝对差值的期望会增加，只是慢于抛掷次数增加的速度。

== 表现形式 ==
大数定律主要有两种表现形式：'''弱大数定律'''和'''强大数定律'''。定律的两种形式都肯定无疑地表明，样本均值

:<math>\overline{X}_n=\frac1n(X_1+\cdots+X_n) </math>

收敛于真值

:<math>\overline{X}_n \to \mu \quad\textrm{as}\quad n \to \infty</math>

其中 <math>X_1</math>, <math>X_2</math>, ... 是独立同分布、期望值<math>\operatorname{E}(X_1)=\operatorname{E}(X_2)=\,\cdots\,=\mu</math> 且皆[[勒贝格可积|勒贝格可积]]的随机变量构成的无穷序列。<math>X_j</math>的勒贝格可积性意味着期望值 <math>\operatorname{E}(X_j)</math>存在且有限。

[[方差|方差]]<math>\operatorname{Var}(X_1)=\operatorname{Var}(X_2)=\,\cdots\,= \sigma^2 <\infty</math>有限的假设是'''非必要'''的。很大或者无穷大的方差会使其收敛得緩慢一些，但大数定律仍然成立。通常采用这个假设来使证明更加简洁。

强和弱之间的差别在所断言的收敛的方式。对于这些方式的解释，参见[[随机变量的收敛|随机变量的收敛]]。

=== 弱大数定律 ===
'''弱大数定律'''也称为辛钦定理，陈述为：样本均值[[依概率收敛|依概率收敛]]于期望值。<ref>{{Cite book | author = Rick Durrett | title = Probability: Theory and Examples | publisher = Cambridge University Press | date = 2010 | pages = 61 | ISBN = 978-0-521-76539-8 | accessdate = 2013-11-18 | | language = en  }} </ref>
: <math>
\overline{X}_n\ \xrightarrow{P}\ \mu \quad\textrm{as}\quad n \to \infty
  </math>

也就是说对于任意正数 ''ε'',
: <math>
\lim_{n\to\infty}P\left(\,|\overline{X}_n-\mu| > \varepsilon\,\right) = 0 </math>

=== 强大数定律 ===
'''强大数定律'''指出，样本均值[[随机变量的收敛#依概率1收敛|以概率1收敛]]于期望值。
: <math>
\overline{X}_n\ \xrightarrow{\text{a.s.}}\ \mu \quad\textrm{as}\quad n \to \infty </math>

即
: <math>
   P\left( \lim_{n\to\infty}\overline{X}_n=\mu\right) = 1 </math>

=== 切比雪夫定理的特殊情况 ===
设 <math>a_1,\ a_2,\ \dots\ ,\ a_n,\ \dots</math> 为相互独立的随机变量，其[[数学期望|数学期望]]为：<math> \operatorname{E}(a_i) = \mu \quad (i = 1,\ 2,\ \dots) </math>，[[方差|方差]]为：<math> \operatorname{Var}(a_i) = \sigma^2 \quad (i=1,\ 2,\ \dots)</math>

则序列<math>\overline{a}= \frac{1}{n} \sum_{i=1}^n a_i</math>[[依概率收敛|依概率收敛]]于<math>\mu</math>（即收敛于此数列的数学期望<math>E(a_i)</math>）。

换言之，在定理条件下，当<math>n</math>无限变大时，<math>n</math>个随机变量的[[算术平均|算术平均]]将变成一个常数。

=== 伯努利大数定律 ===
设在<math>n</math>次独立重复[[伯努利试验|伯努利试验]]中，<br />
事件<math>X</math>发生的次数为<math> n_x</math>。<br />
事件<math>X</math>在每次试验中发生的母體機率为<math>p</math>。<br />
<math> \frac{n_x}{n}</math>代表樣本發生事件<math>X</math>的频率。

大数定律可用機率極限值定義:
则对任意正数<math> \varepsilon >0 </math>，下式成立：
:<math> \lim_{n \to \infty}{P{\left\{ \left|\frac{n_x}{n} - p \right| < \varepsilon \right\}}} = 1</math>

定理表明事件发生的频率依機率收敛于事件的[[母體|母體]]機率。<br />
定理以严格的数学形式表达了频率的稳定性。<br />
就是说当<math>n</math>很大时，事件发生的频率于母體機率有较大偏差的可能性很小。

== 参见 ==
* [[概率论|概率论]]

== 参考文献 ==
{{reflist}}

==外部連結==
* [http://episte.math.ntu.edu.tw/articles/sm/sm_16_06_1/ 二項分布與大數法則理論與實際相連]

[[Category:数学定理|D]]
[[Category:统计学定律|Category:统计学定律]]
[[Category:保險|Category:保險]]