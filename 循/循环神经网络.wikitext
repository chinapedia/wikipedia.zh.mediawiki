{{noteTA
|T=zh-cn:循环神经网络; zh-tw:循環神經網路; 
|1=zh-cn:网络; zh-tw:網路; zh-hk:網絡; 
}}
{{机器学习导航栏}}
{{Distinguish|recursive neural network}}
'''循环神经网络'''（Recurrent neural network：'''RNN'''）是神經網絡的一種。单纯的RNN因为无法处理随着递归，权重指数级爆炸或[[梯度消失问题|梯度消失问题]]，难以捕捉长期时间关联；而结合不同的'''[[LSTM|LSTM]]'''可以很好解决这个问题。<ref>[http://arxiv.org/pdf/1402.3511v1.pdf 时钟结构的RNN] {{Wayback|url=http://arxiv.org/pdf/1402.3511v1.pdf |date=20211030162805 }}，2014年2月。</ref><ref>[http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf 循环网络结构的经验之谈] {{Wayback|url=http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf |date=20170329161943 }}，谷歌论文，2015年。</ref>

时间循环神经网络可以描述动态时间行为，因为和[[前馈神经网络|前馈神经网络]]（feedforward neural network）接受较特定结构的输入不同，RNN将状态在自身网络中循环传递，因此可以接受更广泛的[[时间序列|时间序列]]结构输入。[[手写识别|手写识别]]是最早成功利用RNN的研究结果。<ref>A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, J. Schmidhuber. A Novel Connectionist System for Improved Unconstrained Handwriting Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, 2009.</ref>

== 历史 ==
递归神经网络是基于[[大卫·鲁梅尔哈特|大卫·鲁梅尔哈特]]1986年的工作<ref>{{Cite journal|title=Learning representations by back-propagating errors|last=Williams|first=Ronald J.|last2=Hinton|first2=Geoffrey E.|date=October 1986|journal=Nature|issue=6088|doi=10.1038/323533a0|volume=323|pages=533–536|bibcode=1986Natur.323..533R|issn=1476-4687|last3=Rumelhart|first3=David E.}}</ref>。1982年，约翰·霍普菲尔德发现了[[Hopfield神经网络|Hopfield神经网络]]——一种特殊的RNN。1993年，一个神经历史压缩器系统解决了一个“非常深度学习”的任务，这个任务在RNN展开之后有1000多个后续层<ref name="schmidhuber1993">{{Cite book|url=ftp://ftp.idsia.ch/pub/juergen/habilitation.pdf|title=Habilitation thesis: System modeling and optimization|last=Schmidhuber|first=Jürgen|year=1993}} Page 150 ff demonstrates credit assignment across the equivalent of 1,200 layers in an unfolded RNN.</ref>。

=== LSTM ===
Hochreiter和Schmidhuber于1997年提出了长短期记忆(LSTM)网络，并在多个应用领域创造了精确度记录<ref name="lstm">{{Cite journal|title=Long Short-Term Memory|last=Hochreiter|first=Sepp|last2=Schmidhuber|first2=Jürgen|date=1997-11-01|journal=Neural Computation|issue=8|doi=10.1162/neco.1997.9.8.1735|volume=9|pages=1735–1780|pmid=9377276|author-link=Sepp Hochreiter}}</ref>。

大约在2007年，LSTM开始革新语音识别领域，在某些语音应用中胜过传统模型<ref name="fernandez2007keyword">{{Cite book|last=Fernández|first=Santiago|last2=Graves|first2=Alex|last3=Schmidhuber|first3=Jürgen|year=2007|title=An Application of Recurrent Neural Networks to Discriminative Keyword Spotting|url=http://dl.acm.org/citation.cfm?id=1778066.1778092|journal=Proceedings of the 17th International Conference on Artificial Neural Networks|series=ICANN'07|location=Berlin, Heidelberg|publisher=Springer-Verlag|pages=220–229|isbn=978-3-540-74693-5|access-date=2020-02-27|archive-date=2018-10-06|archive-url=https://web.archive.org/web/20181006174541/https://dl.acm.org/citation.cfm?id=1778066.1778092}}</ref>。2009年，一个由 {{Translink|en|Connectionist temporal classification|4=CTC}} 训练的LSTM网络赢得了多项连笔手写识别竞赛，成为第一个赢得模式识别竞赛的RNN。<ref name="schmidhuber2015" /><ref name="graves20093">{{Cite document |last2=Schmidhuber |first2=Jürgen |year=2009 |editor-last=Bengio |editor-first=Yoshua |title=Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks |url=https://papers.nips.cc/paper/3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks |publisher=Neural Information Processing Systems (NIPS) Foundation |pages=545–552 |editor-last2=Schuurmans |editor-first2=Dale |editor-last3=Lafferty |editor-first3=John |editor-last4=Williams |editor-first4=Chris editor-K. I. |editor-last5=Culotta |editor-first5=Aron |last1=Graves |first1=Alex |journal= |access-date=2020-02-27 |archive-date=2020-09-07 |archive-url=https://web.archive.org/web/20200907203835/https://papers.nips.cc/paper/3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks }}</ref>2014年，[[百度|百度]]在不使用任何传统语音处理方法的情况下，使用经过CTC训练的RNNs打破了Switchboard Hub5'00 语音识别基准。<ref name="hannun2014">{{cite arxiv |last=Hannun |first=Awni |last2=Case |first2=Carl |last3=Casper |first3=Jared |last4=Catanzaro |first4=Bryan |last5=Diamos |first5=Greg |last6=Elsen |first6=Erich |last7=Prenger |first7=Ryan |last8=Satheesh |first8=Sanjeev |last9=Sengupta |first9=Shubho |date=2014-12-17 |title=Deep Speech: Scaling up end-to-end speech recognition |eprint=1412.5567 |class=cs.CL}}</ref>

LSTM还改进了大词汇量语音识别<ref name="sak2014">{{Cite web |url=https://static.googleusercontent.com/media/research.google.com/en/pubs/archive/43905.pdf |title=Long Short-Term Memory recurrent neural network architectures for large scale acoustic modeling |last=Sak |first=Haşim |last2=Senior |first2=Andrew |last3=Beaufays |first3=Françoise |year=2014 |access-date=2020-02-28 |archive-url=https://web.archive.org/web/20190922234538/https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf |archive-date=2019-09-22 |dead-url=yes }}</ref><ref name="liwu2015">{{cite arxiv |last=Li |first=Xiangang |last2=Wu |first2=Xihong |date=2014-10-15 |title=Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition |eprint=1410.4281 |class=cs.CL }}</ref>和[[语音合成|文本到语音合成]]<ref name="fan2015">Fan, Bo; Wang, Lijuan; Soong, Frank K.; Xie, Lei (2015) "Photo-Real Talking Head with Deep Bidirectional LSTM", in ''Proceedings of ICASSP 2015''</ref>并在谷歌[[Android|安卓系统]]中使用<ref name="schmidhuber2015" /><ref name="zen2015">{{Cite web |url=https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43266.pdf |title=Unidirectional Long Short-Term Memory Recurrent Neural Network with Recurrent Output Layer for Low-Latency Speech Synthesis |last=Zen |first=Heiga |last2=Sak |first2=Haşim |year=2015 |website=Google.com |publisher=ICASSP |pages=4470–4474 |access-date=2020-02-27 |archive-date=2021-05-09 |archive-url=https://web.archive.org/web/20210509123113/https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43266.pdf |dead-url=no }}</ref>。据报道，2015年，谷歌语音识别通过接受过CTC训练的LSTM(谷歌语音搜索使用的)实现了49%的引用量的大幅提升。<ref name="sak2015">{{Cite web |url=http://googleresearch.blogspot.ch/2015/09/google-voice-search-faster-and-more.html |title=Google voice search: faster and more accurate |last=Sak |first=Haşim |last2=Senior |first2=Andrew |date=September 2015 |last3=Rao |first3=Kanishka |last4=Beaufays |first4=Françoise |last5=Schalkwyk |first5=Johan |access-date=2020-02-27 |archive-date=2016-03-09 |archive-url=https://web.archive.org/web/20160309191532/http://googleresearch.blogspot.ch/2015/09/google-voice-search-faster-and-more.html |dead-url=no }}</ref>

LSTM打破了改进[[机器翻译|机器翻译]]<ref name="sutskever2014">{{Cite journal |last=Sutskever |first=Ilya |last2=Vinyals |first2=Oriol |last3=Le |first3=Quoc V. |year=2014 |title=Sequence to Sequence Learning with Neural Networks |url=https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf |journal=Electronic Proceedings of the Neural Information Processing Systems Conference |volume=27 |pages=5346 |arxiv=1409.3215 |bibcode=2014arXiv1409.3215S |access-date=2020-02-27 |archive-date=2021-05-09 |archive-url=https://web.archive.org/web/20210509123145/https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf |dead-url=no }}</ref>、[[語言模型|语言建模]]<ref name="vinyals2016">{{cite arxiv |last=Jozefowicz |first=Rafal |last2=Vinyals |first2=Oriol |last3=Schuster |first3=Mike |last4=Shazeer |first4=Noam |last5=Wu |first5=Yonghui |date=2016-02-07 |title=Exploring the Limits of Language Modeling |eprint=1602.02410 |class=cs.CL}}</ref>和多语言处理的记录<ref name="gillick2015">{{cite arxiv |last=Gillick |first=Dan |last2=Brunk |first2=Cliff |last3=Vinyals |first3=Oriol |last4=Subramanya |first4=Amarnag |date=2015-11-30 |title=Multilingual Language Processing From Bytes |eprint=1512.00103 |class=cs.CL}}</ref>。 LSTM 结合[[卷积神经网络|卷积神经网络]]改进了[[图像自动标注|图像自动标注]] 。<ref name="vinyals2015">{{cite arxiv |last=Vinyals |first=Oriol |last2=Toshev |first2=Alexander |last3=Bengio |first3=Samy |last4=Erhan |first4=Dumitru |date=2014-11-17 |title=Show and Tell: A Neural Image Caption Generator |eprint=1411.4555 |class=cs.CV }}</ref>

==循环神经网络==

===编码器===

循环神经网络将输入序列<math>\vec{x}</math>编码为一个固定长度的隐藏状态<math>\vec{h}</math>，这里有（用[[自然语言处理|自然语言处理]]作为例子）：
*  <math>\vec{x} = (x_t, ..., x_1)</math> 是输入序列，比如编码为数字的一系列词语，整个序列就是完整的句子。
* <math>\vec{h_t} = f( x_t, \vec{h_{t-1}} )</math>是随时间更新的隐藏状态。当新的词语输入到方程中，之前的状态<math>\vec{h_{t-1}}</math>就转换为和当前输入<math>x_t</math>相关的<math>\vec{h_{t}}</math>，距离当前时间越长，越早输入的序列，在更新后的状态中所占权重越小，从而表现出时间相关性。<ref name=phrase_RNN>{{Cite web |url=http://arxiv.org/abs/1406.1078 |title=Cho, K., van et al (2014a). 统计机器翻译方法：使用RNN进行短语编码解码。 |access-date=2015-04-23 |archive-date=2022-02-08 |archive-url=https://web.archive.org/web/20220208192330/https://arxiv.org/abs/1406.1078 }}</ref>

其中，计算隐藏状态的方程<math>f(x, h)</math>是一个[[非线性|非线性方程]]，可以是简单的[[逻辑函数|Logistic方程]]（tanh），也可以是复杂的[[LSTM|LSTM]]单元（Long Short-Term Memory）。<ref name=phrase_RNN></ref> <ref name=LSTM>{{Cite web |url=http://dl.acm.org/citation.cfm?id=1246450 |title=经典模型Long Short-Term Memory的原论文，发表于1997年。 |access-date=2015-04-23 |archive-date=2019-08-09 |archive-url=https://web.archive.org/web/20190809205820/https://dl.acm.org/citation.cfm?id=1246450 }}</ref> 而有了隐藏状态序列，就可以对下一个出现的词语进行预测：
* <math>p(y_{t}) = p( y_{t} \, | \, y_{t-1},..., y_{1} )</math>，其中<math>y_t</math>是第t个位置上的输出，它的概率基于之前输出的所有词语。
* 以上概率可以通过隐藏状态来计算：<math>p(y_t) = g( y_{t-1}, \vec{h_t}, \vec{c} )</math>，<math>\vec{c}</math>是所有隐藏状态的编码，总含了所有隐藏状态，比如可以是简单的最终隐藏状态<math>\vec{h_t}</math>，也可以是[[非线性|非线性方程]]的输出<math>f( h_{t}, ..., h_1 )</math>。因为隐藏状态t就编码了第t个输入前全部的输入信息，<math>y_t</math>也迭代式地隐含了之前的全部输出信息，所以这个概率计算方法是合理的。

这里的非线性方程<math>g( y, h, c )</math>可以是一个复杂的[[前馈神经网络|前馈神经网络]]，也可以是简单的非线性方程（但有可能因此无法适应复杂的条件而得不到任何有用结果）。给出的概率可以用[[监督学习|监督学习]]的方法优化内部参数来给出翻译，也可以训练后用来给可能的备选词语，用计算其第j个备选词<math>y_{t, j}</math>出现在下一位置的概率，给它们排序。排序后用于其它翻译系统，可以提升翻译质量。

===解码器===

更复杂的情况下循环神经网络还可以结合编码器作为[[解码器|解码器]]（Decoder），用于将编码后（Encoded）的信息解码为人类可识别的信息。也就是上述例子中的<math>y_t = f( y_{t-1}, h_t, c )</math>过程，当中非线性模型<math>f</math>就是作为输出的循环神经网络。只是在解码过程中，隐藏状态因为是解码器的参数，所以为了发挥时间序列的特性，需要对<math>h_t'</math>继续进行迭代：
* <math>h_t' = g(h_{t-1}, y_{t-1}, c)</math>，<math>\vec{c}</math>是解码器传递给编码器的参数，是解码器中状态的summary。<math>h_t'</math>是解码器的隐藏状态。<math>y_t</math>是第t个输出。
* 当输入仍为<math>\vec{x} = (x_t, ..., x_1)</math>，输出是<math>\vec{y} = (y_t, ..., y_1)</math>，最大化[[条件概率|条件概率]]<math>P(\vec{y} \, | \, \vec{x})</math>后就是最好的翻译结果。

===双向读取===

用两个循环神经网络双向读取一个序列可以使人工智能获得“注意力”。简单的做法是将一个句子分别从两个方向编码为两个隐藏状态，然后将两个<math>\vec{h}</math>拼接在一起作为隐藏状态。<ref>[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=650093 Bidirectional RNN, (1997)]</ref> <ref name=rnn_alignment>{{Cite web |url=http://arxiv.org/abs/1409.0473 |title=Cho. k. et al 使用RNN和对齐模型的短语统计机器翻译方法。(2014.a.) |access-date=2015-04-23 |archive-date=2022-06-19 |archive-url=https://web.archive.org/web/20220619125026/https://arxiv.org/abs/1409.0473 }}</ref>这种方法能提高模型表现的原因之一可能是因为不同方向的读取在输入和输出之间创造了更多短期依赖关系，从而被RNN中的LSTM单元（及其变体）捕捉，例如在实验中发现颠倒输入序列的顺序（但不改变输出的顺序）可以意外达到提高表现的效果。<ref>[http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf 神经网络学习：序列到序列] {{Wayback|url=http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf |date=20180304223020 }}，作者Sutskever，2014年。</ref>

==结构递归神经网络==

结构递归（Recursive）神经网络是一类用结构递归的方式构建的网络，比如说[[递归自编码机|递归自编码机]]（Recursive Autoencoder），在自然语言处理的神经网络分析方法中用于解析语句。<ref name=rae_sentiment>{{Cite web |url=http://dl.acm.org/citation.cfm?id=2145450 |title=半监督RAE方法：预测文本情感分布。 |access-date=2015-04-23 |archive-date=2018-09-27 |archive-url=https://web.archive.org/web/20180927063254/https://dl.acm.org/citation.cfm?id=2145450 }}</ref> <ref name=rae>{{Cite web |url=http://jan.stanford.edu/pubs/emnlp2013_ZouSocherCerManning.pdf |title=Bilingual Word Embeddings for Phrase-Based Machine Translation. (2014). |access-date=2015-04-23 |archive-date=2016-07-05 |archive-url=https://web.archive.org/web/20160705174655/http://jan.stanford.edu/pubs/emnlp2013_ZouSocherCerManning.pdf }}</ref>

== 架构 ==
RNN 有很多不同的变种

=== 完全循环 ===
基本的 RNN 是由{{Translink|en|Artificial neuron|4=人工神经元}}组织成的连续的层的网络。给定层中的每个节点都通过{{Translink|en|Directed graph|4=有向}}(单向)连接连接到下一个连续层中的每个其他节点。每个节点(神经元)都有一个时变的实值激活。每个连接(突触)都有一个可修改的实值{{Translink|en|Weighting|4=权重}}。节点要么是输入节点(从网络外部接收数据)，要么是输出节点(产生结果)，要么是隐藏节点(在从输入到输出的过程中修改数据)。

对于离散时间设置中的[[監督式學習|监督学习]]，实值输入向量序列到达输入节点，一次一个向量。在任何给定的时间步长，每个非输入单元将其当前激活(结果)计算为与其连接的所有单元的激活的加权和的非线性函数。可以在特定的时间步长为某些输出单元提供主管给定的目标激活。例如，如果输入序列是对应于口语数字的语音信号，则在序列末尾的最终目标输出可以是对该数字进行分类的标签。

在[[强化学习|强化学习]]环境中，没有教师提供目标信号。相反，适应度函数或奖励函数偶尔用于评估RNN的性能，它通过影响输出单元来影响其输入流，输出单元和一个可以影响环境的执行器相连。这可以被用来玩一个游戏，在这个游戏中，进度是用赢得的点数来衡量的。

每个序列产生一个误差，作为所有目标信号与网络计算的相应激活的偏差之和。对于大量序列的训练集，总误差是所有单个序列误差的总和。

=== {{Anchor|Elman网络|Jordan网络}}Elman 网络和 Jordan 网络 ===
[[File:Elman_srnn.png|右]]
Elman网络是一个三层网络(在图中水平排列为x、y和z)，添加了一组上下文单元(在图中为u)。中间(隐藏)层连接到这些权重为1的上下文单元<ref name="bmm615">Cruse, Holk; [http://www.brains-minds-media.org/archive/615/bmm615.pdf ''Neural Networks as Cybernetic Systems''] {{Wayback|url=http://www.brains-minds-media.org/archive/615/bmm615.pdf |date=20161020014209 }}, 2nd and revised edition</ref>。在每个时间步，输入被向前反馈，并且学习规则被应用。固定的反向连接在上下文单元中保存隐藏单元的先前值的副本(因为它们在应用学习规则之前在连接上传播)。因此，网络可以保持某种状态，允许它执行诸如序列预测之类的任务，这些任务超出了标准[[多层感知器|多层感知器]]的能力。

Jordan网络类似于Elman网络。上下文单元是从输出层而不是隐藏层馈送的。Jordan网络中的上下文单元也称为状态层。他们与自己有着经常性的联系。<ref name="bmm6152" />

Elman和Jordan网络也被称为“简单循环网络”。

; Elman 网络<ref>{{cite journal|title=Finding Structure in Time|last=Elman|first=Jeffrey L.|journal=Cognitive Science|issue=2|doi=10.1016/0364-0213(90)90002-E|year=1990|volume=14|pages=179–211}}</ref>
: <math>
\begin{align}
h_t &= \sigma_h(W_{h} x_t + U_{h} h_{t-1} + b_h) \\
y_t &= \sigma_y(W_{y} h_t + b_y)
\end{align}
</math>
; Jordan 网络<ref>{{Cite book|last=Jordan|first=Michael I.|date=1997-01-01|title=Serial Order: A Parallel Distributed Processing Approach|journal=Advances in Psychology|series=Neural-Network Models of Cognition|volume=121|pages=471–495|doi=10.1016/s0166-4115(97)80111-2|isbn=9780444819314}}</ref>
: <math>
\begin{align}
h_t &= \sigma_h(W_{h} x_t + U_{h} y_{t-1} + b_h) \\
y_t &= \sigma_y(W_{y} h_t + b_y)
\end{align}
</math>

变量和函数

* <math>x_t</math>: 输入向量
* <math>h_t</math>: 隐藏层向量
* <math>y_t</math>: 输出向量
* <math>W</math>, <math>U</math> 和 <math>b</math>: 参数矩阵和参数向量
* <math>\sigma_h</math> 和 <math>\sigma_y</math>: [[激活函数|激活函数]]

== 参考 ==
{{Reflist|2}}

== 外部連結 ==
* [http://arxiv.org/abs/1506.00019 A Critical Review of Recurrent Neural Networks for Sequence Learning] {{Wayback|url=http://arxiv.org/abs/1506.00019 |date=20220623131450 }}

[[Category:人工智能|Category:人工智能]]
[[Category:人工神经网络|Category:人工神经网络]]