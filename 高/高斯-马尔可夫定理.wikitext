{{Expand|time=2009-07-26}}
{{noteTA
|G1=Math
|T=zh-cn: 高斯-马尔可夫定理 ;zh-tw:高斯-馬可夫定理  ;
|1=zh-cn:互不相关;zh-tw:彼此獨立;
|2=zh-tw:檢定 ;zh-hant: 檢驗;zh-cn:  检验;
|3=zh-tw: 母數 ;zh-cn: 参数 ;zh-hant:參數
|4=zh-tw:不偏 ;zh-cn:无偏  ;
|5=zh-tw: 貝氏;zh-hant:貝葉斯 ;zh-cn:  贝叶斯;
|6=zh-cn:不相关;zh-tw:獨立;
|7=zh-tw:馬可夫 ;zh-hans:马尔可夫 ;
}}

{{回归侧栏}}
'''高斯-馬可夫定理'''（{{lang-en|Gauss-Markov Theorem}}），在[[統計學|統計學]]中陳述的是在[[线性回归|线性回归]]模型中，如果线性模型满足高斯马尔可夫假定，则回归系数的'''最佳线性[[偏差|无偏]][[点估计|估计]]'''（'''BLUE''', Best Linear unbiased estimator）就是[[最小二乘法|普通最小二乘法估计]]。

* 这里'''最佳'''的意思是指相较于其他估计量有更小[[方差|方差]]的[[估计量|估计量]]，同时把对估计量的寻找限制在'''所有可能的线性无偏估计量'''中。
* '''值得注意'''的是这里不需要假定误差满足同分布或[[正态分布|正态分布]]。
*线性模型指对于参数是线性的，因此线性模型并非看起来那么有约束性，通过适当的对y与x做变换(如logy与x)，可以得到y与x的非线性关系，但并未跳出线性模型的范畴。

== 表述 ==

=== 简单（一元）线性回归模型 ===

对于简单（一元）线性回归模型，

:<math>y=\beta_0+\beta_1 x+\varepsilon</math>

其中<math>\beta_0</math>和<math>\beta_1</math>是'''非随机'''但不能观测到的参数，<math>x_i</math>是'''非随机'''且可观测到的一般变量，<math>\varepsilon_i</math>是'''不可观测'''的随机变量，或称为随机误差或噪音，<math>y_i</math>是'''可观测'''的随机变量。

'''高斯-马尔可夫定理的假设条件'''是：

* 在总体模型中，各变量关系为<math>y=\beta_0+\beta_1 x+\varepsilon</math>(线性于参数)
* 我们具有服从于上述模型的随机样本，样本容量为n（随机抽样），
* x的样本结果为非完全相同的数值（解释变量的样本有波动），
* 对于给定的解释变量，误差的期望为零，换言之<math>{\rm E}\left(\varepsilon | x \right)=0</math> （零条件均值），
* 对于给定的解释变量，误差具有相同的方差，换言之 <math>{\rm Var}\left(\varepsilon | x\right)=\sigma^2</math>（同方差性）。

则对<math>\beta_0</math>和<math>\beta_1</math>的最佳线性无偏估计为，

:<math>
    \hat\beta_1 = \frac{ \sum{x_iy_i} - \frac{1}{n}\sum{x_i}\sum{y_i} }
                     { \sum{x_i^2} - \frac{1}{n}(\sum{x_i})^2 } = \frac{\widehat{\text{Cov}\left(x,y\right)}}{ \hat {\sigma_x}^2 } =\hat{\rho} _{xy} \frac{\hat {\sigma_x}}{\hat {\sigma_y}},\quad
    \hat\beta_0 = \overline{y} - \hat\beta_1\,\overline{x}\ .
</math>

=== 多元线性回归模型 ===

对于多元线性回归模型，

:<math>y_i=\sum_{j=0}^p \beta_j x_{ij}+\varepsilon_i</math>, <math>x_{i0}=1; \quad i = 1, \dots n.</math>

使用矩阵形式，线性回归模型可简化记为<math>\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}</math>，其中采用了以下记号：

<math>\mathbf{Y}=(y_1, y_2, \dots, y_n)^T</math> (观测值向量，Vector of Responses), 

<math>\mathbf{X}=(x_{ij})=
    \begin{bmatrix}
    1 & x_{11} & x_{12} & \cdots & x_{1p} \\
    1 & x_{21} & x_{22} & \cdots & x_{2p} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & x_{n1} & x_{n2} & \cdots & x_{np}
    \end{bmatrix}
</math> (设计矩阵，Design Matrix),

<math>\boldsymbol{\beta}=(\beta_0, \beta_1, \dots, \beta_p)^T</math> (参数向量，Vector of Parameters), 

<math>\boldsymbol{\varepsilon}=(\varepsilon_1, \varepsilon_2, \dots, \varepsilon_n)^T</math> (随机误差向量，Vectors of Error)。

'''高斯-马尔可夫定理的假设条件'''是：

* <math>{\rm E}\left(\boldsymbol{\varepsilon}\mid\mathbf{X}\right)=0</math> ，<math>\forall \mathbf{X}</math>（零均值），
* <math>{\rm Var}\left(\boldsymbol{\varepsilon}\mid\mathbf{X}\right)={\rm E}\left(\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}^T\mid\mathbf{X}\right)=\sigma^2_\varepsilon\mathbf{I_n}</math>，（同方差且不相关），其中<math>\mathbf{I_n}</math>为n阶[[单位矩阵|单位矩阵]](Identity Matrix)。

则对<math>\boldsymbol{\beta}</math>的最佳线性无偏估计为

:<math>
    \hat\boldsymbol{\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
</math>

== 证明 ==

'''首先'''，注意的是这里数据是<math>\mathbf{Y}</math>而非<math>\mathbf{X}</math>，我们希望找到<math>\boldsymbol{\beta}</math>对于<math>\mathbf{Y}</math>的线性估计量，记作

:<math>\hat\boldsymbol{\beta} = \mathbf{M}+\mathbf{N}\mathbf{Y}</math>

其中<math>\hat\boldsymbol{\beta}</math>，<math>\mathbf{M}</math>，<math>\mathbf{N}</math>和<math>\mathbf{Y}</math>分别是<math>(p+1)\times1</math>，<math>(p+1)\times1</math>，<math>(p+1)\times n</math>和<math>n\times1</math>矩阵。

根据零均值假设所得，

:<math>{\rm E}\left(\hat\boldsymbol{\beta}\mid\mathbf{X}\right) = \mathbf{M}+\mathbf{N}{\rm E}\left(\mathbf{Y}\mid\mathbf{X}\right)=\mathbf{M}+\mathbf{N}\mathbf{X}\boldsymbol{\beta}</math>

'''其次'''，我们同时限制寻找的估计量为无偏的估计量，即要求<math>{\rm E}\left(\hat\boldsymbol{\beta}\right) = \boldsymbol{\beta}</math>，因此有

:<math>\mathbf{M}=\mathbf{0}</math>（[[零矩阵|零矩阵]]），<math>\mathbf{N}\mathbf{X}=\mathbf{I_{p+1}}</math>

== 参见 ==
{{div col|2}}
* [[方差分析|方差分析]]
* [[安斯库姆四重奏|安斯库姆四重奏]]
* [[横截面回归|横截面回归]]
* [[曲线拟合|曲线拟合]]
* [[经验贝叶斯方法|经验贝叶斯方法]]
* [[邏輯迴歸|邏輯迴歸]]
* [[M估计|M估计]]
* [[非线性回归|非线性回归]]
* [[非参数回归|非参数回归]]
* [[多元自适应回归样条|多元自适应回归样条]]
* [[Lack-of-fit_sum_of_squares|Lack-of-fit sum of squares]]<!--没有什么好的译名-->
* [[截断回归模型|截断回归模型]]
* [[删失回归模型|删失回归模型]]
*[[線性回歸|简单线性回归]]
* [[分段回归|分段线性回归]]
{{div col end}}

== 外部連結 ==
*[https://web.archive.org/web/19990508225359/http://members.aol.com/jeff570/g.html Earliest Known Uses of Some of the Words of Mathematics: G] (brief history and explanation of its name)
*[http://www.xycoon.com/ols1.htm Proof of the Gauss Markov theorem for multiple linear regression] (makes use of matrix algebra)
*[https://web.archive.org/web/20040213071852/http://emlab.berkeley.edu/GMTheorem/index.html A Proof of the Gauss Markov theorem using geometry]
[[Category:数学定理|G]]
[[Category:统计学|Category:统计学]]