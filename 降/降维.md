{{machine learning bar}}
在[[机器学习|机器学习]]和[[统计学|统计学]]领域，'''降维'''是指在某些限定条件下，降低随机变量个数，得到一组“不相关”主变量的过程<ref>{{Cite journal | last1 = Roweis | first1 = S. T. | last2 = Saul | first2 = L. K. | title = Nonlinear Dimensionality Reduction by Locally Linear Embedding | doi = 10.1126/science.290.5500.2323 | journal = Science | volume = 290 | issue = 5500 | pages = 2323–2326 | year = 2000 | pmid =  11125150| pmc = }}</ref>。 降维可进一步细分为[[特征选择|变量选择]]和[[特征提取|特征提取]]两大方法。

==变量选择==
{{main|特征选择|l1=变量选择}}

[[特征选择|变量选择]]假定数据中包含大量冗余或无关变量（或称特征、属性、指标等），旨在从原有变量中找出主要变量。现代统计学中对变量选择的研究文献，大多集中于{{tsl|en|High-dimensional_statistics|高维回归分析}}，其中最具代表性的方法包括：
* {{tsl|en|Lasso}} ({{tsl|en|Robert Tibshirani}}提出)
* [[Elastic_net|Elastic net]] ({{tsl|en|Hui Zou|邹晖}}和{{tsl|en|Trevor Hastie}}提出)
* [[SCAD|SCAD]] ([[范剑青|范剑青]]和{{tsl|en|Runze Li|李润泽}}提出)
* [[SURE_screening|SURE screening]] ([[范剑青|范剑青]]和[[吕金翅|吕金翅]]提出)
* [[PLUS|PLUS]] ([[张存惠|张存惠]]提出)

==特征提取==
{{main|特征提取}}

[[特征提取|特征提取]]可以看作[[特征选择|变量选择]]方法的一般化：[[特征选择|变量选择]]假设在原始数据中，变量数目浩繁，但只有少数几个真正起作用；而[[特征提取|特征提取]]则认为在所有变量可能的函数(比如这些变量各种可能的线性组合)中，只有少数几个真正起作用。有代表性的方法包括：
* [[主成分分析|主成分分析(PCA)]]
* {{tsl|en|Factor analysis|因子分析}}
* {{tsl|en|Kernel method|核方法}}(教科书中称为“Kernel method”或“Kernel trick”，常与其他方法如[[主成分分析|PCA]]组合使用)
* 基于距离的方法，例如：
** [[多维尺度|多维尺度分析]]
** {{tsl|en|Non-negative_matrix_factorization|非负矩阵分解}}
** {{tsl|en|Random projection|随机投影法}}(理论依据是[[约翰逊-林登斯特劳斯定理|约翰逊-林登斯特劳斯定理]])

==参见==
* [[特征选择|变量选择]]
* [[特征提取|特征提取]]
* [[约翰逊-林登斯特劳斯定理|约翰逊-林登斯特劳斯定理]]

==参考文献==

{{Reflist|30em}}

[[Category:统计学|Category:统计学]]