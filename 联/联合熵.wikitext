{{Unreferenced|time=2015-03-16T13:46:07+00:00}}

[[File:Entropy-mutual-information-relative-entropy-relation-diagram.svg|thumb]]

'''联合[[熵_(信息论)|熵]]'''是一集变量之间不确定性的衡量手段。

==定义==
两个变量<math>X</math> 和 <math>Y</math> 的联合[[信息熵|信息熵]]定义为：

:<math>\Eta(X,Y) = -\sum_{x} \sum_{y} P(x,y) \log_2[P(x,y)] \!</math>

其中 <math>x</math> 和 <math>y</math> 是 <math>X</math> 和 <math>Y</math>的特定值, 相应地, <math>P(x,y)</math> 是这些值一起出现的[[联合分布|联合概率]],  若<math>P(x,y)=0</math> 为0，则<math>P(x,y) \log_2[P(x,y)]</math> 定义为0。

对于两个以上的变量 <math>X_1, ..., X_n</math> ，该式的一般形式为：

:<math>\Eta(X_1, ..., X_n) = -\sum_{x_1} ... \sum_{x_n} P(x_1, ..., x_n) \log_2[P(x_1, ..., x_n)] \!</math>

其中 <math>x_1,...,x_n</math> 是 <math>X_1,...,X_n</math> 的特定值，相应地，<math>P(x_1, ..., x_n)</math> 是这些变量同时出现的概率，若<math>P(x_1, ..., x_n)=0</math>为0，则 <math>P(x_1, ..., x_n) \log_2[P(x_1, ..., x_n)]</math> 被定义为0.

==性質==

===大于每个独立的熵===

一集变量的联合熵大于或等于这集变量中任一个的独立熵。

:<math>\Eta(X,Y) \geq \max[\Eta(X),\Eta(Y)]</math>

:<math>\Eta(X_1, ..., X_n) \geq \max[H(X_1), ..., H(X_n)]</math>

===少于或等于独立熵的和===

一集变量的联合熵少于或等于这集变量的独立熵之和。这是[[次加性|次可加性]]的一个例子。该不等式有且只有在<math>X</math>和<math>Y</math>均为统计独立的时候相等。

:<math>\Eta(X,Y) \leq \Eta(X) + \Eta(Y)</math>

:<math>\Eta(X_1, ..., X_n) \leq \Eta(X_1) + ... + \Eta(X_n)</math>

==与其他熵测量手段的关系==

在[[条件熵|条件熵]]的定义中，使用了联合熵

:<math>\Eta(X|Y) = \Eta(X,Y) - \Eta(Y)\,</math>

[[互信息|互信息]]的定义中也出现了联合熵的身影：

:<math>I(X;Y) = \Eta(X) + \Eta(Y) - \Eta(X,Y)\,</math>

在[[量子信息|量子信息]]理论中， 联合熵被扩展到{{Link-en|联合量子熵|joint quantum entropy}}。

[[Category:信息學熵|Category:信息學熵]]