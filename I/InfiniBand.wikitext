{{NoteTA
|G1=IT
}}
'''InfiniBand'''（直译为“无限带宽”技术，缩写为'''IB'''）是一个用于[[超级计算机|高性能计算]]的计算机网络通信标准，它具有极高的[[吞吐量|吞吐量]]和极低的[[潜伏时间_(工程学)|延迟]]，用于计算机与计算机之间的数据互连。InfiniBand也用作服务器与存储系统之间的直接或交换互连，以及存储系统之间的互连。<ref>{{Cite web |url=http://www.ddn.com/products/storage-platform-sfa12kx/config-specs |title=存档副本 |access-date=2017-01-09 |archive-url=https://web.archive.org/web/20170707051213/http://www.ddn.com/products/storage-platform-sfa12kx/config-specs/ |archive-date=2017-07-07 |dead-url=yes }}</ref>

截至2014年，它是超级计算机最常用的互连技术。{{tsl|en|Mellanox}}和[[英特尔|英特尔]]制造InfiniBand[[主机总线适配器|主机总线适配器]]和[[網路交換器|網路交換器]]，并根据2016年2月的报道，<ref>{{Cite web |url=http://www.nextplatform.com/2016/02/22/oracle-engineers-its-own-infiniband-interconnects/ |title=存档副本 |accessdate=2017-01-09 |archive-date=2020-11-29 |archive-url=https://web.archive.org/web/20201129104949/https://www.nextplatform.com/2016/02/22/oracle-engineers-its-own-infiniband-interconnects/ |dead-url=no }}</ref>[[甲骨文公司|甲骨文公司]]已经设计了自己的Infiniband交换机单元和服务器适配芯片，用于自己的产品线和第三方。Mellanox IB卡可用于[[Solaris|Solaris]]、[[Red_Hat_Enterprise_Linux|RHEL]]、{{tsl|en|SUSE_Linux_Enterprise_Server|SLES}}、[[Microsoft_Windows|Windows]]、[[HP-UX|HP-UX]]、{{tsl|en|VMware ESX}}、<ref>{{Cite web|url=http://www.mellanox.com/page/infiniband_cards_overview|title=InfiniBand Cards - Overview|accessdate=30 July 2014|publisher=Mellanox|archive-date=2020-01-11|archive-url=https://web.archive.org/web/20200111055804/http://www.mellanox.com/page/infiniband_cards_overview|dead-url=no}}</ref> [[IBM_AIX|AIX]]。<ref>{{Cite web|url=http://www.redbooks.ibm.com/redbooks/pdfs/sg247351.pdf|title=Implementing InfiniBand on IBM System p (IBM Redbook SG24-7351-00)|accessdate=2017-01-09|archive-date=2020-11-25|archive-url=https://web.archive.org/web/20201125203347/http://www.redbooks.ibm.com/redbooks/pdfs/sg247351.pdf|dead-url=no}}</ref>它被设计为[[可扩展性|可扩展]]和使用{{tsl|en|Switched fabric|交换结构}}的[[网络拓扑|网络拓扑]]。

作为互连技术，IB与[[以太网|以太网]]、[[光纤通道|光纤通道]]和其他专有技术<ref>{{Cite web|url=http://www.nytimes.com/2010/10/28/technology/28compute.html|title=China Wrests Supercomputer Title From U.S.|date=2010-10-28|last=Vance|first=Ashlee|authorlink=Ashlee Vance|work=[[New_York_Times|New York Times]]|accessdate=2017-01-09|archive-date=2021-01-27|archive-url=https://web.archive.org/web/20210127041248/https://www.nytimes.com/2010/10/28/technology/28compute.html|dead-url=no}}</ref>（例如[[克雷公司|克雷公司]]的SeaStar）竞争。该技术由[[InfiniBand贸易联盟|InfiniBand贸易联盟]]推动。

== 规格 ==

=== 性能===
{| class="wikitable sortable"
|+特征
|-
!   !! SDR !! [[双倍数据速率|DDR]] !! {{tsl|en|Quad Data Rate|四倍数据速率|QDR}} !! FDR-10 !! FDR !! EDR !! HDR !! NDR
|-
! 信令速率 (Gb/s)
| 2.5 || 5 || 10 || 10.3125 || 14.0625<ref name="fdr_fact_sheet">{{Cite web |url=https://cw.infinibandta.org/document/dl/7260 |title=存档副本 |accessdate=2017-01-09 |archive-date=2016-08-26 |archive-url=https://web.archive.org/web/20160826064526/https://cw.infinibandta.org/document/dl/7260 |dead-url=no }}</ref> || 25 || 50 || 100
|-
! 理论有效[[吞吐量|吞吐量]]，[[码率单位|Gb/s]]，每1x<ref name="ib_over">{{Cite web |url=http://www.infinibandta.org/content/pages.php?pg=technology_overview# |title=存档副本 |access-date=2017-01-09 |archive-url=https://web.archive.org/web/20110929111021/http://www.infinibandta.org/content/pages.php?pg=technology_overview |archive-date=2011-09-29 |dead-url=yes }}</ref>
| 2 || 4 || 8 || 10 || 13.64 || 24.24 ||  || 
|-
! 4x链路速度 (Gbit/s)
| 8 || 16 || 32 || 40 || 54.54 || 96.97 || ||
|-
! 12x链路速度 (Gbit/s)
| 24 || 48 || 96 || 120 || 163.64 || 290.91 || ||
|-
! 编码（[[位元|位元]]）
| [[8b/10b|8/10]]|| 8/10 || 8/10 || {{tsl|en|64b/66b encoding|64/66}} || 64/66 || 64/66  ||  ||
|-
! 延迟时间（[[微秒|微秒]]）<ref>http://www.hpcadvisorycouncil.com/events/2014/swiss-workshop/presos/Day_1/1_Mellanox.pdf {{Wayback|url=http://www.hpcadvisorycouncil.com/events/2014/swiss-workshop/presos/Day_1/1_Mellanox.pdf |date=20190819014234 }} // Mellanox</ref>
| 5 || 2.5 || 1.3 || 0.7 || 0.7  || 0.5 ||  ||
|-
!  年<ref name="ccgrid11-ib-hse-23">{{cite web|url=http://www.ics.uci.edu/~ccgrid11/files/ccgrid11-ib-hse_last.pdf#page=23|title=Network Speed Acceleration with IB and HSE|last=Panda|first=Dhabaleswar K.|author2=Sayantan Sur|date=2011|work=Designing Cloud and Grid Computing Systems  with InfiniBand and High-Speed Ethernet|publisher=CCGrid 2011|pages=23|accessdate=13 September 2014|location=Newport Beach, CA, USA|archive-date=2020-06-13|archive-url=https://web.archive.org/web/20200613041818/https://www.ics.uci.edu/~ccgrid11/files/ccgrid11-ib-hse_last.pdf#page=23|dead-url=no}}</ref>
| 2001、<br/>2003 || 2005 || 2007 || || 2011 || 2014<ref name="ib_over"/> || 2017<ref name="ib_over"/>|| 2020年后
|}

链路可以聚合：大多数系统使用一个4X聚合。12X链路通常用于[[计算机集群|计算机集群]]和[[超级计算机|超级计算机]]互连，以及用于内部[[網路交換器|網路交換器]]连接。

InfiniBand也提供[[远程直接内存访问|远程直接内存访问]]（RDMA）能力以降低CPU负载。

=== 拓扑 ===

InfiniBand使用一个{{tsl|en|switched fabric|交换结构}}拓扑，不同于早期的共享媒介[[以太网|以太网]]。所有传输开始或结束于通道适配器。每个处理器包含一个主机通道适配器（HCA），每个外设具有一个目标通道适配器（TCA）。这些适配器也可以交换安全性或[[QoS|QoS]]信息。

=== 消息===

InfiniBand以最高4 KB的[[网络封包|封包]]发送消息数据。一条消息可以为：
* 一个[[直接記憶體存取|直接記憶體存取]]的读取或写入，对于一个远程节点（RDMA）。
* 一个[[信道|信道]]发送或接收
* 一个基于事务的操作（可以逆向）
* 一个[[多播|多播]]传输。
* 一个[[原子操作|原子操作]]

=== 物理互连 ===
[[Image:Infinibandport.jpg|thumb]]

除了板式连接，它还支持有源和无源铜缆（最多30米）和[[光缆|光缆]]（最多10公里）。<ref name=faq>{{cite web|title=Specification FAQ|url=http://www.infinibandta.org/content/pages.php?pg=technology_faq|publisher=ITA|accessdate=30 July 2014|archive-url=https://web.archive.org/web/20161124000007/http://infinibandta.org/content/pages.php?pg=technology_faq|archive-date=2016-11-24|dead-url=yes}}</ref>使用{{tsl|en|QSFP}}连接器。

Inifiniband Association也指定了{{tsl|en|CXP (connector)|CXP (连接器)|CXP}}铜连接器系统，用于通过铜缆或有源光缆达到高达120 Gbit/s的能力。

=== API ===
InfiniBand没有标准的[[应用程序接口|应用程序接口]]。標準只列出一組的''動作''例如 <code>ibv_open_device</code> 或是 <code>ibv_post_send</code>，這些都是必須存在的[[子程序|子程序]]或[[方法_(電腦科學)|方法]]的抽象表示方式。這些[[子程序|子程序]]的語法由供應商自行定義。[[De_facto|事實標準]]的軟體堆疊標準是由 {{tsl|en|OpenFabrics Alliance}} 所開發的。它以[[多許可|雙許可證]]方式發佈，[[GNU通用公共许可证|GNU通用公共许可证]]或[[BSD许可证|BSD许可证]]用於 [[GNU|GNU]]/[[Linux|Linux]] 以及 [[FreeBSD|FreeBSD]]，且 WinOF 在 Windows 下可以選擇 [[BSD许可证|BSD许可证]]。它已被大多數 InfiniBand 供應商採用，用於 [[GNU|GNU]]/[[Linux|Linux]]、[[FreeBSD|FreeBSD]] 以及 [[Microsoft_Windows|Windows]]。

== 历史 ==

InfiniBand源于1999年两个竞争设计的合并：未来I/O与下一代I/O。这促成了InfiniBand贸易联盟（InfiniBand Trade Association，缩写IBTA），其中包括[[康柏|康柏]]、[[戴爾|戴爾]]、[[惠普|惠普]]、[[IBM|IBM]]、[[英特尔|英特尔]]、[[微软|微软]]及[[昇陽|昇陽]]。当时有人认为一些更强大的电脑正在接近[[外设组件互连标准|PCI]]总线的{{tsl|en|interconnect bottleneck|互连瓶颈}}，尽管有像[[PCI-X|PCI-X]]的升级。<ref name=pentakalos>{{cite web|last1=Pentakalos|first1=Odysseas|title=An Introduction to the InfiniBand Architecture|url=http://www.oreillynet.com/pub/a/network/2002/02/04/windows.html|website=O'Reilly|accessdate=28 July 2014|archive-date=2014-08-09|archive-url=https://web.archive.org/web/20140809135450/http://www.oreillynet.com/pub/a/network/2002/02/04/windows.html|dead-url=no}}</ref>InfiniBand架构规范的1.0版本发布于2000年。最初，IBTA的IB愿景是取代PCI的I/O，以太网的[[数据中心|机房]]、[[计算机集群|计算机集群]]的互连以及[[光纤通道|光纤通道]]。IBTA也设想在IB{{tsl|en|Fabric computing||结构}}上分担服务器硬件。随着[[互聯網泡沫|互聯網泡沫]]的爆发，业界对投资这样一个意义深远的技术跳跃表现为犹豫不决。<ref name=kim>{{cite web|last1=Kim|first1=Ted|title=Brief History of InfiniBand: Hype to Pragmatism|url=https://blogs.oracle.com/RandomDude/entry/history_hype_to_pragmatism|publisher=Oracle|accessdate=28 July 2014|deadurl=yes|archiveurl=https://web.archive.org/web/20140808200954/https://blogs.oracle.com/RandomDude/entry/history_hype_to_pragmatism|archivedate=2014年8月8日}}</ref>

=== 时间线 ===

* 2001年：{{tsl|en|Mellanox}}售出InfiniBridge 10Gbit/s设备和超过10,000个InfiniBand端口。<ref name=timeline>{{cite web|title=Timeline|url=http://www.mellanox.com/page/timeline|publisher=Mellanox Technologies|accessdate=28 July 2014|archive-date=2019-11-29|archive-url=https://web.archive.org/web/20191129100751/https://www.mellanox.com/page/timeline|dead-url=no}}</ref>
* 2002年：英特尔宣布将着眼于开发[[PCI_Express|PCI Express]]而不是采用IB芯片，以及微软停止IB开发以利于扩展以太网，IB发展受挫，尽管Sun和[[日立製作所|日立]]继续支持IB。<ref>{{cite web|title=Sun confirms commitment to InfiniBand|url=http://www.theregister.co.uk/2002/12/30/sun_confirms_commitment_to_infiniband/|website=The Register|accessdate=28 July 2014|archive-date=2019-06-20|archive-url=https://web.archive.org/web/20190620005946/https://www.theregister.co.uk/2002/12/30/sun_confirms_commitment_to_infiniband/|dead-url=no}}</ref>
* 2003年：[[弗吉尼亚理工学院暨州立大学|弗吉尼亚理工学院暨州立大学]]建立了一个InfiniBand集群，在当时的[[TOP500|TOP500]]排名第三。
* 2004年：IB开始作为集群互连采用，对抗以太网上的延迟和价格。<ref name=kim/>{{tsl|en|OpenFabrics Alliance}}开发了一个标准化的基于Linux的InfiniBand软件栈。次年，Linux添加IB支持。<ref>{{cite web|title=Linux Kernel 2.6.11 Supports InfiniBand|url=http://www.internetnews.com/dev-news/article.php/3485401|accessdate=29 July 2014|archive-date=2020-10-21|archive-url=https://web.archive.org/web/20201021022937/http://www.internetnews.com/dev-news/article.php/3485401|dead-url=no}}</ref>
* 2005年：IB开始被实现为存储设备的互连。<ref>{{Citation | url = http://www.infostor.com/index/articles/display/248655/articles/infostor/volume-10/issue-2/news-analysis-trends/news-analysis-trends/is-infiniband-poised-for-a-comeback.html | title = Is InfiniBand poised for a comeback? | journal = Infostor | volume = 10 | issue = 2 | accessdate = 2017-01-09 | archive-date = 2020-11-25 | archive-url = https://web.archive.org/web/20201125203817/http://www.infostor.com/index/articles/display/248655/articles/infostor/volume-10/issue-2/news-analysis-trends/news-analysis-trends/is-infiniband-poised-for-a-comeback.html | dead-url = no }}</ref>
* 2009年：世界500强超级计算机中，259个使用千兆以太网作为内部互连技术，181个使用InfiniBand。<ref>{{cite web|last1=Lawson|first1=Stephen|title=Two rival supercomputers duke it out for top spot|url=http://news.idg.no/cw/art.cfm?id=FB70C2C5-1A64-6A71-CEEA6C17D51B1E3C|publisher=COMPUTERWORLD|accessdate=29 July 2014|archive-url=https://web.archive.org/web/20170110015721/http://news.idg.no/cw/art.cfm?id=FB70C2C5-1A64-6A71-CEEA6C17D51B1E3C|archive-date=2017-01-10|dead-url=yes}}</ref>
* 2010年：市场领导者Mellanox和Voltaire合并，IB供应商只剩下另一个竞争者——{{tsl|en|QLogic}}，它主要是光纤通道供应商。<ref>{{cite web|last1=Raffo|first1=Dave|title=Largest InfiniBand vendors merge; eye converged networks|url=http://itknowledgeexchange.techtarget.com/storage-soup/largest-infiniband-vendors-merge-eye-converged-networks/|accessdate=29 July 2014|archive-url=https://web.archive.org/web/20170701002647/http://itknowledgeexchange.techtarget.com/storage-soup/largest-infiniband-vendors-merge-eye-converged-networks/|archive-date=2017-07-01|dead-url=yes}}</ref> Oracle makes a major investment in Mellanox.
* 2011年：FDR交换机和适配器在{{tsl|en|International Supercomputing Conference|国际超级计算会议}}上宣布。<ref>{{cite news |url= http://www.cio.com/article/684732/Mellanox_Demos_Souped_Up_Version_of_Infiniband |title= Mellanox Demos Souped-Up Version of InfiniBand |publisher= CIO |date= 20 June 2011 |accessdate= 1 August 2011 |archive-date= 2014-01-07 |archive-url= https://web.archive.org/web/20140107213547/http://www.cio.com/article/684732/Mellanox_Demos_Souped_Up_Version_of_Infiniband |dead-url= no }}</ref>
* 2012年：英特尔收购QLogic的InfiniBand技术。<ref>{{cite news | url = http://www.hpcwire.com/hpcwire/2012-01-23/intel_snaps_up_infiniband_technology,_product_line_from_qlogic.html | title = Intel Snaps Up InfiniBand Technology, Product Line from QLogic | work = HPCwire | date = January 23, 2012 | accessdate = 2012-01-27 | deadurl = yes | archiveurl = https://web.archive.org/web/20120127034802/http://www.hpcwire.com/hpcwire/2012-01-23/intel_snaps_up_infiniband_technology%2C_product_line_from_qlogic.html | archivedate = 2012-01-27 }}</ref>
* 2016年：[[甲骨文公司|甲骨文公司]]制造自己的InfiniBand互连芯片和交换单元。<ref>{{Cite web|title = Oracle Engineers Its Own InfiniBand Interconnects|url = http://www.nextplatform.com/2016/02/22/oracle-engineers-its-own-infiniband-interconnects/|website = The Next Platform|access-date = 2016-02-24|archive-date = 2020-11-29|archive-url = https://web.archive.org/web/20201129104949/https://www.nextplatform.com/2016/02/22/oracle-engineers-its-own-infiniband-interconnects/|dead-url = no}}</ref>
* 2019年：[[Nvidia|Nvidia]]以69亿美元收购{{tsl|en|Mellanox}}。<ref>{{cite web |title=NVIDIA to Acquire Mellanox for $6.9 Billion |url=https://nvidianews.nvidia.com/news/nvidia-to-acquire-mellanox-for-6-9-billion |website=NVIDIA News |accessdate=2020-02-12 |archiveurl=https://web.archive.org/web/20190311111554/https://nvidianews.nvidia.com/news/nvidia-to-acquire-mellanox-for-6-9-billion |archivedate=2019-03-11 |date=2019-03-11 |dead-url=no }}</ref>

== 参见 ==
* {{tsl|en|SCSI RDMA Protocol|SCSI RDMA协议}}
* {{tsl|en|iSCSI Extensions for RDMA}}
* {{tsl|en|iWARP}}
* [[電腦裝置頻寬列表|電腦裝置頻寬列表]]
* [[光互連|光互連]]
* [[光通訊|光通訊]]
* {{tsl|en|Parallel optical interface|并行光接口}}
* [[100吉比特以太网|40/100吉比特以太网]]

== 参考资料 ==
{{reflist|30em}}

== 外部链接 ==
* {{Citation|title=Dissecting a Small InfiniBand Application Using the Verbs API|arxiv=1105.1827}}
* [https://web.archive.org/web/20170110015701/http://www.mellanox.com/page/performance_infiniband Mellanox Technologies: InfiniBand Performance Metrics]

{{总线}}
[[Category:计算机总线|Category:计算机总线]]
[[Category:電腦網路|Category:電腦網路]]
[[Category:串行总线|Category:串行总线]]
[[Category:超級電腦|Category:超級電腦]]