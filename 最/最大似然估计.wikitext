{{noteTA|
|T=zh-cn: 最大似然估计;zh-tw:最大概似估計;
|G1=Math
|1= zh-cn: 似然;zh-tw:概似
|2= zh-cn: 矩;zh-tw:動差;zh-hant: 矩;
|3= zh-cn: 参数;zh-tw:母數;zh-hant:參數
}}

在[[统计学|统计学]]中，'''最大似然估计'''（{{lang-en|'''M'''aximum '''L'''ikelihood '''E'''stimation}}，簡作'''MLE'''），也称'''极大似然估计'''，是用来[[估計|估計]]一个[[概率模型|概率模型]]的参数的一种方法。

== 预备知识 ==

下边的讨论要求读者熟悉[[概率论|概率论]]中的基本定义，如[[概率分布|概率分布]]、[[概率密度函数|概率密度函数]]、[[随机变量|随机变量]]、[[数学期望|数学期望]]等。读者還須先熟悉[[连续|连续]][[实函数|实函数]]的基本技巧，比如使用[[微分|微分]]来求一个函数的[[极值|极值]]（即[[极大值|极大值]]或[[极小值|极小值]]）。<br />
同時，讀者須先擁有[[似然函數|似然函數]]的背景知識，以了解最大似然估計的出發點及應用目的。

== 最大似然估计的原理 ==

给定一个概率分布<math>D</math>，已知其[[概率密度函数|概率密度函数]]（连续分布）或[[概率质量函数|概率质量函数]]（离散分佈）为<math>f_D</math>，以及一个分佈参数<math>\theta</math>，我们可以从这个分布中抽出一个具有<math>n</math>个值的采样<math>X_1, X_2,\ldots, X_n</math>，利用<math>f_D</math>计算出其[[似然函数|似然函数]]：

:<math>\mbox{L}(\theta\mid x_1,\dots,x_n) = f_\theta(x_1,\dots,x_n).</math>

若<math>D</math>是离散分布，<math>f_\theta</math>即是在参数为<math>\theta</math>时观测到这一采样的概率。若其是连续分布，<math>f_\theta</math>则为<math>X_1, X_2,\ldots, X_n</math>联合分布的概率密度函数在观测值处的取值。一旦我们获得<math>X_1, X_2,\ldots, X_n</math>，我们就能求得一个关于<math>\theta</math>的估计。最大似然估计会寻找关于<math>\theta</math>的最可能的值（即，在所有可能的<math>\theta</math>取值中，寻找一个值使这个采样的“可能性”最大化）。从数学上来说，我们可以在<math>\theta</math>的所有可能取值中寻找一个值使得似然[[函数|函数]]取到最大值。这个使可能性最大的<math>\widehat{\theta}</math>值即称为<math>\theta</math>的'''最大似然估计'''。由定义，最大似然估计是样本的函数。

=== 注意 ===
* 这裡的[[似然函数|似然函数]]是指<math>x_1,x_2,\ldots,x_n</math>不变时，关于<math>\theta</math>的一个函数。
* 最大似然估计不一定存在，也不一定唯一。

== 例子 ==

=== 离散分布，离散有限参数空间 ===

考虑一个抛硬币的例子。假设这个硬币正面跟反面轻重不同。我们把这个硬币抛80次（即，我们获取一个采样<math>x_1=\mbox{H}, x_2=\mbox{T}, \ldots, x_{80}=\mbox{T}</math>并把正面的次数记下来，正面记为H，反面记为T）。并把抛出一个正面的概率记为<math>p</math>，抛出一个反面的概率记为<math>1-p</math>（因此，这裡的<math>p</math>即相当于上边的<math>\theta</math>）。假设我们抛出了49个正面，31个反面，即49次H，31次T。假设这个硬币是我们从一个装了三个硬币的盒子里头取出的。这三个硬币抛出正面的概率分别为<math>p=1/3</math>, <math>p=1/2</math>, <math>p=2/3</math>.这些硬币没有标记，所以我们无法知道哪个是哪个。使用'''最大似然估计'''，基于'''二项分布'''中的'''概率质量函数'''公式，通过这些试验数据（即采样数据），我们可以计算出哪个硬币的可能性最大。这个似然函数取以下三个值中的一个：

::<math>\begin{matrix}
\mathbb{L}(p=1/3 \mid \mbox{H=49, T=31 }) & = & \mathbb{P}(\mbox{H=49, T=31 }\mid p=1/3) & = & {80\choose 49}(1/3)^{49}(1-1/3)^{31} \approx 0.000 \\
&&\\
\mathbb{L}(p=1/2 \mid \mbox{H=49, T=31 }) & = & \mathbb{P}(\mbox{H=49, T=31 }\mid p=1/2) & = & {80\choose 49}(1/2)^{49}(1-1/2)^{31} \approx 0.012 \\
&&\\
\mathbb{L}(p=2/3 \mid \mbox{H=49, T=31 }) & = & \mathbb{P}(\mbox{H=49, T=31 }\mid p=2/3) & = & {80\choose 49}(2/3)^{49}(1-2/3)^{31} \approx 0.054 \\
\end{matrix}</math>

我们可以看到当<math>\widehat{p}=2/3</math>时，似然函数取得最大值。<br/>
顯然地，這硬幣的公平性和那種拋出後正面的機率是2/3的硬幣是最接近的。这就是<math>p</math>的最大似然估计。

=== 离散分布，连续参数空间 ===

现在假设例子1中的盒子中有无数个硬币，对于<math>0\leq p \leq 1</math>中的任何一个<math>p</math>， 都有一个抛出正面概率为<math>p</math>的硬币对应，我们来求其似然函数的最大值：

:<math>\begin{matrix}
\mbox{L}(\theta) & = & f_D(\mbox{H=49,T=80-49}\mid p) = {80\choose 49} p^{49}(1-p)^{31} \\
\end{matrix}</math>

其中<math>0\leq p \leq 1</math>.
我们可以使用[[微分法|微分法]]来求[[極值|極值]]。方程两边同时对<math>p</math>取[[微分|微分]]，并使其为零。

:<math>\begin{matrix}
0 & = &  {80\choose 49}\frac{d}{dp} \left(  p^{49}(1-p)^{31} \right) \\
  &   & \\
  & \propto & 49p^{48}(1-p)^{31} - 31p^{49}(1-p)^{30} \\
  &   & \\
  & = & p^{48}(1-p)^{30}\left[ 49(1-p) - 31p \right] \\
\end{matrix}</math>

[[File:BinominalLikelihoodGraph.png|thumb]]并在曲线的最大值处。]]
其解为<math>p=0</math>, <math>p=1</math>，以及<math>p=49/80</math>.使可能性最大的解显然是<math>p=49/80</math>（因为<math>p=0</math>和<math>p=1</math>这两个解会使可能性为零）。因此我们说'''最大似然估计值'''为<math>\widehat{p}=49/80</math>.

这个结果很容易一般化。只需要用一个字母<math>t</math>代替49用以表达[[伯努利试验|伯努利试验]]中的被观察数据（即样本）的“成功”次数，用另一个字母<math>n</math>代表伯努利试验的次数即可。使用完全同样的方法即可以得到'''最大似然估计值''':

:<math>\widehat{p}=\frac{t}{n}</math>

对于任何成功次数为<math>t</math>，试验总数为<math>n</math>的伯努利试验。

=== 连续分布，连续参数空间 ===
最常见的[[连续概率分布|连续概率分布]]是[[正态分布|正态分布]]，其概率密度函数如下：
:<math>f(x\mid \mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}</math>

现在有<math>n</math>个正态随机变量的采样点，要求的是一个这样的正态分布，这些采样点分布到这个正态分布可能性最大（也就是概率密度积最大，每个点更靠近中心点），其<math>n</math>个正态随机变量的采样的对应密度函数（假设其独立并服从同一分布）为：

:<math>f(x_1,\ldots,x_n \mid \mu,\sigma^2) = \left( \frac{1}{2\pi\sigma^2} \right)^\frac{n}{2} e^{-\frac{ \sum_{i=1}^{n}(x_i-\mu)^2}{2\sigma^2}}</math>

或：

:<math>f(x_1,\ldots,x_n \mid \mu,\sigma^2) = \left( \frac{1}{2\pi\sigma^2} \right)^{n/2} \exp\left(-\frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2}\right)</math>,

这个分布有两个参数：<math>\mu,\sigma^2</math>.有人可能会担心两个参数与上边的讨论的例子不同，上边的例子都只是在一个参数上对可能性进行最大化。实际上，在两个参数上的求最大值的方法也差不多：只需要分别把可能性<math>\mbox{L}(\mu,\sigma) = f(x_1,,\ldots,x_n \mid \mu, \sigma^2)</math>在两个参数上最大化即可。当然这比一个参数麻烦一些，但是一点也不复杂。使用上边例子同样的符号，我们有<math>\theta=(\mu,\sigma^2)</math>.

最大化一个似然函数同最大化它的[[自然对数|自然对数]]是等价的。因为[[自然对数|自然对数]]log是一个[[连续|连续]]且在似然函数的[[值域|值域]]内[[严格递增|严格递增]]的上凹函数。[注意：可能性函数（似然函数）的自然对数跟[[信息熵|信息熵]]以及[[Fisher信息|Fisher信息]]联系紧密。]求对数通常能够一定程度上简化运算，比如在这个例子中可以看到：

:<math>\begin{matrix}
0 & = & \frac{\partial}{\partial \mu} \log \left( \left( \frac{1}{2\pi\sigma^2} \right)^\frac{n}{2} e^{-\frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2}} \right) \\
  & = & \frac{\partial}{\partial \mu} \left( \log\left( \frac{1}{2\pi\sigma^2} \right)^\frac{n}{2} - \frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2} \right) \\
  & = & 0 - \frac{-2n(\bar{x}-\mu)}{2\sigma^2} \\
\end{matrix}</math>

这个方程的解是<math>\widehat{\mu} = \bar{x} = \sum^{n}_{i=1}x_i/n </math>.这的确是这个函数的最大值，因为它是<math>\mu</math>里头惟一的一阶导数等于零的点并且二阶导数严格小于零。

同理，我们对<math>\sigma</math>求导，并使其为零。

:<math>\begin{matrix}
0 & = & \frac{\partial}{\partial \sigma} \log \left( \left( \frac{1}{2\pi\sigma^2} \right)^\frac{n}{2} e^{-\frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2}} \right) \\
  & = & \frac{\partial}{\partial \sigma} \left( \frac{n}{2}\log\left( \frac{1}{2\pi\sigma^2} \right) - \frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{2\sigma^2} \right) \\
  & = & -\frac{n}{\sigma} + \frac{ \sum_{i=1}^{n}(x_i-\bar{x})^2+n(\bar{x}-\mu)^2}{\sigma^3}
\\
\end{matrix}</math>

这个方程的解是<math>\widehat{\sigma}^2 = \sum_{i=1}^n(x_i-\widehat{\mu})^2/n</math>.

因此，其关于<math>\theta=(\mu,\sigma^2)</math>的''最大似然估计''为：

:<math>\widehat{\theta}=(\widehat{\mu},\widehat{\sigma}^2) = (\bar{x},\sum_{i=1}^n(x_i-\bar{x})^2/n)</math>.

== 性质 ==

=== 泛函不变性（Functional invariance） ===
如果<math>\hat{\theta}</math>是<math>\theta</math>的一个最大似然估计，那么<math>\alpha = g(\theta)</math>的最大似然估计是<math>\hat{\alpha} = g(\hat{\theta})</math>。函数''g''无需是一个[[双射|双射]]。<ref>请参见George Casella与Roger L. Berger所著的''Statistical Inference''定理Theorem 7.2.10的证明。（中国大陆出版的大部分教材上也可以找到这个证明。）</ref>

=== 渐近线行为 ===
最大似然估计函数在采样样本总数趋于无穷的时候达到最小[[方差|方差]]（其证明可见于[[Cramer-Rao_lower_bound|Cramer-Rao lower bound]]）。当最大似然估计非偏时，等价的，在极限的情况下我们可以称其有最小的[[均方差|均方差]]。
对于独立的观察来说，最大似然估计函数经常趋于[[正态分布|正态分布]]。

=== 偏差 ===
最大似然估计的[[非偏估计|偏差]]是非常重要的。考虑这样一个例子，标有''1''到''n''的''n''张票放在一个盒子中。从盒子中随机抽取票。如果''n''是未知的话，那么''n''的最大似然估计值就是抽出的票上标有的''n''，尽管其期望值的只有<math>(n+1)/2</math>.为了估计出最高的''n''值，我们能确定的只能是''n''值不小于抽出来的票上的值。

== 历史 ==
最大似然估计最早是由[[羅納德·費雪|羅納德·費雪]]在1912年至1922年间推荐、分析并大范围推广的。<ref name="Pfanzagl">{{harvtxt |Pfanzagl |1994 }}</ref>（虽然以前[[高斯|高斯]]、[[拉普拉斯|拉普拉斯]]、[[托瓦爾·尼古拉·蒂勒|托瓦爾·尼古拉·蒂勒]]和[[弗朗西斯·伊西德罗·埃奇沃思|F. Y. 埃奇沃思]]也使用过）。<ref>{{harvtxt |Edgeworth |September 1908 }} and {{harvtxt |Edgeworth |December 1908 }}</ref> 许多作者都提供了最大似然估计发展的回顾。<ref>{{harvtxt |Savage |1976 }}, {{harvtxt |Pratt |1976 }}, {{harvs |txt=yes |last=Stigler |year=1978 |year2=1986 |year3=1999 }}, {{harvs |txt=yes |last=Hald |year=1998 |year2=1999 }}, and {{harvtxt |Aldrich |1997 }}</ref>

大部分的最大似然估计理论都在[[贝叶斯统计|贝叶斯统计]]中第一次得到发展，并被后来的作者简化。<ref name="Pfanzagl" />

== 参见 ==
* [[均方差|均方差]]是衡量一个[[估计函数|估计函数]]的好坏的一个量。

* 关于[[Rao-Blackwell定理|Rao-Blackwell定理]]（Rao-Blackwell theorem）的文章中讨论到如何利用Rao-Blackwellisation过程寻找最佳不偏估计（即使均方差最小）的方法。而最大似然估计通常是一个好的起点。

* 读者可能会对最大似然估计（如果存在）总是一个关于参数的[[充分统计|充分统计]]（sufficient statistic）的函数感兴趣。

* 最大似然估计跟[[一般化矩方法|一般化矩方法]]（generalized method of moments）有关。

== 参考文献 ==
{{Reflist}}

== 外部链接 ==
* [https://web.archive.org/web/20070223105625/http://projecteuclid.org/Dienst/UI/1.0/Summarize/euclid.ss/1030037906 关于最大似然估计的历史的一篇论文，作者John Aldrich]

{{-}}
{{统计学}}
{{系统发生学}}

[[Category:估计理论|Category:估计理论]]
[[Category:统计学|Category:统计学]]