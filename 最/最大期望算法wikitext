{{机器学习导航栏}}
'''最大期望演算法'''（'''Expectation-maximization algorithm'''，又譯'''期望最大化算法'''）在统计中被用于寻找，依赖于不可观察的隐性变量的概率模型中，参数的最大似然估计。

在[[统计|统计]][[计算|计算]]中，'''最大期望（EM）算法'''是在[[概率模型|概率模型]]中寻找[[参数|参数]][[最大似然估计|最大似然估计]]或者[[最大后验概率|最大后验估计]]的[[算法|算法]]，其中概率模型依赖于无法观测的[[隐变量|隐变量]]。最大期望算法经常用在[[机器学习|机器学习]]和[[计算机视觉|计算机视觉]]的[[数据聚类|数据聚类]]（Data Clustering）领域。最大期望算法经过两个步骤交替进行计算，第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；第二步是最大化（M），最大化在E步上求得的[[最大似然估计|最大似然值]]来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。

==历史==
最大期望值算法由{{tsl|en|Arthur P. Dempster}}，{{tsl|en|Nan Laird}}和{{tsl|en|Donald Rubin}}在他们1977年发表的经典论文中提出。他们指出此方法之前其实已经被很多作者「在他们特定的研究领域中多次提出过」。

==介绍==
EM算法用于在方程不能直接求解的情况下寻找统计模型的(局部)最大似然参数。这些模型中较为典型的是含有潜变量，未知参数并且已知观测数据的模型。也就是说，要么数据中存在缺失的值，要么模型可以通过假设存在更多未观测到的数据点来更简单地表示。以混合模型（Mixture Model）为例，通过假设每个观察到的数据点都有一个对应的未观察到的数据点，也可以说是潜在变量，来指定每个数据点所属的混合部分，这样就可以更简单地描述混合模型。

==EM简单教程==
EM是一个在已知部分相关变量的情况下，估计未知变量的迭代技术。EM的算法流程如下：
#初始化分布参数
#重复直到收敛：
##E步骤：根据参数的假设值，给出未知变量的期望估计，应用于缺失值。
##M步骤：根据未知变量的估计值，给出当前的参数的极大似然估计。

==最大期望过程说明==
我们用<math>\textbf{y}</math>表示能够观察到的不完整的变量值，用<math>\textbf{x}</math>表示无法观察到的变量值，这样<math>\textbf{x}</math>和<math>\textbf{y}</math>一起组成了完整的数据。<math>\textbf{x}</math>可能是实际测量丢失的数据，也可能是能够简化问题的隐藏变量，如果它的值能够知道的话。例如，在[[混合模型|混合模型]]中，如果“产生”样本的混合元素成分已知的话最大似然公式将变得更加便利（参见下面的例子）。

===估计无法观测的数据===
让<math>p\,</math>代表矢量<math>\theta</math>: <math>p( \mathbf y, \mathbf x | \theta)</math>定义的参数的全部数据的[[機率密度函數|機率密度函數]]（连续情况下）或者[[機率質量函數|機率質量函數]]（离散情况下），那么从这个函数就可以得到全部数据的[[最大似然估计|最大似然值]]，另外，在给定的观察到的数据条件下未知数据的[[条件分布|条件分布]]可以表示为：

:<math>p(\mathbf x |\mathbf y, \theta) = \frac{p(\mathbf y, \mathbf x | \theta)}{p(\mathbf y | \theta)} = \frac{p(\mathbf y|\mathbf x, \theta)p(\mathbf x |\theta)}{\int p(\mathbf y|\mathbf x, \theta) p(\mathbf x |\theta) d\mathbf x}</math>

==参见==
*[[估计理论|估计理论]]
*[[数据聚类|数据聚类]]

==参考文献==
{{refbegin}}
*Arthur Dempster, Nan Laird, and Donald Rubin. "Maximum likelihood from incomplete data via the EM algorithm". ''Journal of the Royal Statistical Society'', Series B, 39 (1):1–38, 1977 [http://links.jstor.org/sici?sici=0035-9246%281977%2939%3A1%3C1%3AMLFIDV%3E2.0.CO%3B2-Z].
*Robert Hogg, Joseph McKean and Allen Craig. ''Introduction to Mathematical Statistics''. pp. 359-364.  Upper Saddle River, NJ: Pearson Prentice Hall, 2005.
*Radford Neal, [[Geoffrey_Hinton|Geoffrey Hinton]]. "A view of the EM algorithm that justifies incremental, sparse, and other variants". In Michael I. Jordan (editor), ''Learning in Graphical Models'' pp 355-368. Cambridge, MA: MIT Press, 1999.
*[http://www.inference.phy.cam.ac.uk/mackay/itila/ The on-line textbook: Information Theory, Inference, and Learning Algorithms] {{Wayback|url=http://www.inference.phy.cam.ac.uk/mackay/itila/ |date=20150206001819 }}，by [[David_J.C._MacKay|David J.C. MacKay]] includes simple examples of the E-M algorithm such as clustering using the soft K-means algorithm, and emphasizes the variational view of the E-M algorithm.
*[http://citeseer.ist.psu.edu/bilmes98gentle.html A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models] {{Wayback|url=http://citeseer.ist.psu.edu/bilmes98gentle.html |date=20080311190023 }}，by [[J._Bilmes|J. Bilmes]] includes a simplified derivation of the EM equations for Gaussian Mixtures and Gaussian Mixture Hidden Markov Models.
*[http://citeseer.ist.psu.edu/amari95information.html Information Geometry of the EM and em Algorithms for Neural Networks] {{Wayback|url=http://citeseer.ist.psu.edu/amari95information.html |date=20080616055101 }}，by [[Shun-Ichi_Amari|Shun-Ichi Amari]] give a view of EM algorithm from geometry view point.
{{refend}}

[[Category:估计理论|Category:估计理论]]
[[Category:算法|Category:算法]]
[[Category:機器學習演算法|Category:機器學習演算法]]