{{expert|subject=数学|subject2=计算机科学|subject3=电子学}}
{{Expand language|en|time=2019-10-28T12:40:10+00:00}}
{{NoteTA
|G1 = Communication
|G2 = IT
|G3 = Math
}}
{{Distinguish2|'''[[信息学|{{Forceconvert]]'''}}
{{电脑和信息技术}}

'''信息论'''（{{lang-en|information theory}}）是[[应用数学|应用数学]]、[[電子學|電子學]]和[[计算机科学|计算机科学]]的一个分支，涉及[[信息|信息]]的量化、存储和通信等。信息论是由[[克劳德·香农|克劳德·香农]]发展，用来找出[[信号处理|信号处理]]与[[通信|通信]]操作的基本限制，如[[数据压缩|数据压缩]]、可靠的存储和数据[[电信|传输]]等。自创立以来，它已拓展应用到许多其他领域，包括统计推断、[[自然语言处理|自然语言处理]]、[[密码学|密码学]]、[[神经生物学|神经生物学]]<ref>{{cite book|author=F. Rieke, D. Warland, R Ruyter van Steveninck, W Bialek|title=Spikes: Exploring the Neural Code|publisher=The MIT press|year=1997|isbn=978-0262681087}}</ref>、进化论<ref>cf. Huelsenbeck, J. P., F. Ronquist, R. Nielsen and J. P. Bollback (2001) Bayesian inference of phylogeny and its impact on evolutionary biology, ''Science'' '''294''':2310-2314</ref>和分子编码的功能<ref>Rando Allikmets, Wyeth W. Wasserman, Amy Hutchinson, Philip Smallwood, Jeremy Nathans, Peter K. Rogan, [http://alum.mit.edu/www/toms/ Thomas D. Schneider] {{Wayback|url=http://alum.mit.edu/www/toms/ |date=20080821124201 }}, Michael Dean (1998) Organization of the ABCR gene: analysis of promoter and splice junction sequences, ''Gene'' '''215''':1, 111-122</ref>、[[生态学|生态学]]的模式选择<ref>Burnham, K. P. and Anderson D. R. (2002) ''Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach, Second Edition'' (Springer Science, New York) ISBN 978-0-387-95364-9.</ref>、热物理<ref>Jaynes, E. T. (1957) [http://bayes.wustl.edu/ Information Theory and Statistical Mechanics] {{Wayback|url=http://bayes.wustl.edu/ |date=20110830215943 }}, ''Phys. Rev.'' '''106''':620</ref>、[[量子计算|量子计算]]、[[语言学|语言学]]、剽窃检测<ref>Charles H. Bennett, Ming Li, and Bin Ma (2003) [http://sciamdigital.com/index.cfm?fa=Products.ViewIssuePreview&ARTICLEID_CHAR=08B64096-0772-4904-9D48227D5C9FAC75 Chain Letters and Evolutionary Histories] {{Wayback|url=http://sciamdigital.com/index.cfm?fa=Products.ViewIssuePreview&ARTICLEID_CHAR=08B64096-0772-4904-9D48227D5C9FAC75 |date=20071007041539 }}, ''Scientific American'' '''288''':6, 76-81</ref>、[[模式识别|模式识别]]、[[异常检测|异常检测]]和其他形式的[[数据分析|数据分析]]。<ref>{{Cite web
 |author      = David R. Anderson
 |title       = Some background on why people in the empirical sciences may want to better understand the information-theoretic methods
 |date        = November 1, 2003
 |url         = http://aicanderson2.home.comcast.net/~aicanderson2/home.pdf
 |format      = pdf
 |accessdate  = 2010-06-23
 |deadurl     = yes
 |archiveurl  = https://web.archive.org/web/20110723045720/http://aicanderson2.home.comcast.net/~aicanderson2/home.pdf
 |archivedate = 2011-07-23
}}</ref>

[[熵_(信息论)|熵]]是信息的一个关键度量，通常用一条消息中需要存储或传输一个{{le|符号率|Symbol rate|符号}}的平均比特数来表示。熵衡量了预测[[随机变量|随机变量]]的值时涉及到的不确定度的量。例如，指定[[擲硬幣|擲硬幣]]的结果（两个等可能的结果）比指定掷骰子的结果（六个等可能的结果）所提供的信息量更少（熵更少）。

信息论将信息的传递作为一种统计现象来考虑，给出了估算通信信道容量的方法。信息传输和信息压缩是信息论研究中的两大领域。这两个方面又由[[有噪信道编码定理|信道编码定理]]、[[信源－信道隔离定理|信源－信道隔离定理]]相互联系。

信息论的基本内容的应用包括[[无损数据压缩|无损数据压缩]]（如[[ZIP格式|ZIP文件]]）、[[有损数据压缩|有损数据压缩]]（如[[MP3|MP3]]和[[JPEG|JPEG]]）、[[信道容量|信道编码]]（如[[数字用户线路|数字用户线路]]（{{lang|en|DSL}}））。这个领域处在[[数学|数学]]、[[统计学|统计学]]、[[计算机科学|计算机科学]]、[[物理学|物理学]]、[[神经科学|神经科学]]和[[電機工程學|電機工程學]]的交叉点上。信息论对[[航海家計畫|航海家]]深空探测任务的成败、[[光盘|光盘]]的发明、[[手机|手机]]的可行性、[[互联网|互联网]]的发展、[[语言学|语言学]]和人类感知的研究、对[[黑洞|黑洞]]的了解，以及许多其他领域都影响深远。信息论的重要子领域有[[数据压缩|信源编码]]、[[前向錯誤更正|信道编码]]、[[柯氏复杂性|算法复杂性理论]]、[[算法信息论|算法信息论]]、[[資訊理論安全性|資訊理論安全性]]和[[信息度量|信息度量]]等。

== 简述 ==
信息论的主要内容可以类比人类最广泛的交流手段——语言来阐述。

一种简洁的语言（以[[英语|英语]]为例）通常有两个重要特点：
首先，最常用的词（比如"a"、"the"、"I"）应该比不太常用的词（比如"benefit"、"generation"、"mediocre"）要短一些；其次，如果句子的某一部分被漏听或者由于[[噪声|噪声]]干扰（比如一辆车辆疾驰而过）而被误听，听者应该仍然可以抓住句子的大概意思。而如果把电子[[通信系统|通信系统]]比作一种语言的话，这种[[健壮性_(计算机科学)|健壮性]]（{{lang|en|robustness}}）是不可或缺的。将健壮性引入通信是通过[[信道编码|信道编码]]完成的。[[信源编码|信源编码]]和信道编码是信息论的基本研究课题。

注意这些内容同消息的重要性之间是毫不相干的。例如，像“多谢；常来”这样的客套话與像“救命”这样的紧急请求在说起来、或者写起来所花的时间是差不多的，然而明显后者更重要，也更有实在意义。信息论却不考虑一段消息的重要性或内在意义，因为这些是数据的质量的问题而不是数据量（数据的长度）和可读性方面上的问题，后者只是由[[概率|概率]]这一因素单独决定的。

== 信息的度量 ==
===信息熵===
美國數學家[[克劳德·香农|克劳德·香农]]被称为“信息论之父”。人们通常将香农于1948年10月发表于《{{tsl|en|Bell System Technical Journal|贝尔系统技术学报}}》上的论文《{{tsl|en|A Mathematical Theory of Communication|通信的数学理论}}》作为现代信息论研究的开端。这一文章部分基于[[哈里·奈奎斯特|哈里·奈奎斯特]]和{{tsl|en|Ralph Hartley|拉尔夫·哈特利}}於1920年代先後發表的研究成果。在该文中，香农给出了[[信息熵|信息熵]]的定义：

:<math>H(X)=\mathbb{E}_{X} [I(x)] =\sum_{x\in\mathcal{X}}^{}p(x)\log_2\left(\frac{1}{p(x)}\right)</math>

其中<math>\mathcal{X}</math>為有限个事件x的集合，<math>X</math>是定义在<math>\mathcal{X}</math>上的随机变量。信息熵是随机事件不确定性的度量。

信息熵与物理学中的[[热力学熵|热力学熵]]有着紧密的联系:

:<math>S(X)=k_BH(X) </math>

其中S(X)為熱力學熵，H(X)為信息熵，<math>k_B</math>為[[波茲曼常數|波茲曼常數]]。
事實上這個關係也就是廣義的[[波茲曼熵公式|波茲曼熵公式]]，或是在[[正則系綜|正則系綜]]內的熱力學熵表示式。如此可知，[[玻尔兹曼|玻尔兹曼]]与[[吉布斯|吉布斯]]在统计物理学中对熵的工作，啟發了信息論的熵。

信息熵是[[信源編碼定理|信源編碼定理]]中，壓縮率的下限。若編碼所用的資訊量少於信息熵，則一定有資訊的損失。香农在[[大數定律|大數定律]]和{{tsl|en|Asymptotic equipartition property|渐进均分性}}的基础上定義了{{tsl|en|Typical set|典型集}}和典型-{序列}-。典型集是典型-{序列}-的集合。因為一个独立同分布的<math>X</math>-{序列}-属于由<math>X</math>定义的典型集的機率大約為1，所以只需要将屬於典型集的无记忆<math>X</math>信源-{序列}-编为唯一可译碼，其他-{序列}-隨意編碼，就可以達到幾乎無損失的壓縮。

====例子====
設有一個三個面的骰子，三面分別寫有<math>1, 2, 3</math>，<math>X</math>為擲得的數，擲得各面的概率為
:<math>\begin{align}\mathbb P(X = 1) & =1/5, \\
\mathbb P(X = 2) & =2/5, \\
\mathbb P(X = 3) & = 2/5, \end{align}
</math>
則
:<math> H(X)=\frac{1}{5}\log_2 (5)+\frac{2}{5}\log_2\left(\frac{5}{2}\right)+\frac{2}{5}\log_2\left(\frac{5}{2}\right) \approx 1.522 .</math>

===聯合熵與條件熵===
[[聯合熵|聯合熵]]（{{lang|en|Joint Entropy}}）由熵的定義出發，計算[[聯合分布|聯合分布]]的熵：

:<math>H(X,Y)=\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}^{}p(x,y)\log\left(\frac{1}{p(x,y)}\right).</math>

[[條件熵|條件熵]]（{{lang|en|Conditional Entropy}}），顧名思義，是以條件機率<math>p(y|x)</math>計算：

:<math>H(Y|X)=\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}^{}p(x,y)\log\left(\frac{1}{p(y|x)}\right).</math>

由[[貝氏定理|貝氏定理]]，有<math>p(x,y)=p(y|x)p(x)</math>，代入聯合熵的定義，可以分離出條件熵，於是得到聯合熵與條件熵的關係式:

:<math>H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)=H(Y,X).</math>

====链式法則====
可以再對聯合熵與條件熵的關係做推廣，假設現在有<math>n</math>個隨機變量<math>X_i, i=1,2,...,n</math>，重複分離出條件熵，有：

:<math>\begin{align} H(X_1,X_2,...,X_n)&=H(X_1)+H(X_2,...,X_n|X_1)\\
&=H(X_1)+H(X_2|X_1)+H(X_3,...,X_n|X_1,X_2)\\
&=H(X_1)+\sum_{i=2}^{n}H(X_i|X_1,...,X_{i-1})\end{align}.</math>

其直觀意義如下：假如接收一段數列<math>\{X_1,X_2,...,X_n\}</math>，且先收到<math>X_1</math>，再來是<math>X_2</math>，依此類推。那麼收到<math>X_1</math>後總訊息量為<math>H(X_1)</math>，收到<math>X_2</math>後總訊息量為<math>H(X_1)+H(X_2|X_1)</math>，直到收到<math>X_n</math>後，總訊息量應為<math>H(X_1,...,X_n)</math>，於是這個接收過程給出了链式法則。

===互信息===
[[互信息|互信息]]（{{lang|en|Mutual Information}}）是另一有用的信息度量，它是指两个事件集合之间的相关性。两个事件<math>X</math>和<math>Y</math>的互信息定义为：

:<math>I(X;Y) = H(X)-H(X|Y)=H(X) + H(Y) - H(X, Y)=H(Y)-H(Y|X)=I(Y;X).</math>

其意義為，<math>Y</math>包含<math>X</math>的多少資訊。在尚未得到<math>Y</math>之前，對<math>X</math>的不確定性是<math>H(X)</math>，得到<math>Y</math>後，不確定性是<math>H(X|Y)</math>。所以一旦得到<math>Y</math>，就消除了<math>H(X)-H(X|Y)</math>的不確定量，這就是<math>Y</math>對<math>X</math>的資訊量。

如果<math>X,Y</math>互為獨立，則<math>H(X,Y)=H(X)+H(Y)</math>，於是<math>I(X;Y)=0</math>。

又因為<math>H(X|Y)\leq H(X)</math>，所以

:<math>I(X;Y)\leq \min(H(X),H(Y)),</math>
其中等號成立條件為<math>Y=g(X)</math>，<math>g</math>是一個[[双射|-{zh:雙射;zh-hans:双射;zh-hant:對射}-]]函數。

互信息与{{tsl|en|G-test|G检验}}以及[[皮爾森卡方檢定|皮尔森卡方-{A]]有着密切的联系。

== 应用 ==
信息论被广泛应用在：
*[[編碼理論|編碼理論]]
*[[密码学|密码学]]
*[[数据传输|数据传输]]
*[[数据压缩|数据压缩]]
*[[检测理论|检测理论]]
*[[估计理论|估计理论]]
*[[加密|数据加密]]

== 参考文献 ==
{{Reflist|30em}}

== 外部链接 ==
* [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6773024 香农论文：通信的数学理论] {{Wayback|url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6773024 |date=20160801132352 }}

{{-}}
{{数学主要领域}}
{{Computer Science}}
{{控制论}}
{{Authority control}}

[[Category:通信|Category:通信]]
[[Category:控制论|Category:控制论]]
[[Category:信息论|]]
[[Category:形式科學|Category:形式科學]]
[[Category:信息时代|Category:信息时代]]