{{Expand language|1=en|time=2021-04-12T07:17:02+00:00}}
'''联邦学习'''是一种[[机器学习|机器学习]]技术，具體來說就是人們在多个擁有本地[[数据|数据样本]]的分散式[[边缘设备|边缘设备]]或[[服务器|服务器]]上训练算法。这种方法与传统的集中式机器学习技术有顯著不同，传统的集中式机器学习技术将所有的本地数据集上传到一个服务器上，而更经典的分散式方法则通常假设本地数据样本都是[[独立同分布|相同分布]]的。

联合平均 (FedAvg) 是 FedSGD 的泛化，允许本地节点对本地数据进行多次批量更新，并交换更新的权重而不是梯度。此外，对来自相同初始化的调整权重进行平均并不一定会损害所得平均模型的性能。尽管它很简单，但它在现实环境中缺乏理论保证。

传统的联邦学习是基于每个worker设备（节点）的梯度下降：在每一轮中，每个worker通过其本地数据集在本地多次更新其梯度下降权重，然后中央节点聚合器对权重进行平均所有工人，并再次分配给工人。对多轮重复上述过程。这种方法的主要缺点是效率低。众所周知，由于频繁的梯度传输导致的高通信开销会减慢 FL。为了减轻通信开销，已经研究了两种主要技术：（i）表征通信和计算之间权衡的权重的本地更新和（ii）表征通信和精度之间权衡的梯度压缩。<ref>{{Literatur |Autor=Khademi Nori Milad, Yun Sangseok, Kim Il-Min |Titel=Fast Federated Learning by Balancing Communication Trade-Offs |Datum=2021-05-23 |arXiv=2105.11028}}</ref>.

== 参考文献 ==
=== 引用 ===
{{Reflist}}

=== 来源 ===
{{refbegin}}
; 书籍

*  Khademi Nori Milad, Yun Sangseok, Kim Il-Min, Fast Federated Learning by Balancing Communication Trade-Offs (2021). [https://doi.org/10.1109/TCOMM.2021.3083316], [https://arxiv.org/abs/2105.11028].
* Richard O. Duda, Peter E. Hart, David G. Stork (2001). 《模式分类》（第2版）, New York: Wiley. ISBN 0-471-05669-3.
* MacKay, D. J. C. (2003). [http://www.inference.phy.cam.ac.uk/mackay/itila/ 《信息理论、推理和学习算法》]，剑桥大学出版社. ISBN 0-521-64298-1
* Mitchel.l, T. (1997). 《机器学习》, McGraw Hill. ISBN 0-07-042807-7
{{refend}}


联合学习使多个參與者能够在不共享数据的情况下建立一个共同的、强大的机器学习模型，从而可以解决数据隐私、数据安全、数据访问权限和异构数据访问等关键问题。

[[Category:多重代理人系統|Category:多重代理人系統]]
[[Category:分散式演算法|Category:分散式演算法]]
[[Category:机器学习|Category:机器学习]]