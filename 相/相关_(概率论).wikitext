{{noteTA
|G1=Math
|1=zh-cn:协方差;zh-tw:共變異數;
|2=zh-cn:方差;zh-tw:變異數;
|3=zh-hans积矩;zh-hant:積差
}}
{{other uses|因果關係}}

'''相关'''（Correlation），又称为 '''相关性'''、'''关联'''，在[[概率论|概率论]]和[[统计学|统计学]]中，相关显示了两个或几个[[随机变量|随机变量]]之间线性关系的强度和方向。在统计学中，相关的意义是：用来衡量两个变量相对于其相互独立的距离。在这个广义的定义下，有许多根据数据特点用来衡量数据相关性而定义的系数，称作 '''相关系数'''。通常使用相关系数来计量这些随机变量协同变化的程度，当随机变量间呈现同一方向的变化趋势时称为正相关，反之则称为负相关。

[[Image:Correlation_examples2.svg|thumb]]

==历史==
英国生物学家和统计学家[[弗朗西斯·高尔顿|弗朗西斯·高尔顿]]首先提出“相关”这一概念，英国数学家[[卡尔·皮尔逊|卡尔·皮尔逊]]在此基础上做出了进一步发展。

==各種相關係數==
对于不同[[測量尺度|測量尺度]]的變數，有不同的相關系数可用：
* [[皮尔逊积矩相关系数|皮尔逊相关系数]]（Pearson's ''r''）：衡量兩個[[測量尺度#等距尺度|等距尺度]]或[[測量尺度#等比尺度|等比尺度]]變數之相關性。是最常見的，也是學習統計學時第一個接觸的相關係數。
* [[淨相關|淨相關]]（{{Lang-en|partial correlation}}）：在模型中有多個自變數（或解釋變數）時，去除掉其他自變數的影響，只衡量特定一個自變數與因變數之間的相關性。自變數和因變數皆為連續變數。
* 相關比（{{Lang-en|correlation ratio}}）：衡量兩個連續變數之相關性。

* [[Gamma相關係數|Gamma相關係數]]：衡量兩個[[測量尺度#次序尺度|次序尺度]]變數之相關性。
* [[斯皮尔曼等级相关系数|斯皮尔曼等级相关系数]]：衡量兩個[[測量尺度#次序尺度|次序尺度]]變數之相關性。
* {{le|肯德尔等級相關係數|Kendall rank correlation coefficient}}：衡量兩個人為[[測量尺度#次序尺度|次序尺度]]變數（原始資料為[[測量尺度#等距尺度|等距尺度]]）之相關性。
* 肯德尔和諧係數：衡量兩個[[測量尺度#次序尺度|次序尺度]]變數之相關性。

* [[Phi相關係數|Phi相關係數]]（{{Lang-en|Phi coefficient}}）：衡量兩個真正[[測量尺度#名目尺度|名目尺度]]的二分變數之相關性。
* 列聯相關係數（{{Lang-en|contingency coefficient}}）：衡量兩個真正[[測量尺度#名目尺度|名目尺度]]變數之相關性。
* 四分相關（{{Lang-en|tetrachoric correlation}}）：衡量兩個人為[[測量尺度#名目尺度|名目尺度]]（原始資料為[[測量尺度#等距尺度|等距尺度]]）的二分變數之相關性。
* Kappa一致性係數（{{Lang-en|K coefficient of agreement}}）：衡量兩個[[測量尺度#名目尺度|名目尺度]]變數之相關性。

* 點二系列相關係數（{{Lang-en|point-biserial correlation}}）：X變數是真正[[測量尺度#名目尺度|名目尺度]]二分變數。Y變數是連續變數。
* 二系列相關係數（{{Lang-en|biserial correlation}}）：X變數是人為[[測量尺度#名目尺度|名目尺度]]二分變數。Y變數是連續變數。

==皮尔逊积差系数==
{{main|皮尔逊积矩相关系数}}
===数学特征===
:<math>\rho_{X,Y}={\mathrm{cov}(X,Y)\over \sigma_X \sigma_Y} ={E((X-\mu_X)(Y-\mu_Y)) \over \sigma_X\sigma_Y}</math>，
其中，''E''是[[数学期望|数学期望]]，cov表示[[协方差|协方差]]，<math>\sigma_X</math>和<math>\sigma_Y</math>是[[標準差|標準差]]。

因为<math>\mu_X = E (X)</math>，<math>\sigma_X^2 = E(X^2) - E^2 (X)</math>，同样地，对于<math>Y</math>，可以写成

:<math>\rho_{X,Y}=\frac{E (XY)-E (X)E (Y)}{\sqrt{E(X^2)-E^2 (X)}~\sqrt{E(Y^2)-E^2 (Y)}}</math>。

当两个变量的[[標準差|標準差]]都不为零，相关系数才有定义。从[[柯西-施瓦茨不等式|柯西-施瓦茨不等式]]可知，相关系数的[[絕對值|絕對值]]不超过1。当两个变量的线性关系增强时，相关系数趋于1或-1。当一个变量增加而另一变量也增加时，相关系数大于0。当一个变量的增加而另一变量减少时，相关系数小于0。当两个变量独立时，相关系数为0，但反之并不成立。这是因为相关系数仅仅反映了两个变量之间是否线性相关。比如说，''X''是区间［－1，1］上的一个均匀分布的随机变量。''Y'' = ''X''<sup>2</sup>.那么''Y''是完全由''X''确定。因此''Y''和''X''不独立，但相关系数为0。或者说他们是不相关的。当''Y''和''X''服从联合正态分布时，其相互独立和不相关是等价的。

当一个或两个变量带有测量误差时，他们的相关性就受到削弱，这时，“反衰减”性（disattenuation）是一个更准确的系数。

===几何特征===
对于居中的数据来说（何谓居中？也就是每个数据减去样本均值，居中后它们的平均值就为0），相关系数可以看作是两个随机变量中得到的样本集向量之间夹角的cosine函数。一些实际工作者更喜欢用非居中的相关系数（与皮尔逊系数不相兼容）。看下面的例子中有一个比较。例如，假设五个国家的国民生产总值分别是1、2、3、5、8（单位10亿美元），又假设这五个国家的贫困比例分别是11%、12%、13%、15%、18%。则我们现在有两个有序的包含5个元素的向量x、y：x =（1, 2, 3, 5, 8）、 y =（0.11, 0.12, 0.13, 0.15, 0.18）
使用一般的方法来计算向量间夹角（参考[[数量积|数量积]]），未居中的相关性系数如下：
:<math> \cos \theta = \frac { \mathbf{x} \cdot \mathbf{y} } { \left\| \mathbf{x} \right\| \left\| \mathbf{y} \right\| } = \frac { 2.93 } { \sqrt { 103 } \sqrt { 0.0983 } } = 0.920814711</math>。
上面的数据实际上是故意选择了一个完美的线性关系：y = 0.10 + 0.01 x。因此皮尔逊相关系数应该就是1。把数据居中（x中数据减去E (x) = 3.8，y中数据减去E (y) = 0.138）后得到：x =（−2.8, −1.8, −0.8, 1.2, 4.2）、y =（−0.028, −0.018, −0.008, 0.012, 0.042），由此得到了预期结果：
:<math> \cos \theta = \frac { \mathbf{x} \cdot \mathbf{y} } { \left\| \mathbf{x} \right\| \left\| \mathbf{y} \right\| } = \frac { 0.308 } { \sqrt { 30.8 } \sqrt { 0.00308 } } = 1 = \rho_{xy}</math>，

== 统计学上的相关 ==

=== 样本相关系数 ===

对于样本对<math>(X_i, Y_i)</math>，相关系数的计算过程可表示为：将每个变量都通过减去平均值、再除以校正标准差后转化为标准单位，乘积的平均值，再经过{{link-en|贝塞尔校正|Bessel's correction}}即为相关系数<ref>{{cite book | language = en | author = David Freedman | coauthors = Robert Pisani, Roger Purves | title = Statistics | url =https://archive.org/details/statistics00free| date = 1998 | location =  | publisher = Norton & Company | id = 3 | isbn = 9780393960433 | pages = [https://archive.org/details/statistics00free/page/148 148] }}</ref>。

两个变量的关系可以直观地用散点图表示，当其紧密地群聚于一条直线的周围时，变量间存在强相关<ref>{{cite book | language = en | author = David Freedman | coauthors = Robert Pisani, Roger Purves | title = Statistics | url =https://archive.org/details/statistics00free| date = 1998 | location =  | publisher = Norton & Company | id = 3 | isbn = 9780393960433 | pages = [https://archive.org/details/statistics00free/page/156 156] }}</ref>。

一个散点图可以用五个统计量来概括：所有<math>x</math>值的平均数<math>\bar x</math>，所有<math>x</math>值的[[標準差#样本的标准差|校正标准差]]（即样本标准差）<math>s_x</math>，所有<math>y</math>值的平均数<math>\bar y</math>，所有<math>y</math>值的校正标准差<math>s_y</math>，相关系数<math>r_{xy}</math>。

其中：

:<math>
s_x =\sqrt{ \frac{1}{N-1} \sum_{i=1}^N (x_i - \overline{x})^2},
s_y =\sqrt{ \frac{1}{N-1} \sum_{i=1}^N (y_i - \overline{y})^2}
</math>


那么：

:<math>
r_{xy} \quad \overset{\underset{\mathrm{def}}{}}{=} \quad \frac{\sum\limits_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{(n-1)s_x s_y}
      =\frac{\sum\limits_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}
            {\sqrt{\sum\limits_{i=1}^n (x_i-\bar{x})^2 \sum\limits_{i=1}^n (y_i-\bar{y})^2}},
</math>

或写成：

:<math>
r_{xy} \quad \overset{\underset{\mathrm{def}}{}}{=} \quad \frac{n}{n-1}\frac{1}{n}\sum\limits_{i=1}^n \frac{(x_i-\bar{x})}{s_x}\frac{(y_i-\bar{y})}{s_y}</math>,

其中<math>\frac{n}{n-1}</math>为{{link-en|贝塞尔校正|Bessel's correction}}。

== 參考文獻 ==
{{Reflist}}

== 参见 ==
* [[相关不蕴涵因果|相关不蕴涵因果]]
* [[相关系数|相关系数]]

{{-}}
{{统计学}}

[[Category:协方差与相关性|Category:协方差与相关性]]
[[Category:统计学术语|Category:统计学术语]]