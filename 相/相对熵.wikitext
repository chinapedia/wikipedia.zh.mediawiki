{{專家}}
'''KL散度'''（'''Kullback-Leibler divergence'''，簡稱'''KLD'''）<ref>{{cite journal
 |last1=Kullback |first1=S. |authorlink1=Solomon Kullback
 |last2=Leibler |first2=R.A. |authorlink2=Richard Leibler
 |year = 1951
 |title = On Information and Sufficiency
 |url=https://archive.org/details/sim_annals-of-mathematical-statistics_1951-03_22_1/page/79 |journal = [[Annals_of_Mathematical_Statistics|Annals of Mathematical Statistics]]
 |volume = 22
 |issue=1
 |pages=79–86
 |doi=10.1214/aoms/1177729694
 |mr=39968
}}</ref>，在訊息系统中称为'''相对熵'''（relative entropy），在连续时间序列中称为随机性（randomness），在统计模型推断中称为訊息增益（information gain）。也称訊息散度（information divergence）。

'''KL散度'''是两个機率分布P和Q差别的非对称性的度量。 KL散度是用来度量使用基于Q的分布来编码服从P的分布的样本所需的额外的平均比特数。典型情况下，P表示数据的真实分布，Q表示数据的理论分布、估计的模型分布、或P的近似分布。<ref>{{cite journal
 |last1=Kullback |first1=S. |authorlink1=Solomon Kullback
 |last2=Leibler |first2=R.A. |authorlink2=Richard Leibler
 |year = 1951
 |title = On information and sufficiency
 |url=https://archive.org/details/sim_annals-of-mathematical-statistics_1951-03_22_1/page/79 |journal = [[Annals_of_Mathematical_Statistics|Annals of Mathematical Statistics]]
 |volume = 22 |issue=1
 |pages=79–86
 |doi=10.1214/aoms/1177729694 |mr=39968
}}</ref>

==定義==

對於[[离散隨機变量|离散隨機变量]]，其機率分布''P'' 和 ''Q''的KL散度可按下式定義為

:<math>D_{\mathrm{KL}}(P\|Q) = -\sum_i P(i) \ln \frac{Q(i)}{P(i)}. \!</math>

等价于

:<math>D_{\mathrm{KL}}(P\|Q) = \sum_i P(i) \ln \frac{P(i)}{Q(i)}. \!</math>

即按機率''P''求得的''P''和''Q''的[[對數|對數]]商的平均值。KL散度僅當機率''P''和''Q''各自總和均為1，且對於任何''i''皆滿足<math>Q(i)>0</math>及<math>P(i)>0</math>時，才有定義。式中出現<math>0 \ln 0</math>的情況，其值按0處理。

對於[[連續隨機變量|連續隨機變量]]，其機率分佈''P''和''Q''可按積分方式定義為 <ref>C. Bishop (2006). Pattern Recognition and Machine Learning. p. 55.</ref>

: <math>D_{\mathrm{KL}}(P\|Q) = \int_{-\infty}^\infty p(x) \ln \frac{p(x)}{q(x)} \, {\rm d}x, \!</math>

其中''p''和''q''分別表示分佈''P''和''Q''的密度。

更一般的，若''P''和''Q''為集合''X''的機率[[測度|測度]]，且''P''關於''Q''[[绝对连续|絕對連續]]，則從''P''到''Q''的KL散度定義為

:<math> D_{\mathrm{KL}}(P\|Q) = \int_X \ln \frac{{\rm d}P}{{\rm d}Q} \,{\rm d}P, \!</math>

其中，假定右側的表達形式存在，則<math>\frac{{\rm d}Q}{{\rm d}P} </math>為''Q''關於''P''的[[拉东-尼科迪姆定理|R–N導數]]。

相應的，若''P''關於''Q''[[绝对连续|絕對連續]]，則

:<math> D_{\mathrm{KL}}(P\|Q) = \int_X \ln \frac{{\rm d}P}{{\rm d}Q} \,{\rm d}P
                      = \int_X \frac{{\rm d}P}{{\rm d}Q} \ln\frac{{\rm d}P}{{\rm d}Q}\,{\rm d}Q,</math>

即為''P''關於''Q''的相對熵。

==特性==

[[相對熵|相對熵]]的值為非負數：
:<math>D_{\mathrm{KL}}(P\|Q) \geq 0, \,</math>
由[[吉布斯不等式|吉布斯不等式]]可知，當且僅當<math>P=Q</math>時<math>D_{KL}(P\|Q)</math>為零。

尽管从直觉上KL散度是个[[度量|度量或距离函数]], 但是它实际上并不是一个真正的度量或距離。因為KL散度不具有对称性：从分布''P''到''Q''的距离通常并不等于从''Q''到''P''的距离。
:<math>D_{\mathrm{KL}}(P\|Q) \neq D_{\mathrm{KL}}(Q\|P)</math>
<br>

==KL散度和其它量的关系==
[[自信息|自信息]]和KL散度
:<math>I(m) = D_{\mathrm{KL}}(\delta_{im} \| \{ p_i \}), </math>
<br>
[[互信息|互信息]]和KL散度
:<math>\begin{align}I(X;Y) & = D_{\mathrm{KL}}(P(X,Y) \| P(X)P(Y) ) \\
& = \mathbb{E}_X \{D_{\mathrm{KL}}(P(Y|X) \| P(Y) ) \} \\
& = \mathbb{E}_Y \{D_{\mathrm{KL}}(P(X|Y) \| P(X) ) \}\end{align} </math>
<br>
[[信息熵|信息熵]]和KL散度
:<math>\begin{align}H(X) & = \mathrm{(i)} \, \mathbb{E}_x \{I(x)\} \\
& = \mathrm{(ii)} \log N - D_{\mathrm{KL}}(P(X) \| P_U(X) )\end{align}</math>
<br>
[[条件熵|条件熵]]和KL散度
:<math>\begin{align}H(X|Y) & = \log N - D_{\mathrm{KL}}(P(X,Y) \| P_U(X) P(Y) ) \\
& = \mathrm{(i)} \,\, \log N - D_{\mathrm{KL}}(P(X,Y) \| P(X) P(Y) ) - D_{\mathrm{KL}}(P(X) \| P_U(X)) \\
& = H(X) - I(X;Y) \\
& = \mathrm{(ii)} \, \log N - \mathbb{E}_Y \{ D_{\mathrm{KL}}(P(X|Y) \| P_U(X)) \}\end{align}</math>
<br>
[[交叉熵|交叉熵]]和KL散度
:<math>\mathrm{H}(p, q) = \mathrm{E}_p[-\log q] = \mathrm{H}(p) + D_{\mathrm{KL}}(p \| q).\!</math>

==參考文獻==
{{Reflist}}

[[Category:概率与统计|Category:概率与统计]]
[[Category:應用數學|Category:應用數學]]
[[Category:概率论|Category:概率论]]
[[Category:信息论|Category:信息论]]
[[Category:信息學熵|Category:信息學熵]]