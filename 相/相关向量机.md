{{机器学习导航栏}}

'''相关向量机'''（Relevance vector machine，RVM）是使用[[贝叶斯推理|贝叶斯推理]]得到[[回归|回归]]和[[分类|分类]]的[[简约|简约]]解的[[机器学习|机器学习]]技术。RVM的函数形式与[[支持向量机|支持向量机]]相同，但是可以提供概率分类。

其与带[[协方差函数|协方差函数]]的[[高斯过程|高斯过程]]等效。：
:<math>k(\mathbf{x},\mathbf{x'}) = \sum_{j=1}^N \frac{1}{\alpha_j} \phi(\mathbf{x},\mathbf{x}_j)\phi(\mathbf{x}',\mathbf{x}_j) </math>
其中φ是[[核函数|核函数]]（通常是高斯核函数），'''x'''<sub>1</sub>,…,'''x'''<sub>''N''</sub>是[[训练集|训练集]]的输入向量。{{Citation needed|date=February 2010}}

Compared to the SVM the Bayesian formulation allows avoiding the set of free parameters that the SVM has and that usually require cross-validation based post optimizations. However RVMs use an [[Expectation_Maximization|Expectation Maximization]] (EM)-like learning method and are therefore at risk of local minima, unlike the standard [[Sequential_Minimal_Optimization|SMO]]-based algorithms employed by [[Support_vector_machine|SVM]]s which are guaranteed to find a global optimum.{{Citation needed|date=February 2010}}

== 参考 ==
* {{cite journal | last=Tipping | first=Michael E. |title=Sparse Bayesian Learning and the Relevance Vector Machine |year=2001 |journal = [[Journal_of_Machine_Learning_Research|Journal of Machine Learning Research]] |volume=1 |pages=211–244 |url=http://jmlr.csail.mit.edu/papers/v1/tipping01a.html | doi=10.1162/15324430152748236}}

== 软件==
* [http://dlib.net dlib C++ Library]
* [http://www.terborg.net/research/kml/ The Kernel-Machine Library] 

==外部链接==
*[http://www.relevancevector.com Tipping's webpage on Sparse Bayesian Models and the RVM]
*[https://web.archive.org/web/20111005202038/http://www.tristanfletcher.co.uk/RVM%20Explained.pdf A Tutorial on RVM by Tristan Fletcher]

[[Category:分类算法|Category:分类算法]]
[[Category:Ensemble_learning|Category:Ensemble learning]]
[[Category:机器学习|Category:机器学习]]
[[Category:Non-parametric_Bayesian_methods|Category:Non-parametric Bayesian methods]]