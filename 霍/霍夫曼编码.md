{{noteTA
|G1=IT}}

[[File:Huffman_tree_2.svg|right]]
{| class="wikitable" style="float:right; clear:right; margin-left: 1em"
!字母!!頻率!!編碼
|-
|space||7||111
|-
|a    ||4||010
|-
|e    ||4||000
|-
|f    ||3||1101
|-
|h    ||2||1010
|-
|i    ||2||1000
|-
|m    ||2||0111
|-
|n    ||2||0010
|-
|s    ||2||1011
|-
|t    ||2||0110
|-
|l    ||1||11001
|-
|o    ||1||00110
|-
|p    ||1||10011
|-
|r    ||1||11000
|-
|u    ||1||00111
|-
|x    ||1||10010
|}
'''霍夫曼編碼'''（{{lang-en|Huffman Coding}}），又譯為'''哈夫曼编码'''、'''赫夫曼编码'''，是一種用於[[无损数据压缩|无损数据压缩]]的[[熵編碼|熵編碼]]（權編碼）[[演算法|演算法]]。由美國計算機科學家[[大衛·霍夫曼|大衛·霍夫曼]]（{{lang|en|David Albert Huffman}}）在1952年發明。

==簡介==
在[[计算机|计算机]][[資料處理|資料處理]]中，霍夫曼編碼使用[[變長編碼表|變長編碼表]]對源符號（如文件中的一個字母）進行編碼，其中[[變長編碼表|變長編碼表]]是通過一種評估來源符號出現機率的方法得到的，出現機率高的字母使用較短的編碼，反之出現機率低的則使用較長的編碼，這便使編碼之後的字符串的平均長度、[[期望值|期望值]]降低，從而達到[[無損壓縮|無損壓縮]]數據的目的。

例如，在英文中，e的出現[[機率|機率]]最高，而z的出現概率則最低。當利用霍夫曼編碼對一篇英文進行壓縮時，e極有可能用一個[[位元|位元]]來表示，而z則可能花去25個[[位元|位元]]（不是26）。用普通的表示方法時，每個英文字母均占用一個[[字節|字節]]，即8個[[位元|位元]]。二者相比，e使用了一般編碼的1/8的長度，z則使用了3倍多。倘若我們能實現對於英文中各個字母出現概率的較準確的估算，就可以大幅度提高無損壓縮的比例。

霍夫曼樹又稱最優二叉樹，是一種帶權路徑長度最短的[[二叉樹|二叉樹]]。所謂樹的帶權路徑長度，就是樹中所有的葉結點的權值乘上其到根結點的路徑長度（若根結點爲0層，葉結點到根結點的路徑長度爲葉結點的層數）。樹的路徑長度是從樹根到每一結點的路徑長度之和，記爲WPL=（W1*L1+W2*L2+W3*L3+...+Wn*Ln），N個權值Wi（i=1,2,...n）構成一棵有N個葉結點的二叉樹，相應的葉結點的路徑長度爲Li（i=1,2,...n）。可以證明霍夫曼樹的WPL是最小的。

== 歷史 ==
1951年，霍夫曼在[[麻省理工學院|麻省理工學院]]（MIT）攻讀博士學位，他和修讀[[信息論|信息論]]課程的同學得選擇是完成學期報告還是期末[[考試|考試]]。導師[[羅伯特·法諾|羅伯特·法諾]]（Robert Fano）出的學期報告題目是：尋找最有效的二進制編碼。由於無法證明哪個已有編碼是最有效的，霍夫曼放棄對已有編碼的研究，轉向新的探索，最終發現了基於有序頻率[[二叉樹|二叉樹]]編碼的想法，並很快證明了這個方法是最有效的。霍夫曼使用自底向上的方法構建二叉樹，避免了次優算法[[香農-范諾編碼|香農-范諾編碼]]（Shannon–Fano coding）的最大弊端──自頂向下構建樹。

1952年，於論文《一種構建極小多餘編碼的方法》（{{lang|en|''A Method for the Construction of Minimum-Redundancy Codes''}}）中發表了這個編碼方法。

==問題定義與解法==
[[File:TABLE1.JPG|thumb]]
[[File:Huffman_algorithm.gif|thumb]]
[[File:TABLE8.JPG|thumb]]
===廣義===
*給定
:一組符號（Symbol）和其對應的權重值（weight），其權重通常表示成-{zh-cn:概率;zh-tw:機率;}-（%）。
*欲知
:一組二元的前置碼，其二元碼的長度為最短。

===狹義===
*輸入
:符號集合<math>S = \left\{s_{1},s_{2},\cdots,s_{n}\right\}</math>，其S集合的大小為<math>n</math>。<br>
:權重集合<math>W = \left\{w_{1},w_{2},\cdots,w_{n}\right\}</math>，其W集合不為負數且<math>w_{i} = \mathrm{weight}\left(s_{i}\right), 1\leq i \leq n</math>。<br>
*輸出
:一組編碼<math>C \left(S,W\right) = \left\{c_{1},c_{2},\cdots,c_{n}\right\}</math>，其C集合是一組二進制編碼且<math>c_{i}</math>為<math>s_{i}</math>相對應的編碼，<math>1\leq i \leq n</math>。
*目標
:設<math>L \left(C\right) = \sum_{i=1}^{n}{w_{i}\times\mathrm{length}\left(c_{i}\right)}</math>為<math>C</math>的加權的路徑長，對所有編碼<math>T \left(S,W\right)</math>，則<math>L\left(C\right) \leq L\left(T\right)</math>

===範例===
霍夫曼樹常處理符號編寫工作。根據整組資料中符號出現的頻率高低，決定如何給符號編碼。如果符號出現的頻率越高，則給符號的碼越短，相反符號的號碼越長。假設我們要給一個英文單字'''"F O R G E T"'''進行霍夫曼編碼，而每個英文字母出現的頻率分別列在''Fig.1''。

====演算過程====
（一）進行霍夫曼編碼前，我們先建立一個霍夫曼樹。
:⒈將每個英文字母依照出現頻率由小排到大，最小在左，如''Fig.1''。
:⒉每個字母都代表一個終端節點（葉節點），比較'''F.O.R.G.E.T'''六個字母中每個字母的出現頻率，將最小的兩個字母頻率相加合成一個新的節點。如''Fig.2''所示，發現'''F'''與'''O'''的頻率最小，故相加2+3=5。
:⒊比較'''5.R.G.E.T'''，發現'''R'''與'''G'''的頻率最小，故相加4+4=8。
:⒋比較'''5.8.E.T'''，發現'''5'''與'''E'''的頻率最小，故相加5+5=10。
:⒌比較'''8.10.T'''，發現'''8'''與'''T'''的頻率最小，故相加8+7=15。
:⒍最後剩'''10.15'''，沒有可以比較的對象，相加10+15=25。

最後產生的樹狀圖就是霍夫曼樹，參考''Fig.2''。
<br>
（二）進行編碼<br>
:1.給霍夫曼樹的所有左鏈結'0'與右鏈結'1'。
:2.從樹根至樹葉依序記錄所有字母的編碼，如''Fig.3''。

== 實現方法 ==

===資料壓縮===
實現[[霍夫曼編碼|霍夫曼編碼]]的方式主要是創建一個[[二元樹|二元樹]]和其節點。這些樹的節點可以儲存在[[陣列|陣列]]裡，[[陣列|陣列]]的大小為符號（symbols）數的大小'''n'''，而節點分别是終端節點（葉節點）與非終端節點（內部節點）。

一開始，所有的節點都是終端節點，節點內有三個欄位：

1.符號（Symbol）

2.權重（Weight、Probabilities、Frequency）

3.指向父節點的[[連結串列|鏈結]]（Link to its parent node）

而非終端節點內有四個欄位：

1.權重（Weight、Probabilities、Frequency）

2.指向兩個子節點的[[連結串列|鏈結]]（Links to two child node）  

3.指向父節點的[[連結串列|鏈結]]（Link to its parent node）

基本上，我們用'0'與'1'分別代表指向左子節點與右子節點，最後為完成的二元樹共有'''n'''個終端節點與'''n-1'''個非終端節點，去除了不必要的符號並產生最佳的編碼長度。

過程中，每個終端節點都包含著一個權重（Weight、Probabilities、Frequency），兩兩終端節點結合會產生一個新節點，'''新節點的權重'''是由'''兩個權重最小的終端節點權重之總和'''，並持續進行此過程直到只剩下一個節點為止。

實現霍夫曼樹的方式有很多種，可以使用優先佇列（Priority Queue）簡單達成這個過程，給與權重較低的符號較高的優先順序（Priority），演算法如下：

⒈把n個終端節點加入優先佇列，則n個節點都有一個優先權Pi，1 ≤ i ≤ n

⒉如果佇列內的節點數>1，則：
	
:⑴從佇列中移除兩個最小的Pi節點，即連續做兩次remove（min（Pi）, Priority_Queue)

:⑵產生一個新節點，此節點為（1）之移除節點之父節點，而此節點的權重值為（1）兩節點之權重和

:⑶把（2）產生之節點加入優先佇列中

⒊最後在優先佇列裡的點為樹的根節點（root）

而此演算法的時間複雜度（[[計算複雜性理論|Time Complexity]]）為O（''n'' log ''n''）；因為有n個終端節點，所以樹總共有2n-1個節點，使用優先佇列每個迴圈須O（log ''n''）。

此外，有一個更快的方式使時間複雜度降至線性時間（Linear Time）O（''n''），就是使用兩個佇列（Queue）創件霍夫曼樹。第一個佇列用來儲存n個符號（即n個終端節點）的權重，第二個佇列用來儲存兩兩權重的合（即非終端節點）。此法可保證第二個佇列的前端（Front）權重永遠都是最小值，且方法如下：

⒈把n個終端節點加入第一個佇列（依照權重大小排列，最小在前端）

⒉如果佇列內的節點數>1，則：

:⑴從佇列前端移除兩個最低權重的節點

:⑵將（1）中移除的兩個節點權重相加合成一個新節點

:⑶加入第二個佇列

⒊最後在第一個佇列的節點為根節點

雖然使用此方法比使用優先佇列的時間複雜度還低，但是注意此法的第1項，節點必須依照權重大小加入佇列中，如果節點加入順序不按大小，則需要經過排序，則至少花了O（''n'' log ''n''）的時間複雜度計算。

但是在不同的狀況考量下，時間複雜度並非是最重要的，如果我們今天考慮英文字母的出現頻率，變數n就是英文字母的26個字母，則使用哪一種演算法時間複雜度都不會影響很大，因為n不是一筆龐大的數字。

===資料解壓縮===

簡單來說，霍夫曼碼樹的解壓縮就是將得到的前置碼（Prefix Huffman code）轉換回符號，通常藉由樹的追蹤（Traversal），將接收到的位元串（Bits stream）一步一步還原。但是要追蹤樹之前，必須要先重建霍夫曼樹
；某些情況下，如果每個符號的權重可以被事先預測，那麼霍夫曼樹就可以預先重建，並且儲存並重複使用，否則，傳送端必須預先傳送霍夫曼樹的相關資訊給接收端。

最簡單的方式，就是預先統計各符號的權重並加入至壓縮之位元串，但是此法的運算量花費相當大，並不適合實際的應用。若是使用Canonical encoding，則可精準得知樹重建的資料量只占''B''2^''B''位元（其中B為每個符號的位元數（bits））。如果簡單將接收到的位元串一個位元一個位元的重建，例如：'0'表示父節點，'1'表示終端節點，若每次讀取到1時，下8個位元則會被解讀是終端節點（假設資料為8-bit字母），則霍夫曼樹則可被重建，以此方法，資料量的大小可能為2~320位元組不等。雖然還有很多方法可以重建霍夫曼樹，但因為壓縮的資料串包含"traling bits"，所以還原時一定要考慮何時停止，不要還原到錯誤的值，如在資料壓縮時時加上每筆資料的長度等。

==資料長度==
設符號集合<math>S = \left\{s_{1},s_{2},\cdots,s_{n}\right\}</math>，<math>P\left(s_{j}\right)</math>为<math> s_{j}</math>發生的機率
，<math>L\left(s_{j}\right)</math>为<math>s_{j}</math>編碼的長度

*熵（Entropy）:亂度
:<math>entropy = - \sum_{j=1}^{J}{ P \left(s_{j}\right)\times\ln {P \left(s_{j}\right)}  }</math><br>
ex:<br>
<math> P(S_0)=1,entropy = 0</math><br>
<math> P(S_0)=P(S_1)=0.5,entropy = 0.6931</math><br>
<math> P(S_0)=P(S_1)=P(S_2)=P(S_3)=P(S_4)=0.2,entropy = 1.6094</math><br>
<math> P(S_0)=P(S_1)=P(S_2)=P(S_3)=0.1,P(S_4)=0.6,entropy = 1.2275</math><br>
第三與第四個範例，同樣是五種組合，機率分布越集中，則亂度越少
*霍夫曼碼平均長度
:<math> mean \left(L \right) = \sum_{j=1}^{J}{P \left(s_{j}\right)\times\ L \left(s_{j}\right)}</math>
*霍夫曼碼的長度
:'''Shannon編碼定理：''' <math>entropy \over logk</math> ≤ <math> mean \left(L \right)</math> ≤ <math>entropy \over logk</math> <math>+ 1 </math> ，若使用<math>k</math>進位的編碼
'''霍夫曼碼的平均編碼長度：'''設<math>b = mean \left(L \right) N </math>，<math>N</math>為資料長度
:<math>N</math><math> entropy \over logk</math> ≤ <math>b</math> ≤ <math>N</math><math> entropy \over logk</math> <math>+ N </math>

== 範例 ==
*當有100個位子，有白色佔60%、咖啡色佔20%，藍色和紅色各佔10%，則該如何分配才能最優化此座位?
(a)direct:  <br>
假設結果為：白色為00、咖啡色01，藍色10和紅色11個bits
則結果為:100*2 = 200bits<br>
(b)huffman code: (must be satisfy the following conditions，if not change the node)<br>
(1) 所有碼皆在Coding Tree的端點，再下去沒有分枝(滿足一致解碼跟瞬間解碼)<br>
(2) 機率越大，code length越短；機率越小，code length越長<br>
(3) 假設<math>S_a</math>是第L層的node，<math>S_b</math>是第L+1層的node<br>
    則<math>P(S_a) \geq P(S_b)</math>必須滿足<br>
假設結果為：白色佔0、咖啡色10，藍色110和紅色111個bits<br>
則結果為:60*1+20*2+20*3=160bits<br>
<br>
相互比較兩個結果huffman code 整整少了40bits的儲存空間。<br>
== 示範程式 ==
<source lang="cpp" style="overflow:hidden" line="1">
// 以下為C++程式碼，在G++下編譯通過
// 僅用於示範如何根據權值建構霍夫曼樹，
// 沒有經過性能上的優化及加上完善的異常處理。
#include <cstdlib>
#include <iostream>
#include <deque>
#include <algorithm>

using namespace std;

const int size = 10;
struct node {                               // 霍夫曼樹節點結構
    unsigned key;                           // 保存權值
    node *lchild;                           // 左孩子指針
    node *rchild;                           // 右孩子指針
};
deque<node *> forest;
deque<bool> code;                           // 此處也可使用bitset
node *ptr;
int frequency[size] = {0};

void printCode(deque<bool> ptr);            // 用於輸出霍夫曼編碼

bool compare( node *a, node *b) {
    return a->key < b->key;
}
int main(int argc, char *argv[]) {
    for (int i = 0; i < size; i++) {
        cin >> frequency[i];                // 輸入10個權值
        ptr = new node;
        ptr->key = frequency[i];
        ptr->lchild = NULL;
        ptr->rchild = NULL;
        forest.push_back(ptr);
    } // 形成森林，森林中的每一棵樹都是一個節點
    // 從森林構建霍夫曼樹
    for (int i = 0; i < size - 1; i++) {
        sort(forest.begin(), forest.end(), compare);
        ptr = new node;
        // 以下代碼使用下標索引隊列元素，具有潛在危險，使用時請注意
        ptr->key = forest[0]->key + forest[1]->key;
        ptr->lchild = forest[0];
        ptr->rchild = forest[1];
        forest.pop_front();
        forest.pop_front();
        forest.push_back(ptr);
    }
    ptr = forest.front(); // ptr是一個指向根的指針
    system("PAUSE");
    return EXIT_SUCCESS;
}

void printCode(deque<bool> ptr) {
    // deque<bool>
    for (int i = 0; i < ptr.size(); i++) {
        if (ptr[i])
            cout << "1";
        else
            cout << "0";
    }
}
</source>

==參考文獻==
{{refbegin}}
* [[Thomas_H._Cormen|Thomas H. Cormen]], [[Charles_E._Leiserson|Charles E. Leiserson]], [[Ronald_L._Rivest|Ronald L. Rivest]], and [[Clifford_Stein|Clifford Stein]]. ''[[Introduction_to_Algorithms|Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. {{isbn|0-262-03293-7}}. Section 16.3, pp. 385–392.
{{refend}}

== 外部連結 ==
*有關霍夫曼壓縮技術的原文：D.A. Huffman, "[https://web.archive.org/web/20050530145744/http://compression.graphicon.ru/download/articles/huff/huffman_1952_minimum-redundancy-codes.pdf A method for the construction of minimum-redundancy codes]", Proceedings of the I.R.E., sept 1952, pp 1098-1102
*[http://huffman.ooz.ie/ 霍夫曼树图形演示]
*Animation of the Huffman Algorithm: [https://web.archive.org/web/20030911214408/http://student.cs.ucc.ie/03/sam2/traditionalHuffmanDisplay.html Algorithm Simulation by Simon Mescal]
*Background story: [https://web.archive.org/web/20040624203323/http://www.huffmancoding.com/david/scientific.html Profile: David A. Huffman], [[Scientific_American|Scientific American]], Sept. 1991, pp. 54-58
* [https://web.archive.org/web/20050618085402/http://alexvn.freeservers.com/s1/huffman_template_algorithm.html n-ary Huffman Template Algorithm]
* [http://mathforum.org/discuss/sci.math/t/632220 Huffman codes' connection with Fibonacci and Lucas numbers]
* [http://groups.google.com/groups?selm=2ss5aqF1oc42cU1%40uni-berlin.de Fibonacci connection between Huffman codes and Wythoff array]
* [http://www.research.att.com/projects/OEIS?Anum=A098950 Sloane A098950] Minimizing k-ordered sequences of maximum height Huffman tree
* [https://web.archive.org/web/20050731091059/http://semillon.wpi.edu/~aofa/AofA/msg00040.html Computing Huffman codes on a Turing Machine]
* Mordecai J. Golin, Claire Kenyon, Neal E. Young "[http://www.cs.ust.hk/faculty/golin/pubs/LOP_PTAS_STOC.pdf Huffman coding with unequal letter costs]", [http://www.informatik.uni-trier.de/~ley/db/conf/stoc/stoc2002.html STOC 2002]: 785-791
* [https://web.archive.org/web/20050907041425/http://gumuz.looze.net/wordpress/index.php/archives/2004/11/25/huffman-encoding/ Huffman Coding, implemented in python]

{{压缩方法}}

[[Category:数据结构|H]]
[[Category:无损压缩算法|Category:无损压缩算法]]
[[Category:编码理论|Category:编码理论]]
[[Category:数据压缩|Category:数据压缩]]