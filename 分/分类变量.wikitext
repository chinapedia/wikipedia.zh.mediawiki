'''分类变量'''或称'''类别变量'''是[[统计学|统计学]]中的有限多个取值的[[变量与属性_(数据处理)|变量]]，其每个值对应于{{tsl|en|qualitative property|定性属性}}的特定分组（group）或{{tsl|en|nominal category|定类类别}}。<ref name="yates">{{cite book | last = Yates | first = Daniel S. | last2 = Moore | first2 = David S | last3 = Starnes | first3 = Daren S. | year = 2003 | title = The Practice of Statistics | edition = 2nd | publisher = [[W._H._Freeman_and_Company|Freeman]] | location = New York | url = http://bcs.whfreeman.com/yates2e/ | isbn = 978-0-7167-4773-4 | access-date = 2014-09-28 | archive-url = https://web.archive.org/web/20050209001108/http://bcs.whfreeman.com/yates2e/# | archive-date = 2005-02-09 | url-status = dead }}</ref>在计算机科学或一些数学分支中，分类变量对应于[[列举法_(集合论)|列举法]]或[[枚举类型|枚举类型]]。通常，分类变量的每个值成为一个'''level'''。其概率分布称为{{tsl|en|categorical distribution|分类分布}}。

'''分类数据'''（Categorical data）是一种[[统计数据类型|统计数据类型]]，由分类变量及其数据组成。具体说，分类数据可从[[定性数据|定性数据]]计数汇总或生成{{tsl|en|cross tabulation|列联表}}，或从[[定量数据|定量数据]]按照给定的间隔分组得到。 

分类变量如果只可能有两个取值，被称为{{tsl|en|binary variable|二值变量}}（binary variable或dichotomous variable），如[[伯努利变量|伯努利变量]]。分类变量如果取多于2个值，成为多值变量（polytomous variables）。

==分类变量的例子==
*[[血型|血型]]： A, B, AB 或 O.
*一个国家的合法政党
*岩石类型： [[火成岩|火成岩]], [[沉积岩|沉积岩]], [[变质岩|变质岩]].

==表示法==
为使统计处理简便，分类变量可以赋以数值索引值，如从1到K，对于K值分类变量。这种表示可以用于相等比较、作为[[元素_(數學)|集合的元素]]做集合运算。

分类变量的集合的[[集中趋势|集中趋势]]可用[[众数_(数学)|众数]]表示，但不能定义[[均值|均值]]或[[中位数|中位数]]。 

==可能值的数量==
分类的[[随机变量|随机变量]]用统计学的{{tsl|en|categorical distribution|分类分布}}，允许任意''K''值分类变量用每个值的单独的概率来表示（即K值的离散概率分布）。这种多值分类变量常用[[多项分布|多项分布]]来分析。分类结果的[[回归分析|回归分析]]是通过[[多项逻辑回归|多项逻辑回归]]、{{tsl|en|multinomial probit}}或相关的{{tsl|en|discrete choice}}模型。

分类变量也可以只有两种可能结果，称为二值变量或伯努利变量。由于重要性，这种情形常被视作独立分布（伯努利分布）、独立的回归模型（逻辑回归、{{tsl|en|probit regression}}等）。反之，分类变量常被用于指大于等于3种结果，或称“多值变量”（multi-way variable）。 

==Categorical variables and regression==
<!--
Categorical variables represent a [[Qualitative_data|qualitative]] method of scoring data (i.e. represents categories or group membership). These can be included as [[independent_variable|independent variable]]s in a [[regression_analysis|regression analysis]] or as dependent variables in [[logistic_regression|logistic regression]] or [[probit_regression|probit regression]], but must be converted to [[quantitative_data|quantitative data]] in order to be able to analyze the data. One does so through the use of coding systems. Analyses are conducted such that only ''g'' -1 (''g'' being the number of groups) are coded. This minimizes redundancy while still representing the complete data set as no additional information would be gained from coding the total ''g'' groups: for example, when coding gender (where ''g'' = 2: male and female), if we only code females everyone left over would necessarily be males. In general, the group that one does not code for is the group of least interest.<ref name = Cohen>{{cite book|author1=Cohen, J. |author2=Cohen, P. |author3=West, S. G. |author4= Aiken, L. S.|author4-link= Leona S. Aiken |title=Applied multiple regression/correlation analysis for the behavioural sciences (3rd ed.)|year=2003|publisher=Routledge|location=New York, NY}}</ref>

There are three main coding systems typically used in the analysis of categorical variables in regression: dummy coding, effects coding, and contrast coding. The regression equation takes the form of '''''Y = bX + a''''', where ''b'' is the slope and gives the weight empirically assigned to an explanator, ''X'' is the explanatory variable, and ''a'' is the [[Y-intercept|''Y''-intercept]], and these values take on different meanings based on the coding system used. The choice of coding system does not affect the ''[[F_statistic|F]]'' or [[Pearson_product-moment_correlation_coefficient|''R''<sup>2</sup>]] statistics. However, one chooses a coding system based on the comparison of interest since the interpretation of ''b'' values will vary.<ref name = Cohen/>

===Dummy coding===

Dummy coding is used when there is a [[Control_group|control]] or comparison group in mind. One is therefore analyzing the data of one group in relation to the comparison group: ''a'' represents the mean of the control group and ''b'' is the difference between the mean of the [[experimental_group|experimental group]] and the mean of the control group. It is suggested that three criteria be met for specifying a suitable control group: the group should be a well-established group (e.g. should not be an “other” category), there should be a logical reason for selecting this group as a comparison (e.g. the group is anticipated to score highest on the dependent variable), and finally, the group's sample size should be substantive and not small compared to the other groups.<ref>{{cite book|last=Hardy|first=Melissa|title=Regression with dummy variables|year=1993|publisher=Sage|location=Newbury Park, CA}}</ref>

In dummy coding, the reference group is assigned a value of 0 for each code variable, the group of interest for comparison to the reference group is assigned a value of 1 for its specified code variable, while all other groups are assigned 0 for that particular code variable.<ref name = Cohen/>

The ''b'' values should be interpreted such that the experimental group is being compared against the control group. Therefore, yielding a negative b value would entail the experimental group have scored less than the control group on the [[dependent_variable|dependent variable]]. To illustrate this, suppose that we are measuring optimism among several nationalities and we have decided that French people would serve as a useful control. If we are comparing them against Italians, and we observe a negative ''b'' value, this would suggest Italians obtain lower optimism scores on average.

The following table is an example of dummy coding with ''French'' as the control group and C1, C2, and C3 respectively being the codes for ''Italian'', ''German'', and ''Other'' (neither French nor Italian nor German):

{| class="wikitable"
|-
| '''Nationality''' || '''C1''' || '''C2''' || '''C3'''
|-
| French || 0 || 0 || 0
|-
| Italian || 1|| 0 || 0
|-
| German || 0 || 1 || 0
|-
| Other || 0 || 0 || 1
|}

===Effects coding===

In the effects coding system, data are analyzed through comparing one group to all other groups. Unlike dummy coding, there is no control group. Rather, the comparison is being made at the mean of all groups combined (''a'' is now the [[grand_mean|grand mean]]). Therefore, one is not looking for data in relation to another group but rather, one is seeking data in relation to the grand mean.<ref name = Cohen/>

Effects coding can either be weighted or unweighted. Weighted effects coding is simply calculating a weighted grand mean, thus taking into account the sample size in each variable. This is most appropriate in situations where the sample is representative of the population in question. Unweighted effects coding is most appropriate in situations where differences in sample size are the result of incidental factors. The interpretation of ''b'' is different for each: in unweighted effects coding ''b'' is the difference between the mean of the experimental group and the grand mean, whereas in the weighted situation it is the mean of the experimental group minus the weighted grand mean.<ref name = Cohen/>

In effects coding, we code the group of interest with a 1, just as we would for dummy coding. The principal difference is that we code −1 for the group we are least interested in. Since we continue to use a ''g'' - 1 coding scheme, it is in fact the −1 coded group that will not produce data, hence the fact that we are least interested in that group. A code of 0 is assigned to all other groups.

The ''b'' values should be interpreted such that the experimental group is being compared against the mean of all groups combined (or weighted grand mean in the case of weighted effects coding). Therefore, yielding a negative ''b'' value would entail the coded group as having scored less than the mean of all groups on the dependent variable. Using our previous example of optimism scores among nationalities, if the group of interest is Italians, observing a negative ''b'' value suggest they obtain a lower optimism score.

The following table is an example of effects coding with ''Other'' as the group of least interest.

{| class="wikitable"
|-
| '''Nationality''' || '''C1''' || '''C2''' || '''C3'''
|-
| French || 0 || 0 || 1
|-
| Italian || 1 || 0 || 0
|-
| German|| 0 || 1 || 0 
|-
| Other || −1 || −1 || −1 
|}

===Contrast coding===

The contrast coding system allows a researcher to directly ask specific questions. Rather than having the coding system dictate the comparison being made (i.e., against a control group as in dummy coding, or against all groups as in effects coding) one can design a unique comparison catering to one's specific research question. This tailored hypothesis is generally based on previous theory and/or research. The hypotheses proposed are generally as follows: first, there is the central hypothesis which postulates a large difference between two sets of groups; the second hypothesis suggests that within each set, the differences among the groups are small. Through its [[A_priori_(epistemology)|a priori]] focused hypotheses, contrast coding may yield an increase in [[Power_(statistics)|power]] of the [[statistical_test|statistical test]] when compared with the less directed previous coding systems.<ref name = Cohen/>

Certain differences emerge when we compare our a priori coefficients between [[ANOVA|ANOVA]] and regression. Unlike when used in ANOVA, where it is at the researcher's discretion whether they choose coefficient values that are either [[Orthogonality|orthogonal]] or non-orthogonal, in regression, it is essential that the coefficient values assigned in contrast coding be orthogonal. Furthermore, in regression, coefficient values must be either in fractional or decimal form. They cannot take on interval values.

The construction of contrast codes is restricted by three rules:

# The sum of the contrast coefficients per each code variable must equal zero.
# The difference between the sum of the positive coefficients and the sum of the negative coefficients should equal 1.
# Coded variables should be orthogonal.<ref name = Cohen/>

Violating rule 2 produces accurate ''R''<sup>2</sup> and ''F'' values, indicating that we would reach the same conclusions about whether or not there is a significant difference; however, we can no longer interpret the ''b'' values as a mean difference.

To illustrate the construction of contrast codes consider the following table. Coefficients were chosen to illustrate our a priori hypotheses: Hypothesis 1: French and Italian persons will score higher on optimism than Germans (French = +0.33, Italian = +0.33, German = −0.66). This is illustrated through assigning the same coefficient to the French and Italian categories and a different one to the Germans. The signs assigned indicate the direction of the relationship (hence giving Germans a negative sign is indicative of their lower hypothesized optimism scores). Hypothesis 2: French and Italians are expected to differ on their optimism scores (French = +0.50, Italian = −0.50, German = 0). Here, assigning a zero value to Germans demonstrates their non-inclusion in the analysis of this hypothesis. Again, the signs assigned are indicative of the proposed relationship.

{| class="wikitable"
|-
| '''Nationality''' || '''C1''' || '''C2'''
|-
| French || +0.33 || +0.50
|-
| Italian || +0.33 || −0.50
|-
| German || −0.66 || 0
|}

===Nonsense coding===

Nonsense coding occurs when one uses arbitrary values in place of the designated “0”s “1”s and “-1”s seen in the previous coding systems.  Although it produces correct mean values for the variables, the use of nonsense coding is not recommended as it will lead to uninterpretable statistical results.<ref name = Cohen/>

===Embeddings===
''Embeddings'' are codings of categorical values into high-dimensional [[Real_numbers|real-valued]] (sometimes [[Complex_numbers|complex-valued]]) vector spaces, usually in such a way that ‘similar’ values are assigned ‘similar’ vectors, or with respect to some other kind of criterion making the vectors useful for the respective application. A common special case are [[word_embedding|word embedding]]s, where the possible values of the categorical variable are the [[word|word]]s in a [[language|language]] and words with similar meanings are to be assigned similar vectors.

===Interactions===

An [[Interaction_(statistics)|interaction]] may arise when considering the relationship among three or more variables, and describes a situation in which the simultaneous influence of two variables on a third is not additive. Interactions may arise with categorical variables in two ways: either categorical by categorical variable interactions, or categorical by continuous variable interactions.

====Categorical by categorical variable interactions====

This type of interaction arises when we have two categorical variables. In order to probe this type of interaction, one would code using the system that addresses the researcher's hypothesis most appropriately. The product of the codes yields the interaction. One may then calculate the ''b'' value and determine whether the interaction is significant.<ref name = Cohen/>

====Categorical by continuous variable interactions====

Simple slopes analysis is a common [[post_hoc_test|post hoc test]] used in regression which is similar to the simple effects analysis in ANOVA, used to analyze interactions. In this test, we are examining the simple slopes of one independent variable at specific values of the other independent variable. Such a test is not limited to use with continuous variables, but may also be employed when the independent variable is categorical. We cannot simply choose values to probe the interaction as we would in the continuous variable case because of the nominal nature of the data (i.e., in the continuous case, one could analyze the data at high, moderate, and low levels assigning 1 standard deviation above the mean, at the mean, and at one standard deviation below the mean respectively). In our categorical case we would use a simple regression equation for each group to investigate the simple slopes. It is common practice to [[Standardized_variable|standardize]] or center variables to make the data more interpretable in simple slopes analysis; however, categorical variables should never be standardized or centered. This test can be used with all coding systems.<ref name = Cohen/>
 
-->
==参考文献==
{{reflist}}

==进一步阅读 ==
* Andersen, Erling B. 1980. ''Discrete Statistical Models with Social Science Applications''. North Holland, 1980.
* {{cite book |title=Discrete Multivariate Analysis: Theory and Practice |last=Bishop |first=Y. M. M. |author1-link=Yvonne Bishop |first2=S. E. |last2=Fienberg |authorlink2=Stephen Fienberg |first3=P. W. |last3=Holland |year=1975 |publisher=MIT Press |isbn=978-0-262-02113-5 |mr=381130 |url-access=registration |url=https://archive.org/details/discretemultivar00bish }}
* {{cite book | last=Christensen | first=Ronald | title=Log-linear models and logistic regression | edition=Second | series=Springer Texts in Statistics | publisher=Springer-Verlag | location=New York | year=1997 | pages=xvi+483 | ISBN=0-387-98247-7 | mr=1633357 }}
* [[Friendly,_Michael|Friendly, Michael]]. ''[http://datavis.ca/papers/casm/casm.pdf Visualizing categorical data] {{Wayback|url=http://datavis.ca/papers/casm/casm.pdf |date=20190712063643 }}.'' SAS Institute, 2000.
* {{cite book | first=Steffen L. | last=Lauritzen | authorlink=Steffen L. Lauritzen | title=Lectures on Contingency Tables | year=2002 | orig-year=1979 | edition=updated electronic version of the (University of Aalborg) 3rd (1989) | url=http://www.stats.ox.ac.uk/~steffen/papers/cont.pdf | access-date=2020-11-20 | archive-date=2020-04-30 | archive-url=https://web.archive.org/web/20200430162835/http://www.stats.ox.ac.uk/~steffen/papers/cont.pdf | dead-url=no }}
* NIST/SEMATEK (2008) [http://www.itl.nist.gov/div898/handbook/ ''Handbook of Statistical Methods''] {{Wayback|url=http://www.itl.nist.gov/div898/handbook/ |date=20090227174918 }}

[[Category:統計資料型態|Category:統計資料型態]]