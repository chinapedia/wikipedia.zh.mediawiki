{{NoteTA
|G1=IT
|G2=Math
}}
'''自组织映射'''（SOM）或'''自组织特征映射'''（SOFM）是一种使用[[非監督式學習|非監督式學習]]来产生训练样本的输入空间的一个低维（通常是二维）离散化的表示的[[人工神经网络|人工神经网络]]（ANN）。自组织映射与其他[[人工神经网络|人工神经网络]]的不同之处在于它使用一个邻近函数来保持输入空间的[[拓扑学|拓扑]]性质。

==学习算法==

自组织映射中学习的目标是使网络的不同部分对输入模式有相似的响应。这部分的灵感是来自于人类[[大脑皮层|大脑皮层]]的不同部分处理视觉、听觉或其他[[感官|感官]]信息的方式。<ref name="Haykin">{{cite book |first=Simon |last=Haykin |title=Neural networks - A comprehensive foundation |url=https://archive.org/details/neuralnetworksco0000hayk_2ed |chapter=9. Self-organizing maps |edition=2nd |publisher=Prentice-Hall |year=1999 |isbn=0-13-908385-5 }}</ref>

[[Image:Somtraining.svg|thumb]]<!--
-->
神经元的权重初始化为小随机值或均匀采样自两个最大的[[主成份分析|主成分]][[特征向量|特征向量]]范围。用后一种方法初始化，学习的速率就会更快，因为初始的权重已经是SOM权重的一个很好的近似了。<ref name="SOMIntro">{{cite web |title=Intro to SOM |first=Teuvo |last=Kohonen |work=SOM Toolbox |url=http://www.cis.hut.fi/projects/somtoolbox/theory/somalgorithm.shtml |year=2005<!-- last updated 18 March 2005 --> |accessdate=2006-06-18 |archive-date=2006-06-18 |archive-url=https://web.archive.org/web/20060618081209/http://www.cis.hut.fi/projects/somtoolbox/theory/somalgorithm.shtml |dead-url=no }}</ref>

必须提供给网络大量的能够实例向量来尽可能近地表示映射中期望的那些类型的向量。这些例子通常在迭代中数次使用。

训练采用竞争性学习。当训练样本提供给网络的时候，就会计算它与每个权重之间的[[欧氏距离|欧氏距离]]。权重向量与输入最相似的神经元称作最佳匹配单元（BMU）。SOM栅格中BMU的权重以及与其邻近的神经元会向着输入向量调整。从BMU变化的量会随着（栅格中）时间和距离而降低。拥有权值 '''W<sub>v</sub>'''(s) 的神经元v的更新公式为
:'''W<sub>v</sub>'''(s + 1) = '''W<sub>v</sub>'''(s) + Θ(u, v, s) α(s)('''D'''(t) - '''W<sub>v</sub>'''(s)),
其中 s 为步长指数，t 是训练样本的指数，'''D'''(t) 是输入向量，u 是 '''D'''(t) 的BMU指数，α(s) 是一个[[单调函数|单调递减]]的学习系数；Θ(u, v, s) 是在步长为 s 下给出神经元 u 和神经元 v 之间距离的[[邻近函数|邻近函数]]。<ref name="Scholarpedia">{{cite web |title=Kohonen network |first1=Teuvo |last1=Kohonen |first2=Timo |last2=Honkela |work=Scholarpedia |url=http://www.scholarpedia.org/article/Kohonen_network |year=2011<!-- last approved revision 2011-11-15 --> |accessdate=2012-09-24 |archive-date=2009-03-02 |archive-url=https://web.archive.org/web/20090302023601/http://www.scholarpedia.org/article/Kohonen_network |dead-url=no }}</ref> 根据实现的不同，t 可以系统地（ 0, 1, 2...T-1，然后重复，T 为训练样本的大小），也可以随机从数据集中取出（Bootstrap抽样），或采用其他一些抽样方法（如jackknifing）。

邻近函数Θ(u, v, s)取决于BMU（神经元u）与神经元v之间的栅格距离。在最简单的形式中，对所有足够接近BMU的神经元都是1，其余都是0，但[[高斯函数|高斯函数]]也是一种常见的选择。不管函数形式如何，邻近函数都会随着时间收缩。<ref name="Haykin" /> 在起初邻域范围很大的时候，自组织发生在全局范围内。当邻域范围缩小到仅有两个神经元时，权值会收敛于局部估计值。在一些实现中，学习系数α会随着增加s而平稳减小，另一些实现（特别是t扫描训练数据集的情况）中步进式地下降，每 T 步一次。

对每个输入向量这个过程都会重复（通常很多次）循环λ。该网络把输入数据集中的类别或模式与相关联的输出节点拉近。如果可以重新命名这些模式，这些名称就会附加到已训练的网络中的相关节点中。

在映射中，会有一个单一“获胜”神经元：该神经元的权重向量与输入向量最接近。这可以很容易由计算输入向量和权值向量的欧氏距离确定。

=== 算法 ===
# 随机排列映射中节点的权值向量
# 取出一个输入向量 <math>\mathbf{D(t)}</math>
# 在映射中遍历每个节点
## 使用[[欧氏距离|欧氏距离]]公式得到输入向量和该映射节点的权值向量之间的相似度
## 找出距离最小的节点（这个节点是最佳匹配单元，BMU）
# 通过更新将BMU邻域内的节点（包括BMU本身）拉向输入向量。
## '''W<sub>v</sub>'''(s + 1) = '''W<sub>v</sub>'''(s) + Θ(u, v, s) α(s)('''D'''(t) - '''W<sub>v</sub>'''(s))
# 增加 s 并回到第二步只要 <math>s < \lambda</math> 就继续循环

== 参考资料 ==
{{Reflist|2}}

[[Category:人工神经网络|Category:人工神经网络]]
[[Category:芬兰发明|Category:芬兰发明]]