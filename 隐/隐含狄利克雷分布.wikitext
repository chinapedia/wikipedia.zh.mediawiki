'''隐含狄利克雷分布'''（{{lang-en|Latent Dirichlet allocation}}，简称'''LDA'''），是一种[[主题模型|主题模型]]，它可以将文档集中每篇文档的主题按照[[概率分布|概率分布]]的形式给出。同时它是一种[[非監督式學習|无监督学习]]算法，在训练时不需要手工标注的训练集，需要的仅仅是文档集以及指定主题的数量k即可。此外LDA的另一个优点则是，对于每一个主题均可找出一些词语来描述它。

LDA首先由 David M. Blei、[[吴恩达|吴恩达]]和[[迈克尔·乔丹_(学者)|迈克尔·I·乔丹]]于2003年提出<ref name="blei2003">{{cite journal
 |last1         = Blei
 |first1        = David M.
 |last2         = Ng
 |first2        = Andrew Y.
 |last3         = Jordan
 |first3        = Michael I
 |authorlink3   = 迈克尔·乔丹 (学者)
 |title         = Latent Dirichlet allocation
 |journal       = [[Journal_of_Machine_Learning_Research|Journal of Machine Learning Research]]
 |volume        = 3
 |pages         = ''pp.'' 993–1022
 |url           = http://jmlr.csail.mit.edu/papers/v3/blei03a.html
 |doi           = 10.1162/jmlr.2003.3.4-5.993
 |editor1-last  = Lafferty
 |editor1-first = John
 |issue         = 4–5
 |date          = January 2003
 |author        = 
 |access-date   = 2013-07-08
 |archive-url   = https://web.archive.org/web/20120501152722/http://jmlr.csail.mit.edu/papers/v3/blei03a.html
 |archive-date  = 2012-05-01
 |dead-url      = yes
}}</ref>，目前在[[文本挖掘|文本挖掘]]领域包括文本主题识别、文本分类以及文本相似度计算方面都有应用。

==数学模型==
[[File:Smoothed_LDA.png|缩略图]]
LDA是一种典型的词袋模型，即它认为一篇文档是由一组词构成的一个集合，词与词之间没有顺序以及先后的关系。一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。

另外，正如[[Β分布|Beta分布]]是[[二项式分布|二项式分布]]的共轭[[先验概率|先验概率]]分布，狄利克雷分布作为多项式分布的共轭[[先验概率|先验概率]]分布。因此正如LDA[[貝氏網路|贝斯网络]]结构中所描述的，在LDA模型中一篇文档生成的方式如下:

* 从狄利克雷分布<math>\alpha </math>中取样生成文档i的主题分布<math>\theta_i</math>
* 从主题的多项式分布<math>\theta_i</math>中取样生成文档i第j个词的主题<math>z_{i, j}</math>
* 从狄利克雷分布<math>\beta </math>中取样生成主题<math>z_{i, j}</math>的词语分布<math>\phi_{z_{i, j}}</math>
* 从词语的多项式分布<math>\phi_{z_{i, j}}</math>中采样最终生成词语<math>w_{i, j}</math>

因此整个模型中所有可见变量以及隐藏变量的[[联合分布|联合分布]]是

:<math>p(w_i, z_i, \theta_i, \Phi | \alpha, \beta) = \prod_{j = 1}^{N} p(\theta_i|\alpha)p(z_{i, j}|\theta_i)p(\Phi|\beta)p(w_{i, j}|\phi_{z_{i, j}})</math>

最终一篇文档的单词分布的[[最大似然估计|最大似然估计]]可以通过将上式的<math>\theta_i</math>以及<math>\Phi</math>进行积分和对<math>z_i</math>进行求和得到

:<math>p(w_i | \alpha, \beta)  = \int_{\theta_i}\int_{\Phi }\sum_{z_i}p(w_i, z_i, \theta_i, \Phi | \alpha, \beta) </math>

根据<math>p(w_i | \alpha, \beta) </math>的最大似然估计，最终可以通过[[吉布斯采样|吉布斯采样]]等方法估计出模型中的参数。

==使用吉布斯采样估计LDA参数==
在LDA最初提出的时候，人们使用EM算法进行求解，后来人们普遍开始使用较为简单的Gibbs Sampling，具体过程如下：
* 首先对所有文档中的所有词遍历一遍，为其都随机分配一个主题，即<math>z_{m,n}=k\sim Mult(1/K) </math>，其中m表示第m篇文档，n表示文档中的第n个词，k表示主题，K表示主题的总数，之后将对应的<math>n_{m}^k+1</math>，<math>n_{m}+1</math>，<math>n_{k}^t+1</math>，<math>n_{k}+1</math>，他们分别表示在m文档中k主题出现的次数，m文档中主题数量的和，k主题对应的t词的次数，k主题对应的总词数。
* 之后对下述操作进行重复迭代。
* 对所有文档中的所有词进行遍历，假如当前文档m的词t对应主题为k，则<math>n_{m}^k-1</math>，<math>n_{m}-1</math>，<math>n_{k}^t-1</math>，<math>n_{k}-1</math>，即先拿出当前词，之后根据LDA中topic sample的概率分布sample出新的主题，在对应的<math>n_{m}^k</math>，<math>n_{m}</math>，<math>n_{k}^t</math>，<math>n_{k}</math>上分别+1。
:<math>p(z_i=k|z_{-i},w)</math>∝<math>(n^{(t)}_{k,-i}+\beta_t)(n_{m,-i}^{(k)}+\alpha_k)/(\sum_{t=1}^{V}n_{k,-i}^{(t)}+\beta_t)</math>
* 迭代完成后输出主题-词参数矩阵φ和文档-主题矩阵θ
:<math>\phi_{k,t}=(n_k^{(t)}+\beta_t)/(n_k+\beta_t)</math>
:<math>\theta_{m,k}=(n_m^{(k)}+\alpha_k)/(n_m+\alpha_k)</math>

== 参见 ==
{{div col |cols = 3 }}
* {{le|萬能翻譯機|universal translator}}
* [[電腦語言學|電腦語言學]]
* [[受限自然語言|受限自然語言]]
* [[信息抽取|信息抽取]]
* [[資訊檢索|資訊檢索]]
* [[自然語言理解|自然語言理解]]
* [[潛在語義索引|潛在語義索引]]
* [[潜在语义学|潜在语义学]]
* {{le|隨機文法|Stochastic grammar}}
* [[機器記者|機器記者]]
* {{le|寫作自動評分|Automated essay scoring}}
* {{le|生物醫學文件探勘系統|Biomedical text mining}}
* {{le|複合詞處理|Compound term processing}}
* [[计算语言学|计算语言学]]
* {{le|電腦輔助審查|Computer-assisted reviewing}}
* [[深度学习|深度学习]]
* {{le|深度語言處理|Deep linguistic processing}}
* {{le|輔助外文閱讀|Foreign language reading aid}}
* {{le|輔助外文寫作|Foreign language writing aid}}
* {{le|語言科技|Language technology}}
* [[隐含狄利克雷分布|隐含狄利克雷分布]]（LDA）
<!--* [[List_of_natural_language_processing_toolkits|List of natural language processing toolkits]]-->
* {{le|母语识别|Native-language identification}}
* {{le|自然語言編程|Natural language programming}}
* {{le|自然語言使用者界面|Natural language user interface}}
* [[擴展查詢|擴展查詢]]
* {{le|具體化 (語言學)|Reification (linguistics)}}
* {{le|語義折疊|Semantic folding}}
* [[语音处理|语音处理]]
* {{le|口語對話系統|Spoken dialogue system}}
* [[校對|校對]]
* {{le|文字简化|Text simplification}}
* {{le|Thought vector|Thought vector}}
* {{le|Truecasing|Truecasing}}
* [[問答系統|問答系統]]
* [[Word2vec|Word2vec]]
{{div col end}}

{{-}}
{{Computer Science}}

{{Authority control}}

== 参考文献 ==
{{Reflist}}

[[Category:概率模型|Category:概率模型]]
[[Category:机器学习|Category:机器学习]]
[[Category:自然語言處理|Category:自然語言處理]]